/*******************************************************************************
* Copyright 2016-2020 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

#include <assert.h>
#include <math.h>

#include "common/c_types_map.hpp"
#include "common/dnnl_thread.hpp"
#include "common/type_helpers.hpp"

#include "cpu/ref_lrn.hpp"

namespace dnnl {
namespace impl {
namespace cpu {

namespace {

typedef float acc_data_t;

static inline acc_data_t fast_negative_powf(acc_data_t omega, acc_data_t beta) {
    acc_data_t Y;
    /*
         * Y = omega^(-3/4) =
         * = 1.0f / sqrtf(omega) * sqrtf(1.0f / sqrtf(omega))
         * = sqrtf(1.0f / sqrtf(omega)) * 1.0f / sqrtf(omega)
         * = sqrtf(1.0f / sqrtf(omega)) / sqrtf(omega)
         * = sqrtf(1.0f / sqrtf(omega) / omega)
         * = sqrtf(1.0f / (sqrtf(omega) * omega))
         */
    if (beta == 0.75f) {
        Y = sqrtf(1.0f / (sqrtf(omega) * omega));
    } else {
        Y = 1.0f / powf(omega, beta);
    }
    return Y;
};
} // namespace

// Forward LRN formula:
// y_i = x_i * (k + a / n * Sum:j [x_j^2])^-b, where
// k, a(alpha), b(beta), n(local_size) - lrn hyperparameters;
// j - kernel points, j in [i - n/2, i + n/2] for ACROSS, 2d-shape for WITHIN;

// ? constexpr unsigned ndims<tag> ?
#define OFFSET_ARGS memory_desc_wrapper const& md, \
    dim_t const mb, dim_t const stride_mb, \
    dim_t const c, dim_t const C, \
    dim_t const d, dim_t const D, \
    dim_t const h, dim_t const H, \
    dim_t const w, dim_t const W
template <impl::format_tag_t tag> static inline
dim_t offset(OFFSET_ARGS) {
    if (md.ndims() >= 5) return md.off(mb, c, d, h, w);
    if (md.ndims() >= 4) return md.off(mb, c, h, w);
    if (md.ndims() >= 3) return md.off(mb, c, w);
    return md.off(mb, c);
};
template<> inline dim_t offset<format_tag::nChw8c>(OFFSET_ARGS) {
    constexpr int blksize = 8;
    return mb * stride_mb + (c / blksize) * H * W * blksize
        + h * W * blksize + w * blksize + c % blksize;
};
template<> inline dim_t offset<format_tag::nChw16c>(OFFSET_ARGS) {
    constexpr int blksize = 16;
    return mb * stride_mb + (c / blksize) * H * W * blksize
        + h * W * blksize + w * blksize + c % blksize;
};
template<> inline dim_t offset<format_tag::nchw>(OFFSET_ARGS) {
    return mb * stride_mb + c * H * W + h * W + w;
};
template<> inline dim_t offset<format_tag::nhwc>(OFFSET_ARGS) {
    return mb * stride_mb + h * W * C + w * C + c;
};

template <impl::data_type_t d_type>
template <impl::format_tag_t tag>
void ref_lrn_fwd_t<d_type>::execute_forward(const exec_ctx_t &ctx) const {
    using namespace alg_kind;
    using namespace format_tag;

    auto src = CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
    auto dst = CTX_OUT_MEM(data_t *, DNNL_ARG_DST);

    const memory_desc_wrapper data_d(pd()->src_md());

    const dim_t C = pd()->C();
    const dim_t D = pd()->D();
    const dim_t H = pd()->H();
    const dim_t W = pd()->W();
    const auto stride_mb = data_d.blocking_desc().strides[0];
    const bool across_channels = pd()->desc()->alg_kind == lrn_across_channels;
    static constexpr dim_t blksize = tag == nChw16c ? 16 : 8;
    const auto ndims = data_d.ndims();

    auto compute_n_summands = [&](dim_t size) {
        if (across_channels) {
            return size;
        } else { // within_channel
            dim_t n_summands = 1;
            for (auto d = ndims - 2; d > 0; --d)
                n_summands *= size;
            return n_summands;
        }
    };

    const acc_data_t alpha = static_cast<acc_data_t>(pd()->desc()->lrn_alpha);
    const acc_data_t beta = static_cast<acc_data_t>(pd()->desc()->lrn_beta);
    const acc_data_t k = static_cast<acc_data_t>(pd()->desc()->lrn_k);
    const dim_t size = pd()->desc()->local_size;
    const dim_t half_size = (size - 1) / 2;
    const dim_t summands = compute_n_summands(size);

#define KER_OC 2 /* vec across lrn window (both) */
#define VE_VEC 2

#if 1 // KER_OC==0 || VE_VEC==0 || !defined(__ve)
    auto data_off = [&](dim_t mb, dim_t c, dim_t d, dim_t h, dim_t w) -> dim_t {
        switch (tag) {
            case nChw16c:
            case nChw8c:
                return mb * stride_mb + (c / blksize) * H * W * blksize
                        + h * W * blksize + w * blksize + c % blksize;
            case nchw: return mb * stride_mb + c * H * W + h * W + w;
            case nhwc: return mb * stride_mb + h * W * C + w * C + c;
            default:
                if (ndims >= 5) return data_d.off(mb, c, d, h, w);
                if (ndims >= 4) return data_d.off(mb, c, h, w);
                if (ndims >= 3) return data_d.off(mb, c, w);
                return data_d.off(mb, c);
        }
    };
#endif

    // old VE way
    //
#define DATA_OFF_NCHW(mb,c,h,w) ((uint64_t)mb * (uint64_t)stride_mb + (uint64_t)c * (uint64_t)H * (uint64_t)W + (uint64_t)h * (uint64_t)W + (uint64_t)w)
#define DATA_OFF_NHWC(mb,c,h,w) (mb * stride_mb + h * W * C + w * C + c)
#define DATA_OFF_nChwNc(mb,c,h,w) (mb * stride_mb + c / blksize * H * W * blksize \
        + h * W * blksize + w * blksize + c % blksize)
#define DATA_OFF_ANY(mb,c,d,h,w) data_d.off(mb,c,d,h,w)


    // very slow -- function call prohibits vectorization
#define DATA_OFF(mb,c,d,h,w) ( \
        fmt==nchw?                      DATA_OFF_NCHW(mb,c,h,w) \
        :fmt==nhwc?                     DATA_OFF_NHWC(mb,c,h,w) \
        :fmt==nChw16c || fmt==nChw8c?   DATA_OFF_nChwNc(mb,c,h,w) \
        :                               DATA_OFF_ANY(mb,c,d,h,w) \
        )

    // pass by value due to icc170 and icc180 problem on KNL
    auto ker = [=](data_t *d, dim_t mb, dim_t oc, dim_t od, dim_t oh,
                       dim_t ow) {
        acc_data_t sum = 0;
        acc_data_t central = 0;
        if (across_channels) {
            // half_size is quite small -- little opportunity for vectorization
            const dim_t c_st = nstl::max(oc - half_size + 0, (dim_t)0);
            const dim_t c_en = nstl::min(oc + half_size + 1, C);

#if KER_OC==0 //orig -- never vectorizable?
            central = src[data_off(mb, oc, od, oh, ow)];
            for (dim_t c = c_st; c < c_en; ++c) {
                const acc_data_t s = src[data_off(mb, c, od, oh, ow)];
                sum += s * s;
            }
#elif KER_OC==2
            //central = src[data_off(mb, oc, od, oh, ow)];
            central = src[offset<tag>(data_d, mb,stride_mb, oc,C, od,D, oh,H, ow,W)];
            for (dim_t c = c_st; c < c_en; ++c) {
                const acc_data_t s = src[offset<tag>(data_d, mb,stride_mb, c,C, od,D, oh,H, ow,W)];
                sum += s * s;
            }
#elif 1 // VE vectorization mods (from gen-dnn-v0.16)
            // Note: norm across small-ish lrn window (perhaps (c_en-c_st ~ 5)
            if(tag==nchw){
                //asm("### nchw across channels\n");
                central = src[DATA_OFF_NCHW(mb, oc, oh, ow)];
                for (int c = c_st; c < c_en; ++c) {
                    const float s = src[DATA_OFF_NCHW(mb,c,oh,ow)];
                    sum += s * s;
                }
                //asm("### nchw across channels DONE\n");
            }else if (tag==nhwc){
                //asm("### nhwc across channels\n");
                central = src[DATA_OFF_NHWC(mb, oc, oh, ow)];
                for (int c = c_st; c < c_en; ++c) {
                    const float s = src[DATA_OFF_NHWC(mb,c,oh,ow)];
                    sum += s * s;
                }
                //asm("### nhwc across channels DONE\n");
            }
            else if(tag==nChw16c || tag==nChw8c){
                // note: blksize is const 8 or 16, so we may be able to use shifts if split cases
                //uint64_t src_off[c_en-c_st];
                //asm("### nChwNc across channels\n");
                central = src[DATA_OFF_nChwNc(mb, oc, oh, ow)];
                for (int c = c_st; c < c_en; ++c) {
                    //src_off[c-c_st] = DATA_OFF_nChwNc(mb,c,oh,ow);
                    //const float s = src[src_off[c-c_st]];
                    const float s = src[DATA_OFF_nChwNc(mb,c,oh,ow)];
                    sum += s * s;
                }
                //asm("### nChwNc across channels DONE\n");
            }
            else{
                uint64_t src_off[c_en-c_st];
                //asm("### any across channels\n");
                // not vectorizable XXX TBD mdw_opt::off_v_vec XXX
                central = src[DATA_OFF_ANY(mb, oc, od, oh, ow)];
                for (int c = c_st; c < c_en; ++c) {
                    src_off[c-c_st] = DATA_OFF_ANY(mb,c,od,oh,ow);
                    const float s = src[src_off[c-c_st]];
                    //const float s = src[DATA_OFF_ANY(mb,c,h,w)];
                    sum += s * s;
                }
                //asm("### any across channels DONE\n");
            }
#endif


        } else {
            dim_t d_st = nstl::max(od - half_size + 0, (dim_t)0);
            dim_t d_en = nstl::min(od + half_size + 1, D);
            dim_t h_st = nstl::max(oh - half_size + 0, (dim_t)0);
            dim_t h_en = nstl::min(oh + half_size + 1, H);
            dim_t w_st = nstl::max(ow - half_size + 0, (dim_t)0);
            dim_t w_en = nstl::min(ow + half_size + 1, W);
#if VE_VEC==0// orig
            central = src[data_off(mb, oc, od, oh, ow)];
            for_(dim_t d = d_st; d < d_en; ++d)
            for_(dim_t h = h_st; h < h_en; ++h)
            for (dim_t w = w_st; w < w_en; ++w) {
                const acc_data_t s = src[data_off(mb, oc, d, h, w)];
                sum += s * s;
            }
#elif VE_VEC==2 // much faster, av.VL = 5
            central = src[offset<tag>(data_d, mb,stride_mb, oc,C, od,D, oh,H, ow,W)];
            for_(dim_t d = d_st; d < d_en; ++d)
            for_(dim_t h = h_st; h < h_en; ++h)
            for (dim_t w = w_st; w < w_en; ++w) {
                const acc_data_t s = src[offset<tag>(
                        data_d, mb,stride_mb, oc,C, d,D, h,H, w,W)];
                sum += s * s;
            }
#else // VE mods (vectorization) higher vl, but not much code elim. (still faster than orig)
            if(tag==nchw){
                //central = src[DATA_OFF_NCHW(mb, oc, oh, ow)];
                uint64_t src_off[(h_en-h_st)*(w_en-w_st)];
                //for_(dim_t d = d_st; d < d_en; ++d)
                for_(dim_t h = h_st; h < h_en; ++h)
                for (dim_t w = w_st; w < w_en; ++w) {
                    uint64_t const hw_seq = (h-h_st)*(w_en-w_st)+(w-w_st);
                    src_off[hw_seq] = DATA_OFF_NCHW(mb,oc,h,w);
                    const float s = src[src_off[hw_seq]];
                    sum += s * s;
                }
                // slightly better...
                central = src[src_off[(oh-h_st)*(w_en-w_st)+(ow-w_st)]];
            }else if(tag==nhwc){
                central = src[DATA_OFF_NHWC(mb, oc, oh, ow)];
                uint64_t src_off[(h_en-h_st)*(w_en-w_st)];
                //for_(dim_t d = d_st; d < d_en; ++d)
                for_(dim_t h = h_st; h < h_en; ++h)
                for (dim_t w = w_st; w < w_en; ++w) {
                    uint64_t const hw_seq = (h-h_st)*(w_en-w_st)+(w-w_st);
                    src_off[hw_seq] = DATA_OFF_NHWC(mb,oc,h,w);
                    const float s = src[src_off[hw_seq]];
                    sum += s * s;
                }
            }else if(tag==nChw16c || tag==nChw8c){
                central = src[DATA_OFF_nChwNc(mb, oc, oh, ow)];
                uint64_t src_off[(h_en-h_st)*(w_en-w_st)];
                //for_(dim_t d = d_st; d < d_en; ++d)
                for_(dim_t h = h_st; h < h_en; ++h)
                for (dim_t w = w_st; w < w_en; ++w) {
                    uint64_t const hw_seq = (h-h_st)*(w_en-w_st)+(w-w_st);
                    src_off[hw_seq] = DATA_OFF_nChwNc(mb,oc,h,w);
                    const float s = src[src_off[hw_seq]];
                    sum += s * s;
                }
            }else{
                central = src[data_off(mb, oc, od, oh, ow)];
                // VE non-vectorizable XXX use memory_desc_wrapper_opt, with public vec_off_v?
                for_(dim_t d = d_st; d < d_en; ++d)
                for_(dim_t h = h_st; h < h_en; ++h)
                for (dim_t w = w_st; w < w_en; ++w) {
                        const float s = src[DATA_OFF_ANY(mb, oc, d, h, w)];
                        sum += s * s;
                }
            }
#endif
        }
        // next section also can optimize data_off XXX
        sum = k + alpha * sum / summands;
        //const acc_data_t central = src[data_off(mb, oc, od, oh, ow)];
        d[0] = static_cast<data_t>(central * fast_negative_powf(sum, beta));
    };

    const dim_t MB = pd()->MB();
    if (tag == nChw16c || tag == nChw8c) {
        parallel_nd(MB, utils::div_up(C, blksize), H, W,
                [&](dim_t mb, dim_t c_blk, dim_t h, dim_t w) {
                    dim_t c = c_blk * blksize;
                    //const dim_t off = mb * stride_mb + c * H * W
                    //        + (h * W + w) * blksize;
                    const dim_t off = DATA_OFF_nChwNc(mb,c,h,w);
                    PRAGMA_OMP_SIMD() //PragmaQuote("_NEC novector")
                    for (dim_t cc = 0; cc < nstl::min(blksize, C - c); ++cc)
                        ker(&dst[off + cc], mb, c + cc, 0, h, w);
                });
    } else if (tag == nchw) {
        parallel_nd(MB, C, H, W, [&](dim_t mb, dim_t c, dim_t h, dim_t w) {
            const dim_t off = DATA_OFF_NCHW(mb,c,h,w);
            ker(&dst[off], mb, c, 0, h, w);
        });
    } else if (tag == nhwc) {
        parallel_nd(MB, H, W, C, [&](dim_t mb, dim_t h, dim_t w, dim_t c) {
            //const dim_t off = mb * stride_mb + h * W * C + w * C + c;
            const dim_t off = DATA_OFF_NHWC(mb,c,h,w);
            ker(&dst[off], mb, c, 0, h, w);
        });
    } else {
        parallel_nd(MB, C, D, H, W,
                [&](dim_t mb, dim_t c, dim_t d, dim_t h, dim_t w) {
                    //const dim_t off = DATA_OFF_ANY(mb, c, d, h, w);
                    const dim_t off = data_off(mb, c, d, h, w);
                    ker(&dst[off], mb, c, d, h, w);
                });
    }
}

// Backward LRN formula (refer to Forward LRN formula):
// Partial derivatives:
// dy_i/dx_j =         - 2*a*b/n * x_i * O(i)^-b / O(i) * x_j, i != j
//             O(i)^-b - 2*a*b/n * x_i * O(i)^-b / O(i) * x_j, i == j, where
// O(i) = (k + a / n * Sum:j [x_j^2]), j in [i - n/2, i + n/2]. Note: j depends
//     on i, which means that O(i) may use more points than local_size.
// Now, z_i = Sum:k [dE/dy_k * dy_k/dx_j], where k in [i - n/2, i + n/2]
//     for ACROSS. 2d-shape for WITHIN.
// Then, dE/dy_k = diffDst_k. Finally,
// z_i = Sum:k [dd_k * dy_k/dx_j] = A - B (code variables) =
//     = dd_i * O(i)^-b - 2*a*b/n * x_i * Sum:k {O(k)^-b / O(k) * x_k * dd_k};

template <impl::data_type_t d_type>
template <dnnl_format_tag_t tag>
void ref_lrn_bwd_t<d_type>::execute_backward(const exec_ctx_t &ctx) const {
    using namespace alg_kind;
    using namespace format_tag;

    auto src = CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
    auto diff_dst = CTX_IN_MEM(const data_t *, DNNL_ARG_DIFF_DST);
    auto diff_src = CTX_OUT_MEM(data_t *, DNNL_ARG_DIFF_SRC);

    const memory_desc_wrapper data_d(pd()->src_md());

    const dim_t C = pd()->C();
    const dim_t D = pd()->D();
    const dim_t H = pd()->H();
    const dim_t W = pd()->W();
    const auto stride_mb = data_d.blocking_desc().strides[0];
    const bool across_channels = pd()->desc()->alg_kind == lrn_across_channels;
    static constexpr dim_t blksize = tag == nChw16c ? 16 : 8;
    const auto ndims = data_d.ndims();

    auto compute_n_summands = [&](dim_t size) {
        if (across_channels) {
            return size;
        } else { // within_channel
            dim_t n_summands = 1;
            for (auto d = ndims - 2; d > 0; --d)
                n_summands *= size;
            return n_summands;
        }
    };

    const acc_data_t alpha = static_cast<acc_data_t>(pd()->desc()->lrn_alpha);
    const acc_data_t beta = static_cast<acc_data_t>(pd()->desc()->lrn_beta);
    const acc_data_t k = static_cast<acc_data_t>(pd()->desc()->lrn_k);
    const dim_t size = pd()->desc()->local_size;
    const dim_t half_size = (size - 1) / 2;
    const dim_t summands = compute_n_summands(size);

    auto data_off = [&](dim_t mb, dim_t c, dim_t d, dim_t h, dim_t w) -> dim_t {
        switch (tag) {
            case nChw16c:
            case nChw8c:
                return mb * stride_mb + (c / blksize) * H * W * blksize
                        + h * W * blksize + w * blksize + c % blksize;
            case nchw: return mb * stride_mb + c * H * W + h * W + w;
            case nhwc: return mb * stride_mb + h * W * C + w * C + c;
            default:
                if (ndims >= 5) return data_d.off(mb, c, d, h, w);
                if (ndims >= 4) return data_d.off(mb, c, h, w);
                if (ndims >= 3) return data_d.off(mb, c, w);
                return data_d.off(mb, c);
        }
    };

    // pass by value due to icc170 and icc180 problem on KNL
    auto get_omega = [=](dim_t mb, dim_t oc, dim_t od, dim_t oh, dim_t ow) {
        acc_data_t sum = 0;
        if (across_channels) {
            const dim_t c_st = nstl::max(oc - half_size + 0, (dim_t)0);
            const dim_t c_en = nstl::min(oc + half_size + 1, C);

            for (dim_t c = c_st; c < c_en; ++c) {
                const acc_data_t s = src[data_off(mb, c, od, oh, ow)];
                sum += s * s;
            }
        } else {
            dim_t d_st = nstl::max(od - half_size + 0, (dim_t)0);
            dim_t d_en = nstl::min(od + half_size + 1, D);
            dim_t h_st = nstl::max(oh - half_size + 0, (dim_t)0);
            dim_t h_en = nstl::min(oh + half_size + 1, H);
            dim_t w_st = nstl::max(ow - half_size + 0, (dim_t)0);
            dim_t w_en = nstl::min(ow + half_size + 1, W);
            for_(dim_t d = d_st; d < d_en; ++d)
            for_(dim_t h = h_st; h < h_en; ++h)
            for (dim_t w = w_st; w < w_en; ++w) {
                const acc_data_t s = src[data_off(mb, oc, d, h, w)];
                sum += s * s;
            }
        }
        return (acc_data_t)(k + alpha * sum / summands);
    };

    // pass by value due to icc170 and icc180 problem on KNL
    auto ker = [=](data_t *d, dim_t mb, dim_t oc, dim_t od, dim_t oh,
                       dim_t ow) {
        acc_data_t A = 0, B = 0;
        if (across_channels) {
            const dim_t c_st = nstl::max(oc - half_size + 0, (dim_t)0);
            const dim_t c_en = nstl::min(oc + half_size + 1, C);

            for (dim_t c = c_st; c < c_en; c++) {
                const auto off = data_off(mb, c, od, oh, ow);
                const acc_data_t omega = get_omega(mb, c, od, oh, ow);
                const acc_data_t omega_in_beta
                        = fast_negative_powf(omega, beta);
                const acc_data_t tmp
                        = omega_in_beta * (acc_data_t)diff_dst[off];
                if (c == oc) A = tmp;
                B += (src[off] * tmp / omega);
            }
        } else {
            dim_t d_st = nstl::max(od - half_size + 0, (dim_t)0);
            dim_t d_en = nstl::min(od + half_size + 1, D);
            dim_t h_st = nstl::max(oh - half_size + 0, (dim_t)0);
            dim_t h_en = nstl::min(oh + half_size + 1, H);
            dim_t w_st = nstl::max(ow - half_size + 0, (dim_t)0);
            dim_t w_en = nstl::min(ow + half_size + 1, W);
            for_(dim_t d = d_st; d < d_en; ++d)
            for_(dim_t h = h_st; h < h_en; ++h)
            for (dim_t w = w_st; w < w_en; ++w) {
                const auto off = data_off(mb, oc, d, h, w);
                const acc_data_t omega = get_omega(mb, oc, d, h, w);
                const acc_data_t omega_in_beta
                        = fast_negative_powf(omega, beta);
                const acc_data_t tmp
                        = omega_in_beta * (acc_data_t)diff_dst[off];
                if (d == od && h == oh && w == ow) A = tmp;
                B += (src[off] * tmp / omega);
            }
        }
        const auto off = data_off(mb, oc, od, oh, ow);
        B *= (2.0f * alpha * beta * src[off] / summands);
        *d = static_cast<data_t>(A - B);
    };

    const dim_t MB = pd()->MB();
    if (tag == nChw16c || tag == nChw8c) {
        parallel_nd(MB, utils::div_up(C, blksize), H, W,
                [&](dim_t mb, dim_t c_blk, dim_t h, dim_t w) {
                    dim_t c = c_blk * blksize;
                    const dim_t off = mb * stride_mb + c * H * W
                            + (h * W + w) * blksize;
                    PRAGMA_OMP_SIMD()
                    for (dim_t cc = 0; cc < nstl::min(blksize, C - c); ++cc)
                        ker(&diff_src[off + cc], mb, c + cc, 0, h, w);
                });
    } else if (tag == nhwc) {
        parallel_nd(MB, H, W, C, [&](dim_t mb, dim_t h, dim_t w, dim_t c) {
            const dim_t off = mb * stride_mb + h * W * C + w * C + c;
            ker(&diff_src[off], mb, c, 0, h, w);
        });
    } else {
        parallel_nd(MB, C, D, H, W,
                [&](dim_t mb, dim_t c, dim_t d, dim_t h, dim_t w) {
                    const dim_t off = data_off(mb, c, d, h, w);
                    ker(&diff_src[off], mb, c, d, h, w);
                });
    }
}

template void
ref_lrn_fwd_t<data_type::f32>::execute_forward<format_tag::nChw16c>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_fwd_t<data_type::f32>::execute_forward<format_tag::nChw8c>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::f32>::execute_forward<format_tag::nchw>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::f32>::execute_forward<format_tag::nhwc>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::f32>::execute_forward<format_tag::any>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::f32>::execute_backward<format_tag::nChw16c>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::f32>::execute_backward<format_tag::nChw8c>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_bwd_t<data_type::f32>::execute_backward<format_tag::nchw>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_bwd_t<data_type::f32>::execute_backward<format_tag::nhwc>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_bwd_t<data_type::f32>::execute_backward<format_tag::any>(
        const exec_ctx_t &ctx) const;

template void
ref_lrn_fwd_t<data_type::bf16>::execute_forward<format_tag::nChw16c>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_fwd_t<data_type::bf16>::execute_forward<format_tag::nChw8c>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::bf16>::execute_forward<format_tag::nchw>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::bf16>::execute_forward<format_tag::nhwc>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::bf16>::execute_forward<format_tag::any>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::bf16>::execute_backward<format_tag::nChw16c>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::bf16>::execute_backward<format_tag::nChw8c>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::bf16>::execute_backward<format_tag::nchw>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::bf16>::execute_backward<format_tag::nhwc>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_bwd_t<data_type::bf16>::execute_backward<format_tag::any>(
        const exec_ctx_t &ctx) const;

} // namespace cpu
} // namespace impl
} // namespace dnnl

// vim: et ts=4 sw=4 cindent cino=+s,l0,\:4,N-s
