/*******************************************************************************
* Copyright 2016-2020 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

#include <assert.h>
#include <math.h>

#include "common/c_types_map.hpp"
#include "common/dnnl_thread.hpp"
#include "common/type_helpers.hpp"
#include "common/ve/memory_desc_wrapper_opt.hpp" // we use CoordsFor vectorization helper.

#include "cpu/ref_lrn.hpp"

#include <iostream> // debug

namespace dnnl {
namespace impl {
namespace cpu {

namespace {

typedef float acc_data_t;

static inline acc_data_t fast_negative_powf(acc_data_t omega, acc_data_t beta) {
    acc_data_t Y;
    /*
         * Y = omega^(-3/4) =
         * = 1.0f / sqrtf(omega) * sqrtf(1.0f / sqrtf(omega))
         * = sqrtf(1.0f / sqrtf(omega)) * 1.0f / sqrtf(omega)
         * = sqrtf(1.0f / sqrtf(omega)) / sqrtf(omega)
         * = sqrtf(1.0f / sqrtf(omega) / omega)
         * = sqrtf(1.0f / (sqrtf(omega) * omega))
         */
    if (beta == 0.75f) {
        Y = sqrtf(1.0f / (sqrtf(omega) * omega));
    } else {
        Y = 1.0f / powf(omega, beta);
    }
    return Y;
};
} // namespace

// Forward LRN formula:
// y_i = x_i * (k + a / n * Sum:j [x_j^2])^-b, where
// k, a(alpha), b(beta), n(local_size) - lrn hyperparameters;
// j - kernel points, j in [i - n/2, i + n/2] for ACROSS, 2d-shape for WITHIN;

// ? constexpr unsigned ndims<tag> ?
#define OFFSET_ARGS memory_desc_wrapper const& md, \
    dim_t const mb, dim_t const stride_mb, \
    dim_t const c, dim_t const C, \
    dim_t const d, dim_t const D, \
    dim_t const h, dim_t const H, \
    dim_t const w, dim_t const W

/** For nc++, templated specialized templates make dead-code elimination easier
 * than the original lambda function. */
template <impl::format_tag_t tag> static inline dim_t offset(OFFSET_ARGS) {
    if (md.ndims() >= 5) return md.off(mb, c, d, h, w);
    if (md.ndims() >= 4) return md.off(mb, c, h, w);
    if (md.ndims() >= 3) return md.off(mb, c, w);
    return md.off(mb, c);
};
template<> inline dim_t offset<format_tag::nChw8c>(OFFSET_ARGS) {
    constexpr int blksize = 8;
    return mb * stride_mb + (c / blksize) * H * W * blksize
        + h * W * blksize + w * blksize + c % blksize;
};
template<> inline dim_t offset<format_tag::nChw16c>(OFFSET_ARGS) {
    constexpr int blksize = 16;
    return mb * stride_mb + (c / blksize) * H * W * blksize
        + h * W * blksize + w * blksize + c % blksize;
};
template<> inline dim_t offset<format_tag::nchw>(OFFSET_ARGS) {
    return mb * stride_mb + c * H * W + h * W + w;
};
template<> inline dim_t offset<format_tag::nhwc>(OFFSET_ARGS) {
    return mb * stride_mb + h * W * C + w * C + c;
};

template <impl::data_type_t d_type>
template <impl::format_tag_t tag>
void ref_lrn_fwd_t<d_type>::execute_forward(const exec_ctx_t &ctx) const {
    using namespace alg_kind;
    using namespace format_tag;
    typedef CoordsForNd<6,uint64_t,uint64_t> Coords;

    auto src = CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
    auto dst = CTX_OUT_MEM(data_t *, DNNL_ARG_DST);

    // offset calcs with builting formula vectorize well:
    //bool const formula = true;
    //bool const formula = false;
    bool const formula = (tag == nchw || tag == nhwc || tag == nChw8c || tag == nChw16c);

    //const memory_desc_wrapper data_d(pd()->src_md());
    //
    // If we have a formula, it vectorizes well.  If not, we pull in some vectorization
    // machinery to calculate physical offset in simd-length batches
    // I'd LIKE to have an empty data_opt wrapper if I do not need it:
    //
    //memory_desc_wrapper_opt data_opt(formula? memory_desc_t(): *pd()->src_md());
    //
    // UNFORTUNATELY, wrapper asserts that it is_block_desc() at several points..
    //
    // so just always construct the optimized one (slightly more work)
    //
    const memory_desc_wrapper_opt data_opt(pd()->src_md());
    const memory_desc_wrapper& data_d = data_opt;

    const dim_t C = pd()->C();
    const dim_t D = pd()->D();
    const dim_t H = pd()->H();
    const dim_t W = pd()->W();
    const auto stride_mb = data_d.blocking_desc().strides[0];
    const bool across_channels = pd()->desc()->alg_kind == lrn_across_channels;
    static constexpr dim_t blksize = tag == nChw16c ? 16 : 8;
    const auto ndims = data_d.ndims();

    auto compute_n_summands = [&](dim_t size) {
        if (across_channels) {
            return size;
        } else { // within_channel
            dim_t n_summands = 1;
            for (auto d = ndims - 2; d > 0; --d)
                n_summands *= size;
            return n_summands;
        }
    };

    const acc_data_t alpha = static_cast<acc_data_t>(pd()->desc()->lrn_alpha);
    const acc_data_t beta = static_cast<acc_data_t>(pd()->desc()->lrn_beta);
    const acc_data_t k = static_cast<acc_data_t>(pd()->desc()->lrn_k);
    const dim_t size = pd()->desc()->local_size;
    const dim_t half_size = (size - 1) / 2;
    const dim_t summands = compute_n_summands(size);

#if 0
    // compiler seems to to dead-code-eliminate too late to recognize
    // many important optimizations... like vectorization of simple cases :)
    auto data_off = [&](dim_t mb, dim_t c, dim_t d, dim_t h, dim_t w) -> dim_t {
        switch (tag) {
            case nChw16c:
            case nChw8c:
                return mb * stride_mb + (c / blksize) * H * W * blksize
                        + h * W * blksize + w * blksize + c % blksize;
            case nchw: return mb * stride_mb + c * H * W + h * W + w;
            case nhwc: return mb * stride_mb + h * W * C + w * C + c;
            default:
                if (ndims >= 5) return data_d.off(mb, c, d, h, w);
                if (ndims >= 4) return data_d.off(mb, c, h, w);
                if (ndims >= 3) return data_d.off(mb, c, w);
                return data_d.off(mb, c);
        }
    };
#endif

    // pass by value due to icc170 and icc180 problem on KNL
    auto ker = [&](data_t * const d, dim_t const mb, dim_t const oc,
            dim_t const od, dim_t const oh, dim_t const ow) {
        acc_data_t sum = 0;
        data_t central = src[offset<tag>(data_d, mb,stride_mb, oc,C, od,D, oh,H, ow,W)];
        if (across_channels) {
            // half_size is quite small -- little opportunity for vectorization
            const dim_t c_st = nstl::max(oc - half_size + 0, (dim_t)0);
            const dim_t c_en = nstl::min(oc + half_size + 1, C);

            if (1) {
                // DENSE layouts and formula-driven vectorize well
                for (dim_t c = c_st; c < c_en; ++c) {
                    const acc_data_t s = src[offset<tag>(data_d, mb,stride_mb, c,C, od,D, oh,H, ow,W)];
                    sum += s * s;
                }
            }
        } else if (data_d.ndims()==2) { // trivial case should use completely different approach
            printf(" warning: TBD data_d.ndims()==2 fwd lrn_within_channel is a trivial case\n");
            sum = central * central;
        } else { // generic fwd lrn_within_channel
            assert(data_d.ndims() >= 3);
            assert(data_d.ndims() <= 5);
            dim_t d_st = nstl::max(od - half_size + 0, (dim_t)0);
            dim_t d_en = nstl::min(od + half_size + 1, D);
            dim_t h_st = nstl::max(oh - half_size + 0, (dim_t)0);
            dim_t h_en = nstl::min(oh + half_size + 1, H);
            dim_t w_st = nstl::max(ow - half_size + 0, (dim_t)0);
            dim_t w_en = nstl::min(ow + half_size + 1, W);

            if (tag == nchw || tag == nhwc || tag == nChw8c || tag == nChw16c) {
                // DENSE layouts and formula-driven vectorize well
                for_(dim_t d = d_st; d < d_en; ++d)
                for_(dim_t h = h_st; h < h_en; ++h)
                for (dim_t w = w_st; w < w_en; ++w) {
                    const acc_data_t s = src[offset<tag>(
                            data_d, mb,stride_mb, oc,C, d,D, h,H, w,W)];
                    sum += s * s;
                }
                // -lrn --tag=nchw --alg=WITHIN ic202ih10 --> 2.125 ms
                // -lrn --tag=nhwc --alg=WITHIN ic202ih10 --> 2.183
                // --lrn --tag=nChw16c --alg=WITHIN ic202ih10 --> 2.281
                // func call test
                // --lrn --tag=ncdhw --alg=WITHIN ic202id10 --> 1837.29
            } else if(1) {
                // test 1: CoordsFor vectorization helper, but still func call for offset.
                using dnnl::impl::CoordsFor;
                for(auto cf = CoordsFor<3,dim_t>{{d_st,h_st,w_st}, {d_en,h_en,w_en}};
                        cf; ++cf) {
                    // loop over this simd-length batch
                    auto const vl = cf.get_vl();
#if 1
                    for(unsigned i=0U; i<vl; ++i) {
                        const acc_data_t s = src[offset<tag>(
                                data_d, mb,stride_mb, oc,C,
                                cf.vp[0][i], D,
                                cf.vp[1][i], H,
                                cf.vp[2][i], W)];
                        sum += s * s;
                    }
#else // 1625.63 ms, not significantly faster
                    dim_t off[MVL];
                    for(unsigned i=0U; i<vl; ++i) {
                        off[i] = offset<tag>(
                                data_d, mb,stride_mb, oc,C,
                                cf.vp[0][i], D,
                                cf.vp[1][i], H,
                                cf.vp[2][i], W);
                    }
                    for(unsigned i=0U; i<vl; ++i) {
                        const acc_data_t s = src[off[i]];
                        sum += s * s;
                    }
#endif
                }
                // generic 3d CoordsFor
                // -lrn --tag=nchw --alg=WITHIN ic202ih10 --> 7.25 ms
                // --lrn --tag=nhwc --alg=WITHIN ic202ih10 --> 7.46
                // --lrn --tag=nChw16c --alg=WITHIN ic202ih10 --> 7.100
                // func call test
                // --lrn --tag=ncdhw --alg=WITHIN ic202id10 --> 1638.88
            }
#if 0
            else {
                // test 1: CoordsForNd and vec_off_v, vectorized offsets calc
                // Here we have iterator helper with run-time dimensionality.
                //using dnnl::impl::CoordRegs<uint64_t,6>; // match memory_desc_wrapper_opt::VecPos
                typedef CoordsForNd<6,uint64_t,uint64_t> Coords;
                auto cf = (data_d.ndims() >= 5)? Coords.mk(mb,mb+1, c,c+1, d_st,d_en, h_st,h_en, w_st,w_en)
                : (data_d.ndims() >= 4)? Coords.mk(mb,mb+1, c,c+1, h_st,h_en, w_st,w_en)
                : /*(data_d.ndims() >= 3)?*/ cf.init(mb,mb+1, c,c+1, w_st,w_en);
                for ( ; cf; ++cf) {
                    dim_t off[MVL];
                    for(unsigned i=0U; i<vl; ++i) {
                        off[i] = data_d.off(
                                data_d, mb,stride_mb, oc,C,
                                cf.vp[0][i], D,
                                cf.vp[1][i], H,
                                cf.vp[2][i], W);
                    }
                    for(unsigned i=0U; i<vl; ++i) {
                        const acc_data_t s = src[off[i]];
                        sum += s * s;
                    }
                }
            }
#endif
        }
        sum = k + alpha * sum / summands;
        d[0] = static_cast<data_t>(central * fast_negative_powf(sum, beta));
    };

#define NOVEC PragmaQuote(_NEC novector);
#define FOR_CHAN for(int i=0; i<C; ++i)

#if 0
    auto ker2 = [&](dim_t const dst_off, dim_t const mb, dim_t const oc,
            dim_t const od, dim_t const oh, dim_t const ow) {
        acc_data_t sum = 0;
        data_t central = src[offset<tag>(data_d, mb,stride_mb, oc,C, od,D, oh,H, ow,W)];
        // half_size is quite small -- little opportunity for vectorization
        const dim_t c_st = nstl::max(oc - half_size + 0, (dim_t)0);
        const dim_t c_en = nstl::min(oc + half_size + 1, C);

        for (dim_t c = c_st; c < c_en; ++c) {
            const acc_data_t s = src[offset<tag>(data_d, mb,stride_mb, c,C, od,D, oh,H, ow,W)];
            sum += s * s;
        }

        sum = k + alpha * sum / summands;
        dst[dst_off] = static_cast<data_t>(central * fast_negative_powf(sum, beta));
    };

    auto ker3 = [&](dim_t const mb, dim_t const oc,
            dim_t const od, dim_t const oh, dim_t const ow) {
        const dim_t dst_off = offset<tag>(data_d, mb,stride_mb, oc,C, od,D, oh,H, ow,W);
        acc_data_t sum = 0;
        //data_t central = src[offset<tag>(data_d, mb,stride_mb, oc,C, od,D, oh,H, ow,W)];
        data_t central = src[dst_off];
        if (across_channels) {
            // half_size is quite small -- little opportunity for vectorization
            const dim_t c_st = nstl::max(oc - half_size + 0, (dim_t)0);
            const dim_t c_en = nstl::min(oc + half_size + 1, C);

            for (dim_t c = c_st; c < c_en; ++c) {
                const acc_data_t s = src[offset<tag>(data_d, mb,stride_mb, c,C, od,D, oh,H, ow,W)];
                sum += s * s;
            }

        } else {
            // this would require D*H*W*(2*half_size+1) offsets
            // OR calculate them with a "modernized" off_v_vec routine
            // OR look into 'for_nd'
            // ---> version that:
            //          nd_block_init(MVL,0,D,0,W,0,H) blocks nested loops by MVL
            //          with offsets[dims][MVL] intermediate outputs,
            //          ready to pass into existing off_v_vec
            // Pseudocode:
            //    auto blk = nd_block_init(MVL,0,D,0,W,0,H) simple block-by-MVL loops
            // or auto blk = nd_block_init(MVL,d_st,d_en,D,w_st,w_en,W,h_st,h_en,H)
            //    for(int b=0; b<blk.blocks(); ++b){
            //       auto const& coords = blk.next();
            //       auto offsets = mdw_opt::off_v_vec(blk.len(), coords,);
            //    }
            dim_t d_st = nstl::max(od - half_size + 0, (dim_t)0);
            dim_t d_en = nstl::min(od + half_size + 1, D);
            dim_t h_st = nstl::max(oh - half_size + 0, (dim_t)0);
            dim_t h_en = nstl::min(oh + half_size + 1, H);
            dim_t w_st = nstl::max(ow - half_size + 0, (dim_t)0);
            dim_t w_en = nstl::min(ow + half_size + 1, W);

            for_(dim_t d = d_st; d < d_en; ++d)
            for_(dim_t h = h_st; h < h_en; ++h)
            for (dim_t w = w_st; w < w_en; ++w) {
                const acc_data_t s = src[offset<tag>(
                        data_d, mb,stride_mb, oc,C, d,D, h,H, w,W)];
                sum += s * s;
            }
        }
        sum = k + alpha * sum / summands;
        dst[dst_off] = static_cast<data_t>(central * fast_negative_powf(sum, beta));
    };
#endif

#ifndef MVL
#if defined(__ve)
#define MVL 256
#else
#define MVL 32
#endif
#endif
#if 0 // cannot capture var-len array into lambda :(
    // calc once, re-use:
    dim_t c_st[C];
    dim_t c_en[C];
    if (across_channels && tag==nchw || tag==nhwc) {
        for(int i=0; i<C; ++i) {
            c_st[i] = nstl::max(i - half_size + 0, (dim_t)0);
            c_en[i] = nstl::min(i + half_size + 1, C);
        }
    }
#elif 0 // for ker_across_vec_blk
    dim_t c_st[MVL];
    dim_t c_en[MVL];
    if (C <= MVL && across_channels && tag==nchw || tag==nhwc) {
        for(int i=0; i<C; ++i) {
            c_st[i] = nstl::max(i - half_size + 0, (dim_t)0);
            c_en[i] = nstl::min(i + half_size + 1, C);
        }
    }
#endif
    // vectorized over oc_st..oc_en, vl<=MVL
    //auto ker_across_vec = [&src,&dst,&stride_mb,&C,&D,&H,&W,&half_size,&k,&alpha,&beta,&summands](
    auto ker_across_vec = [&](
            size_t * const dst_off /*[0..C-1]*/,
            dim_t const mb, // internally c=0..C-1
            dim_t const od, dim_t const oh, dim_t const ow) {
        acc_data_t sum[C];
        FOR_CHAN sum[i]= 0;
        FOR_CHAN {
            dim_t c_st = nstl::max(i - half_size + 0, (dim_t)0);
            dim_t c_en = nstl::min(i + half_size + 1, C);
            for(dim_t c = c_st; c < c_en; ++c) {
                const acc_data_t s = src[dst_off[c]];
                sum[i] += s * s;
            }
        }
        FOR_CHAN sum[i] = k + alpha * sum[i] / summands;
        FOR_CHAN dst[dst_off[i]] = static_cast<data_t>( src[dst_off[i]]
                    * fast_negative_powf(sum[i], beta));
    };

    /** When func call to memory_desc_wrapper cannot be avoided,
     * use vectorizable extension, \c data_opt. */
    auto channel_offsets = []( memory_desc_wrapper_opt const& data_opt,
            size_t * const data_off, dim_t const mb, dim_t const clo, dim_t const chi,
            dim_t const d, dim_t const h, dim_t const w) {
        assert(!data_opt.is_zero());
        typedef CoordsForNd<6,uint64_t,uint64_t> Coords;
        auto const dm = data_opt.ndims();
        auto cf = (dm >= 5)? Coords::mk(mb,mb+1, clo,chi, d,d+1, h,h+1, w,w+1)
            : (dm >= 4)? Coords::mk(mb,mb+1, clo,chi, h,h+1, w,w+1)
            : (dm >= 3)? Coords::mk(mb,mb+1, clo,chi, w,w+1)
            : Coords::mk(mb,mb+1, clo,chi); // dm>=2 ?
        for ( ; cf; ++cf) { // channel coords in simd-length chunks
            data_opt.vec_off_v(
                    cf.base(), // VecPos& vector register data
                    (dim_t*)&data_off[cf.get_pos()], // outputs
                    cf.get_vl(), // register len, num ouptuts
                    false/*is_pos_padded*/);
        }
    };

    auto ker_across_vec_lapped = [&](
            dim_t const clo, dim_t const chi, // 'central' channels range
            size_t * const dst_off, // now from max(0,clo-half_size) to min(chi+half_size+1,C)
            // but &dst_off[0] corresponds to 'clo'  (small -ve offsets possible)
            dim_t const mb, // internally c=0..C-1
            dim_t const od, dim_t const oh, dim_t const ow) {
        dim_t const cspan = chi - clo;
        dim_t const clolo = nstl::max(clo - half_size + 0, (dim_t)0); // corresp. dst_off[0]
        acc_data_t sum[cspan];
        for(int c=0; c<cspan; ++c) sum[c]= 0;
        for(int c=clo; c<chi; ++c) { // c ~ central channel
            dim_t c_st = nstl::max(c - half_size + 0, (dim_t)0);
            dim_t c_en = nstl::min(c + half_size + 1, C);
            for(dim_t k = c_st; k < c_en; ++k) { // k ~ lrn window
                const acc_data_t s = src[dst_off[k-clolo]]; // lrn window datum
                sum[c-clo] += s * s; // accum central sum
            }
        }
        for(int c=0; c<cspan; ++c) sum[c] = k + alpha * sum[c] / summands;
        // offset data for 'central' region, fwd by up to half_size
        size_t * central_off = dst_off + (clo - clolo);
        for(int c=0; c<cspan; ++c)
            dst[central_off[c]] = static_cast<data_t>(
                    src[central_off[c]] * fast_negative_powf(sum[c], beta));
    };


#define KER_ACROSS_VEC(dst_off,mb,od,oh,ow) do { \
    acc_data_t sum[C]; \
    FOR_CHAN sum[i]= 0; \
    FOR_CHAN { \
        dim_t c_st = nstl::max(i - half_size + 0, (dim_t)0); \
        dim_t c_en = nstl::min(i + half_size + 1, C); \
        for(dim_t c = c_st; c < c_en; ++c) { \
            const acc_data_t s = src[dst_off[c]]; \
            sum[i] += s * s; \
        } \
    } \
    FOR_CHAN sum[i] = k + alpha * sum[i] / summands; \
    FOR_CHAN dst[dst_off[i]] = static_cast<data_t>( src[dst_off[i]] \
            * fast_negative_powf(sum[i], beta)); \
}while(0)

    const dim_t MB = pd()->MB();
    if (across_channels && C >= size && C <= 32768 ) {
        // vectorize across channels, using fast offsets calculated on stack.
        //
        // limit range of C to bound stack space
        //  - or use scratchpad
        //  - or half-size-overlapped ker_across_vec_blocked
        // common impl, vectorizing over c=0..C
        //
        // formulas vectorize well
        // --lrn --tag=nchw ic202ih10 0.26416
        // --lrn --tag=nhwc ic202ih10 0.26416
        // --lrn --tag=nChw16c ic202ih10 0.327148
        // --lrn --tag=ncdhw ic202id10 19.2898      <-- offset<tag> is a func call
        // func class with some help:
        // --lrn --tag=ncdhw ic202id10 2.85 ms (vec_off_v ~ 7x faster)
        // --lrn --tag=ncdhw ic202id10 2.66 ms w/ hi==lo+1 optimization
        parallel_nd(MB, D, H, W, [&](dim_t mb, dim_t d, dim_t h, dim_t w) {
                size_t data_off[C];
                if (formula) {
                    for(unsigned c=0U; c<C; ++c) data_off[c] = offset<tag>(
                            data_d, mb,stride_mb, c,C, d,D, h,H, w,W );
                } else { // function call phys offset needs a little help to vectorize
                    channel_offsets(data_opt, data_off, mb, 0, C, d, h, w);
                }
                ker_across_vec(&data_off[0], mb, /*c,*/ d, h, w);
            });
        // XXX optimizing fwd lrn "across" for C>32768 possible too, but harder
    } else if (1 && across_channels && C > 32768 ) {
        // --lrn --tag=nchw ic32777ih10 33.3181 (improved from below code 246.64 ms)
        // --lrn --tag=ncdhw ic32777id10 3045.35 (func call improve from 24733.4, w/o vec_off_v
        // Large channels: split over threads, and overlap offset calcs,
        // because lrn window can extend half_size past the central range. */
        dim_t nblks = C/16384; // just a rough estimate
        dim_t blksz = utils::div_up(C, nblks); // can avoid a small "tail" chunk
        // XXX does balance211 do this?
        parallel_nd(MB, utils::div_up(C, blksz), D, H, W,
                [&](dim_t mb, dim_t c_blk, dim_t d, dim_t h, dim_t w) {
            dim_t clo = c_blk * blksz;
            dim_t chi = nstl::min(clo + blksz, C);
            // lrn kernel extends up to half_size past [clo,chi)
            dim_t clolo = nstl::max(clo - half_size, dim_t{0});
            dim_t chihi = nstl::min(chi + half_size + 1, C);
            dim_t offset_span = chihi - clolo;
            size_t data_off[offset_span];
            if(formula) {
                for(unsigned c=clolo; c<chihi; ++c){
                    data_off[c-clolo] = offset<tag>(data_d, mb,stride_mb,
                            c,C, d,D, h,H, w,W );
                }
            }else{ // vec_off_v phys offset calc in simd-length chunks
                channel_offsets(data_opt, data_off, mb, clolo, chihi, d, h, w);
            }
            ker_across_vec_lapped( clo, chi, &data_off[0], mb, /*c,*/ d, h, w);
            });
    } else {
        // --lrn --tag=nchw ic32777ih10 246.63 ms
        // --lrn --tag=ncdhw ic32777id10 24733.4
        // fall back to across/within vectorized only across lrn size (typically 5)
        if (tag == nChw16c || tag == nChw8c) {
            parallel_nd(MB, utils::div_up(C, blksize), H, W,
                    [&](dim_t mb, dim_t c_blk, dim_t h, dim_t w) {
                    dim_t c = c_blk * blksize;
                    const dim_t off = offset<tag>(data_d, mb,stride_mb, c,C, 0,1, h,H, w,W);
                    PRAGMA_OMP_SIMD()
                    for (dim_t cc = 0; cc < nstl::min(blksize, C - c); ++cc)
                    ker(&dst[off + cc], mb, c + cc, 0, h, w);
                    });
        } else if (tag == nchw || tag == nhwc) {
#if 0 // only slgihtly better vectorization (not worth code lines)
            if (across_channels /*&& C >= size*/ ) {
                parallel_nd(MB, H, W, [&](dim_t mb, dim_t h, dim_t w) {
                        size_t dst_off[C];
#if 1
                        if (tag == nchw ) FOR_CHAN 
                        dst_off[i] = mb * stride_mb + i * H * W + h * W + w;
                        else if (tag == nhwc ) FOR_CHAN
                        dst_off[i] = mb * stride_mb + h * W * C + w * C + i;
#else
                        FOR_CHAN dst_off[i] = offset<tag>(data_d, mb,stride_mb,
                                i,C, 0,1, h,H, w,W );
#endif
                        ker_across_vec( &dst_off[0], mb, /*c=0..vl-1,*/ 0, h, w);
                });
            }else
#endif
            {
                parallel_nd(MB, C, H, W, [&](dim_t mb, dim_t c, dim_t h, dim_t w) {
                        const dim_t off = offset<tag>(data_d, mb,stride_mb, c,C, 0,1, h,H, w,W);
                        ker(&dst[off], mb, c, 0, h, w);
                        });
            }
        } else {
            {
                parallel_nd(MB, C, D, H, W,
                        [&](dim_t mb, dim_t c, dim_t d, dim_t h, dim_t w) {
                        const dim_t off = offset<tag>(data_d, mb,stride_mb, c,C, d,D, h,H, w,W);
                        ker(&dst[off], mb, c, d, h, w);
                        });
            }
        }
    }
}

// Backward LRN formula (refer to Forward LRN formula):
// Partial derivatives:
// dy_i/dx_j =         - 2*a*b/n * x_i * O(i)^-b / O(i) * x_j, i != j
//             O(i)^-b - 2*a*b/n * x_i * O(i)^-b / O(i) * x_j, i == j, where
// O(i) = (k + a / n * Sum:j [x_j^2]), j in [i - n/2, i + n/2]. Note: j depends
//     on i, which means that O(i) may use more points than local_size.
// Now, z_i = Sum:k [dE/dy_k * dy_k/dx_j], where k in [i - n/2, i + n/2]
//     for ACROSS. 2d-shape for WITHIN.
// Then, dE/dy_k = diffDst_k. Finally,
// z_i = Sum:k [dd_k * dy_k/dx_j] = A - B (code variables) =
//     = dd_i * O(i)^-b - 2*a*b/n * x_i * Sum:k {O(k)^-b / O(k) * x_k * dd_k};

template <impl::data_type_t d_type>
template <dnnl_format_tag_t tag>
void ref_lrn_bwd_t<d_type>::execute_backward(const exec_ctx_t &ctx) const {
    using namespace alg_kind;
    using namespace format_tag;

    auto src = CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
    auto diff_dst = CTX_IN_MEM(const data_t *, DNNL_ARG_DIFF_DST);
    auto diff_src = CTX_OUT_MEM(data_t *, DNNL_ARG_DIFF_SRC);

    const memory_desc_wrapper data_d(pd()->src_md());

    const dim_t C = pd()->C();
    const dim_t D = pd()->D();
    const dim_t H = pd()->H();
    const dim_t W = pd()->W();
    const auto stride_mb = data_d.blocking_desc().strides[0];
    const bool across_channels = pd()->desc()->alg_kind == lrn_across_channels;
    static constexpr dim_t blksize = tag == nChw16c ? 16 : 8;
    const auto ndims = data_d.ndims();

    auto compute_n_summands = [&](dim_t size) {
        if (across_channels) {
            return size;
        } else { // within_channel
            dim_t n_summands = 1;
            for (auto d = ndims - 2; d > 0; --d)
                n_summands *= size;
            return n_summands;
        }
    };

    const acc_data_t alpha = static_cast<acc_data_t>(pd()->desc()->lrn_alpha);
    const acc_data_t beta = static_cast<acc_data_t>(pd()->desc()->lrn_beta);
    const acc_data_t k = static_cast<acc_data_t>(pd()->desc()->lrn_k);
    const dim_t size = pd()->desc()->local_size;
    const dim_t half_size = (size - 1) / 2;
    const dim_t summands = compute_n_summands(size);

    // REMOVED for VE: pass by value due to icc170 and icc180 problem on KNL
    auto get_omega = [&](dim_t const mb, dim_t const oc, dim_t const od,
            dim_t const oh, dim_t const ow) {
        acc_data_t sum = 0;
        if (across_channels) {
            const dim_t c_st = nstl::max(oc - half_size + 0, (dim_t)0);
            const dim_t c_en = nstl::min(oc + half_size + 1, C);

            for (dim_t c = c_st; c < c_en; ++c) {
                const acc_data_t s = src[offset<tag>(data_d, mb,stride_mb,
                        c,C, od,D, oh,H, ow,W)];
                sum += s * s;
            }
        } else {
            dim_t d_st = nstl::max(od - half_size + 0, (dim_t)0);
            dim_t d_en = nstl::min(od + half_size + 1, D);
            dim_t h_st = nstl::max(oh - half_size + 0, (dim_t)0);
            dim_t h_en = nstl::min(oh + half_size + 1, H);
            dim_t w_st = nstl::max(ow - half_size + 0, (dim_t)0);
            dim_t w_en = nstl::min(ow + half_size + 1, W);
            for_(dim_t d = d_st; d < d_en; ++d)
            for_(dim_t h = h_st; h < h_en; ++h)
            for (dim_t w = w_st; w < w_en; ++w) {
                const acc_data_t s = src[offset<tag>(
                        data_d, mb,stride_mb, oc,C, d,D, h,H, w,W)];
                sum += s * s;
            }
        }
        return (acc_data_t)(k + alpha * sum / summands);
    };

    auto ker = [&](data_t * const d, dim_t const mb, dim_t const oc,
            dim_t const od, dim_t const oh, dim_t const ow) {
        acc_data_t A = 0, B = 0;
        data_t central = src[offset<tag>(data_d, mb,stride_mb, oc,C, od,D, oh,H, ow,W)];
        if (across_channels) {
            const dim_t c_st = nstl::max(oc - half_size + 0, (dim_t)0);
            const dim_t c_en = nstl::min(oc + half_size + 1, C);

            for (dim_t c = c_st; c < c_en; c++) {
                const auto off = offset<tag>(data_d, mb,stride_mb,
                        c,C, od,D, oh,H, ow,W);
                const acc_data_t omega = get_omega(mb, c, od, oh, ow);
                const acc_data_t omega_in_beta
                        = fast_negative_powf(omega, beta);
                const acc_data_t tmp
                        = omega_in_beta * (acc_data_t)diff_dst[off];
                if (c == oc) A = tmp;
                B += (src[off] * tmp / omega);
            }
        } else {
            dim_t d_st = nstl::max(od - half_size + 0, (dim_t)0);
            dim_t d_en = nstl::min(od + half_size + 1, D);
            dim_t h_st = nstl::max(oh - half_size + 0, (dim_t)0);
            dim_t h_en = nstl::min(oh + half_size + 1, H);
            dim_t w_st = nstl::max(ow - half_size + 0, (dim_t)0);
            dim_t w_en = nstl::min(ow + half_size + 1, W);
            for_(dim_t d = d_st; d < d_en; ++d)
            for_(dim_t h = h_st; h < h_en; ++h)
            for (dim_t w = w_st; w < w_en; ++w) {
                const auto off = offset<tag>(data_d, mb,stride_mb,
                        oc,C, d,D, h,H, w,W);
                const acc_data_t omega = get_omega(mb, oc, d, h, w);
                const acc_data_t omega_in_beta
                        = fast_negative_powf(omega, beta);
                const acc_data_t tmp
                        = omega_in_beta * (acc_data_t)diff_dst[off];
                if (d == od && h == oh && w == ow) A = tmp;
                B += (src[off] * tmp / omega);
            }
        }
        B *= (2.0f * alpha * beta * central / summands);
        *d = static_cast<data_t>(A - B);
    };

    const dim_t MB = pd()->MB();
    if (tag == nChw16c || tag == nChw8c) {
        parallel_nd(MB, utils::div_up(C, blksize), H, W,
                [&](dim_t mb, dim_t c_blk, dim_t h, dim_t w) {
                    dim_t c = c_blk * blksize;
                    //const dim_t off = mb * stride_mb + c * H * W
                    //        + (h * W + w) * blksize;
                    const dim_t off = offset<tag>(data_d, mb,stride_mb, c,C, 0,1, h,H, w,W);
                    PRAGMA_OMP_SIMD()
                    for (dim_t cc = 0; cc < nstl::min(blksize, C - c); ++cc)
                        ker(&diff_src[off + cc], mb, c + cc, 0, h, w);
                });
    } else if (tag == nhwc || tag == nchw) {
        parallel_nd(MB, H, W, C, [&](dim_t mb, dim_t h, dim_t w, dim_t c) {
            const dim_t off = offset<tag>(data_d, mb,stride_mb, c,C, 0,1, h,H, w,W);
            ker(&diff_src[off], mb, c, 0, h, w);
        });
    } else {
        parallel_nd(MB, C, D, H, W,
                [&](dim_t mb, dim_t c, dim_t d, dim_t h, dim_t w) {
                    const dim_t off = offset<tag>(data_d, mb,stride_mb, c,C, d,D, h,H, w,W);
                    ker(&diff_src[off], mb, c, d, h, w);
                });
    }
}

template void
ref_lrn_fwd_t<data_type::f32>::execute_forward<format_tag::nChw16c>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_fwd_t<data_type::f32>::execute_forward<format_tag::nChw8c>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::f32>::execute_forward<format_tag::nchw>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::f32>::execute_forward<format_tag::nhwc>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::f32>::execute_forward<format_tag::any>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::f32>::execute_backward<format_tag::nChw16c>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::f32>::execute_backward<format_tag::nChw8c>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_bwd_t<data_type::f32>::execute_backward<format_tag::nchw>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_bwd_t<data_type::f32>::execute_backward<format_tag::nhwc>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_bwd_t<data_type::f32>::execute_backward<format_tag::any>(
        const exec_ctx_t &ctx) const;

template void
ref_lrn_fwd_t<data_type::bf16>::execute_forward<format_tag::nChw16c>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_fwd_t<data_type::bf16>::execute_forward<format_tag::nChw8c>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::bf16>::execute_forward<format_tag::nchw>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::bf16>::execute_forward<format_tag::nhwc>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_fwd_t<data_type::bf16>::execute_forward<format_tag::any>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::bf16>::execute_backward<format_tag::nChw16c>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::bf16>::execute_backward<format_tag::nChw8c>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::bf16>::execute_backward<format_tag::nchw>(
        const exec_ctx_t &ctx) const;
template void
ref_lrn_bwd_t<data_type::bf16>::execute_backward<format_tag::nhwc>(
        const exec_ctx_t &ctx) const;
template void ref_lrn_bwd_t<data_type::bf16>::execute_backward<format_tag::any>(
        const exec_ctx_t &ctx) const;

} // namespace cpu
} // namespace impl
} // namespace dnnl

// vim: et ts=4 sw=4 cindent cino=+s,l0,\:4,N-s
