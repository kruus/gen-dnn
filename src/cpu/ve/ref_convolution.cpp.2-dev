/*******************************************************************************
* Copyright 2016-2020 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

#define FWD_IMPL 6
// errors:
// 0 : rconv-0.log OK, mistrusted 3 of conv.in mode=C Real Time 17.934
// 1 : rconv-1.log seg fault in --conv g32ic32ih112oc32oh112kh3ph1n"mobilenet:conv2_1/dw"
// 1 : rconv-1.log OK RTime 15.98 s
// 2 : rconv-2.log OK 15.72 s
// 3 : rconv-3.log OK 15.43 s
// 4 : rconv-4.log OK 15.23 s
// 5 : rconv-5.log OK  3.52 s
// 6 : rconv-6.log OK  2.42 s
#define ELT_SCALAR (FWD_IMPL>=3)
// conv-gemm.in
// gemm :       190,213,145,265 (191,194,120)
//   add --skip-impl=gemm to run ref impls?
//   NO. comment them out in cpu_convolution_list.cpp
//   Perhaps benchdnn should use iterator api to continue trying
//   to find a non-skipped impl.
//
//   Cannot skip gemm convolution and still run the ref impl (lower) !)
// conv.in (much smaller tests when no gemm impl present)
// -1 : 48.5984 75.4461 68.4999 107.548 31.2228 24.5472 25.612  24.4907 203.698
//  0 : 48.6771 84.7707 87.7413 108.005 40.8471 24.7904 25.6382 24.2934 215.264
//  1 : xx.5149 51.7053 43.977  73.167  39.4347 18.7586 18.9639 18.6456 132.379
//  2 : 32.7245 40.2169 38.188  63.3137 36.6195 16.373  16.5362 16.4141 102.588
//  3 : 31.4836 38.6193 31.9966 59.813  32.5218 15.7574 15.9223 15.7973 95.3827
//  --- add a FWD_D test
//  3 : 31.3331 38.3888 31.7553 59.4035 59.3386 32.1725 15.4924 15.4991 15.491 92.2239
//  4 : 30.8901 37.9487 31.4165 58.203 58.1731 32.5846 15.2864 15.2907 15.2818 89.9907
// 4 : 30.9838 39.6846 32.9677 58.3858 58.3249 32.5824 15.3221 15.3263 15.3218 89.7068
// fix test_convolution_forward_u8s8s32 and test_convolution_eltwise_forward_x8s8f32s32 bugs
// for "most advanced" of each impl ...
// ...........................................
//  3     : 41.21 47.58 39.59 76.27 76.19
//        : 14477 3208. 190.8 18254 18259 34.30 postops-plain: 20.47 20.47 20.47 120.1
//  4plain: 38.61 42.98 38.35  68.23 68.16
//     any: 13528 3081. 182.6 17049 17050 33.01 postops-plain: 19.09 19.11 19.10 103.5
//  5     : 41.06 46.42 39.19 75.12 75.06  3.38 postops-plain: 20.33 20.34 20.33 118.1
//     any: 377.6 168.4 144.3 524.2 524.1 
//  6plain: 42.34 50.19 44.38  79.06 79.03
//          179.1 92.30 72.30 256.04 255.92 (wcrd for inner)
//  6any  : 240.82 86.59 66.90 337.87 337.93 1.70 postops-plain 20.91 20.96 20.87 125.91
//        : 233.44 78.32 55.64 321.82 321.73 1.51 postops-plain 87.41 87.41 87.41,628.67
//   --> use wcrd only for generic kernel (vectorize offset calcs)
//       because ker_plain6 vectorizes well with nc++
//   --> revert to 4 for ker_plain? what is the main diff?
//6:-1 : 43.82 54.13 44.51 83.62  83.58
//  any: 248.5 85.25 58.90 341.5 332.45 1.63 postops-plain: 21.69 21.71 21.71 135.1
//6:0  : 76.53 78.86 60.23 140.6 140.5 
//  any: 233.7 79.04 57.25 322.4 322.2 1.52 postops-plain: 37.77 37.77 37.77 264.3
//6:1  : 173.1 96.14 77.00 252.1 251.1
//  any: 233.6 79.36 57.51 321.9 321.8 1.51 postops-plain 85.06 85.55 84.42 616.0
//6:2  : 181.4 100.6 78.92 259.2 250.2 259.2
//  any: 233.7 78.35 56.91 322.0 321.9 1.51 postops-plain 88.61 88.46 88.61 637.3
//  --> use 4 for plain, and 6:1 for small improvement of 'any'
//6:0 with unsigned IKLIMS macro: (above used div_floor)
//       40.78 45.75 38.11 74.46 74.41
//       235.9 81.27 57.63 325.6 325.6 1.57 postops-plain: 20.19 10.19 10.19 116.5
// Enabling more IKLIMS test cases, I find that NOT using the fn call leads to
// miscompilation !!!  Only hoist_ApiB(...) fn call avoided all segfaults.
#if FWD_IMPL==-1
#include "cpu/ve/ref_convolution_fwd_orig.ipp"
#else

#include "cpu/ve/ref_convolution_util.hpp"
#include "cpu/ve/hoist.hpp"     // nc++: hoist linear conditions out of loops
#include <iostream> // tmp debug


#if FWD_IMPL>0
#include "common/ve/memory_desc_wrapper_opt.hpp"
#define LISTVEC PragmaQuote(_NEC list_vector)
#endif // FWD_IMPL

static int verbose=0; // 0 = print all, increase to reduce verbosity
static int file_errors=0;
static bool chk( bool cond, char const* msg, char const* file, int const lineno ){
    if (!cond){
        printf("@@@ error: %s : [%s:%d]\n", msg, file, lineno);
        ++file_errors;
        //exit(1);
    }
    return cond;
}
static bool trivial( int const verb, bool const cond, char const* msg,
                             char const* file, int const lineno ){
        if (verb > verbose && cond){
                    printf("@@@ trivial: %s : [%s:%d]\n", msg, file, lineno); fflush(stdout);
                        }

            return cond;
}
//#define MUST( COND )
#define MUST( COND )    chk(    (COND), #COND, __PRETTY_FUNCTION__, __LINE__)
#define PRINTF(...)     do{ printf(__VA_ARGS__); fflush(stdout);}while(0)
#define TRIVIAL( COND ) COND
//#define TRIVIAL( COND ) trivial(1, (COND), #COND, __PRETTY_FUNCTION__, __LINE__)

#if defined(NDEBUG)
#define DPRINTF(...)
#define DMUST(...) 1
#else
#define DPRINTF(...)  do{printf(__VA_ARGS__); fflush(stdout);}while(0)
#define DMUST(...)    MUST(__VA_ARGS__)
#endif

inline ALWAYS_INLINE void hoist_ApiBx(
        int& ilo, int& ihi,                     // sub-for-loop outputs
        const int ibeg, const int iend,         // orig for(i=ibeg;i<iend;++i) <-- stride 1
        const int a,    const int b,            // linear fn o=a+ib
        const int obeg, const int oend)         // linear fn range [obeg,oend)
{
    // div_floor approach for int args, not the unsigned generalization
    int err = 0;
    if(!MUST( b > 0 )) ++err;
    ilo = div_floor( obeg -a+b-1, b );
    if(!MUST( a + (ilo-1) * b < obeg )) ++err;
    if(!MUST( a + (ilo  ) * b >= obeg )) ++err;
    ihi = div_floor( oend -a+b-1, b );
    if(!MUST( a + (ihi-1) * b < oend )) ++err;
    if(!MUST( a + (ihi  ) * b >= oend )) ++err;
    if(err) printf(" hoist err: ibeg,iend=%d,%d  a,b=%d,%d  obeg,oend=%d,%d  ilo=%d ihi=%d\n",
            ibeg,iend,a,b,obeg,oend,ilo,ihi);
    if( ilo < ibeg ) ilo = ibeg;
    else if( ilo > iend ) ilo = iend; // intentionally NOT enforced
    if( ihi >= iend ) ihi = iend;
    else if( ihi < ibeg ) ihi = ibeg; // intentionally NOT enforced
}
#define CONV_CHECK(EXPR) do {if (!(EXPR)) {printf(" FAILED: " #EXPR "\n");}} while(0)

// vector window-tile access macros
#define DHW_W 0
#define DHW_H 1
#define DHW_D 2
// which = 0,1,2 for w, h, d respectively
// following COULD be done reasonably, but nc++ precalculates
// most pointers, using mem load instead "lea" calc (could use single reg).
// so still fair amount of register spill,restore and mem loads
#define DHW_SHIFT(which,idx)    dhw[((which*3  )*MVL)+idx]
#define DHW_ST(which,idx)       dhw[((which*3+1)*MVL)+idx]
#define DHW_EN(which,idx)       dhw[((which*3+2)*MVL)+idx]

#define DEFINE_IN_RANGE_VEC(idx, which, ksz, isz) \
    DHW_ST(which,idx) = (DHW_SHIFT(which,idx)       >   0 \
            ? DHW_SHIFT(which,idx): 0); \
    DHW_EN(which,idx) = (DHW_SHIFT(which,idx) + ksz < isz \
            ? DHW_SHIFT(which,idx) + ksz: isz); \
    if (DHW_EN(which,idx) < DHW_ST(which,idx)) \
            DHW_EN(which,idx) = DHW_ST(which,idx)

// macro version of window_precalc
#define POOL_WINDOW_LOOP(d_spatial, dvl, STRIDE, PAD, KSZ, ISZ, p_off, p_st, p_en, ssz) do \
{ \
    ShortLoop() for(int i=0; i<dvl; ++i) { \
        p_off[i] = d_spatial[i]/*od or oh or ow*/ * STRIDE - PAD; \
        p_st[i] = (p_off[i] > 0? p_off[i]: 0); \
        p_en[i] = (p_off[i] + KSZ < ISZ? p_off[i] + KSZ: ISZ); \
        if (p_en[i] < p_st[i]) p_en[i] = p_st[i]; \
        ssz[i] *= p_en[i] - p_st[i]; \
    } \
} while(0)

// alternate precalc loop, that backward avg-pooling really prefers !

#define VFOR(VAR,LIM) ShortLoop() for(int VAR = 0; VAR < LIM; ++VAR)
#define PRAG(...) PragmaQuote(_NEC __VA_ARGS__)

// expects SD,KD,ID,padF, etc int consts defined as usual
// dcrd info passed in as dm~dcrd.get_dim() dvl~dcrd.get_vl(),
// and d_spatial0 ~ (int32_t*)&dcrd.vp[2][0].
#define POOL_WINDOW_PRECALC(dm, dvl, d_spatial0, ssz, dhw) do \
{ \
    int const* d_spatial = d_spatial0; \
    int * p_off; /* input coord offset (from destination coord) */ \
    int * p_st; /* input coord start */ \
    int * p_en; /* input coord end, >= start*/ \
    ShortLoop() for(int i=0; i<dvl; ++i) \
        ssz[i] = 1; \
    if (dm >= 5) { \
        p_off = dhw + 6*MVL; \
        p_st  = dhw + 7*MVL; \
        p_en  = dhw + 8*MVL; \
        POOL_WINDOW_LOOP(d_spatial, dvl, SD, padF, KD, ID, \
                p_off, p_st, p_en, ssz); \
        d_spatial += MVL; /*dcrd.MaxVl*/ \
    } \
    if (dm >= 4) { \
        p_off = dhw + 3*MVL; \
        p_st  = dhw + 4*MVL; \
        p_en  = dhw + 5*MVL; \
        POOL_WINDOW_LOOP(d_spatial, dvl, SH, padT, KH, IH, \
                p_off, p_st, p_en, ssz); \
        d_spatial += MVL; /*dcrd.MaxVl*/ \
    } \
    p_off = dhw + 0*MVL; \
    p_st  = dhw + 1*MVL; \
    p_en  = dhw + 2*MVL; \
    POOL_WINDOW_LOOP(d_spatial, dvl, SW, padL, KW, IW, \
            p_off, p_st, p_en, ssz); \
} while(0)

// SCRD_LIMITS macro uses POOL_WINDOW_PRECALC constants to set up
// iterator limits for pooling window overlapped with src
//
// pack up the i'th overlapped pooling window limits into rlo,rhi vectors.
// mb_ptr ~ &dcrd.vp[0][0]
// dhw ~ precalc offset,start,end vector data for ovlp window limits
// ssz ~ precalc window num-elements
// dm ~ dimension (3,4,5)
// scrd ~ (output) initialized CONV WINDOW src-coordinate-iterator mb,icxg[,d[,h]],w
// wcrd ~ [g,]oc,ic[,kd,[,kh]],kw
#define SCRD_LIMITS(i, mb_ptr, dhw, ssz, dm, scrd) do \
{ \
    int32_t* slo = (int32_t*)scrd.raw_lo(); \
    int32_t* shi = (int32_t*)scrd.raw_hi(); \
    auto const* crds = &mb_ptr[i]; \
    slo[0] = *crds; /*MB*/ \
    shi[0] = *crds+1; \
    slo[1] = 0; \
    shi[1] = 1; /*placehoder for ic < ICxG*/ \
    auto const* precalc = &dhw[i]; /* for(w,h,d): shift, start ,end vectors*/ \
    if (dm >= 5) { \
        slo[2] = precalc[(DHW_D*3+1)*MVL];  \
        shi[2] = precalc[(DHW_D*3+2)*MVL]; \
        slo[3] = precalc[(DHW_H*3+1)*MVL]; \
        shi[3] = precalc[(DHW_H*3+2)*MVL]; \
        slo[4] = precalc[(DHW_W*3+1)*MVL]; \
        shi[4] = precalc[(DHW_W*3+2)*MVL]; \
    } else if (dm >= 4) { \
        slo[2] = precalc[(DHW_H*3+1)*MVL]; \
        shi[2] = precalc[(DHW_H*3+2)*MVL]; \
        slo[3] = precalc[(DHW_W*3+1)*MVL]; \
        shi[3] = precalc[(DHW_W*3+2)*MVL]; \
    } else { \
        slo[2] = precalc[(DHW_W*3+1)*MVL]; \
        shi[2] = precalc[(DHW_W*3+2)*MVL];  \
    } \
    *scrd.raw_sz() = ssz[i]; \
    *scrd.raw_dim() = dm; \
    scrd.init_nd(0); \
} while(0)

namespace dnnl {
namespace impl {
namespace cpu {

using math::get_bias;

#if FWD_IMPL>0
typedef CoordsForNd<6,uint64_t,uint64_t> Coords;

// let's use 32-bit Crd (Pos can still be u64)
// oh. Pos u64 stilll required by memory_desc_wrapper_opt
typedef CoordsForNd<6,uint32_t,int64_t> Coords32;
// src-pixel coords (mb,oc,id,ih,iw when they are valid)

// When you don't need a full iterator (bkwd max-pool)
typedef memory_desc_wrapper_opt::VecPos32 VecPos32;
// a.k.a. CoordRegs<uint32_t, 6>,
// cast to int32_t* sometimes avoids VE conversion nonsense
#endif

//namespace {

#if 0
template <typename dst_data_t> inline
void maybe_postops_vec(
        float *d, dst_data_t const *dst_value, int const dvl,
        post_ops_t const& ops,
        ref_eltwise_scalar_fwd_t * const eltwises_[dnnl_post_ops::capacity]
        )
{
    //printf(" eltwise_@%p\n", (void*)eltwises_);
#if 0 //
    NOVEC for (int i=0; i<dvl; ++i) {
        NOVEC for (int idx = 0; idx < ops.len_; ++idx) {
            const auto &e = ops.entry_[idx];
            if (e.kind == dnnl_sum) {
                d[i] += e.sum.scale * dst_value[i];
            } else {
                dst_data_t const dst_val = dst_value[i];
                float a = d[i];
                a = eltwises_[idx]->compute_scalar(a);
                d[i] = a;
            }
        }
    }
#elif 1 //
        NOVEC PRAG(unroll(dnnl_post_ops::capacity))//;
        for (int idx = 0; idx < ops.len_; ++idx) {
            const auto &e = ops.entry_[idx];
            if (e.kind == dnnl_sum) {
                VFOR(i,dvl) {
                    d[i] += e.sum.scale * dst_value[i];
                }
            } else {
                VFOR(i,dvl) {
                    d[i] = eltwises_[idx]->compute_scalar(d[i]);
                }
            }
        }
#endif
};
#endif

//}//anon::

template <data_type_t src_type, data_type_t wei_type, data_type_t dst_type,
        data_type_t acc_type>
void ref_convolution_fwd_t<src_type, wei_type, dst_type,
        acc_type>::execute_forward(const exec_ctx_t &ctx) const {
    //printf("\nFWD_IMPL=%d\n", (int)FWD_IMPL); fflush(stdout);
    auto src = CTX_IN_MEM(const src_data_t *, DNNL_ARG_SRC);
    auto weights = CTX_IN_MEM(const wei_data_t *, DNNL_ARG_WEIGHTS);
    auto bias = CTX_IN_MEM(const char *, DNNL_ARG_BIAS);
    auto dst = CTX_OUT_MEM(dst_data_t *, DNNL_ARG_DST);

    typedef typename ref_convolution_fwd_t<src_type, wei_type, dst_type,
            acc_type>::pd_t mypd_t;
    mypd_t const* mypd = pd();
    assert( mypd->ker_type() >= 0 || mypd->ker_type() <= 1 ); // ker vs ker_plain

    const memory_desc_wrapper src_d(mypd->src_md());
    const memory_desc_wrapper dst_d(mypd->dst_md());
    const memory_desc_wrapper weights_d(mypd->weights_md(0));
    const memory_desc_wrapper bias_d(mypd->weights_md(1));

    const bool with_groups = mypd->with_groups();

    const int G = mypd->G();
    const int MB = mypd->MB();
    const int OD = mypd->OD();
    const int OH = mypd->OH();
    const int OW = mypd->OW();
    const int ID = mypd->ID();
    const int IH = mypd->IH();
    const int IW = mypd->IW();

    const int OCxG = mypd->OC();        // all channels
    const int ICxG = mypd->IC();
    const int OC = OCxG / G;            // channels per group
    const int IC = ICxG / G;
    //printf(" g%dic%doc%d IC,OC(per group)=%d,%d\n", G,ICxG,OCxG, IC,OC);
    const int KD = mypd->KD();
    const int KH = mypd->KH();
    const int KW = mypd->KW();

    const int KSD = mypd->KSD();
    const int KSH = mypd->KSH();
    const int KSW = mypd->KSW();

    const int KDD = mypd->KDD() + 1;
    const int KDH = mypd->KDH() + 1;
    const int KDW = mypd->KDW() + 1;

    const int padFront = mypd->padFront();
    const int padT = mypd->padT();
    const int padL = mypd->padL();

    const int ndims = mypd->desc()->src_desc.ndims;

    using namespace data_type;
    bool constexpr is_int_conv = utils::one_of(src_type, s32, s8, u8);

    // scale_idx_mult = 1 for per_oc scales and 0, otherwise
    const int scale_idx_mult
            = mypd->attr()->output_scales_.mask_ == (1 << 1);
    const float *scales = mypd->attr()->output_scales_.scales_;

#if FWD_IMPL<=1
    auto maybe_oscale = [=](float &d, int g, int oc) {
        const int scale_idx_mult
                = mypd->attr()->output_scales_.mask_ == (1 << 1);
        const float *scales = mypd->attr()->output_scales_.scales_;
        // scale_idx_mult = 1 for per_oc scales and 0, otherwise
        d *= scales[(g * OC + oc) * scale_idx_mult];
    };
#endif

    const post_ops_t& ops = mypd->attr()->post_ops_;

#if FWD_IMPL<=2
    auto maybe_postops = [&](float &d, dst_data_t const dst_value) {
        // Sum and post ops:
        NOVEC for (int idx = 0; idx < ops.len_; ++idx) {
            const auto &e = ops.entry_[idx];
            if (e.kind == dnnl_sum)
                d += e.sum.scale * dst_value;
            else
                d = eltwises_[idx]->compute_scalar(d);
        }
    };
#endif

#if FWD_IMPL>=2
    auto maybe_postops_vec2 = [&](float *d, dst_data_t const *dst_value, int const dvl) {
        NOVEC //;PRAG(unroll(dnnl_post_ops::capacity))//;
        for (int idx = 0; idx < ops.len_; ++idx) {
            const auto &e = ops.entry_[idx];
            if (e.kind == dnnl_sum) {
                VFOR(i,dvl) {
                    d[i] += e.sum.scale * dst_value[i];
                }
            } else {
                VFOR(i,dvl) {
                    d[i] = eltwises_[idx]->compute_scalar(d[i]);
                }
            }
        }
    };
#endif

#if FWD_IMPL>=3
    auto maybe_postops_vec3 = [&](float *a, float const *dst_float, int const dvl) {
        NOVEC //;PRAG(unroll(dnnl_post_ops::capacity))//;
        for (int idx = 0; idx < ops.len_; ++idx) {
            const auto &e = ops.entry_[idx];
            if (e.kind == dnnl_sum) {
                VFOR(i,dvl) {
                    a[i] += e.sum.scale * dst_float[i];
                }
            } else {
#if 0 //orig
                VFOR(i,dvl) {
                    a[i] = eltwises_[idx]->compute_scalar(a[i]);
                }
#elif 1 //new BUGGY sometimes
                eltwises_[idx]->compute_vec_reg(a, a, dvl);
                //using cvt = Cvt<data_t, is_int_dt>; // postops are pure-float!
#else // assumes dvl small:
                assert( dvl <= MVL );
                float tmp[MVL];
                eltwises_[idx]->compute_vec_reg(tmp, a, dvl);
                VFOR(i,dvl) a[i] = tmp[i];
#endif
            }
        }
    };
#endif

    // Sum and post ops:

    // make offset calls "look the same". We suffer a fn call anyway for the offset.
    auto off_abxg = (with_groups
            ? (ndims == 5? offg5d: ndims == 4? offg4d:
                ndims == 3? offg3d: oops)
            : (ndims == 5? off5d: ndims == 4? off4d:
                ndims == 3? off3d: oops));
    auto off_abx = (ndims == 5? off5d: ndims == 4? off4d:
                ndims == 3? off3d: oops);

#if FWD_IMPL<=4
    auto ker = [=](int g, int mb, int oc, int od, int oh, int ow) {
        acc_data_t d = 0;
        for_(int ic = 0; ic < IC; ++ic)
        for_(int kd = 0; kd < KD; ++kd)
        for_(int kh = 0; kh < KH; ++kh)
        for (int kw = 0; kw < KW; ++kw) {
            const int id = od * KSD - padFront + kd * KDD;
            const int ih = oh * KSH - padT + kh * KDH;
            const int iw = ow * KSW - padL + kw * KDW;

            if (id < 0 || id >= ID) continue;
            if (ih < 0 || ih >= IH) continue;
            if (iw < 0 || iw >= IW) continue;

#if 0
            if (ndims == 5)
                d += (acc_data_t)src[src_d.off(mb, g * IC + ic, id, ih, iw)]
                        * (with_groups ? weights[weights_d.off(
                                   g, oc, ic, kd, kh, kw)]
                                       : weights[weights_d.off(
                                               oc, ic, kd, kh, kw)]);
            else if (ndims == 4)
                d += (acc_data_t)src[src_d.off(mb, g * IC + ic, ih, iw)]
                        * (with_groups ? weights[weights_d.off(
                                   g, oc, ic, kh, kw)]
                                       : weights[weights_d.off(
                                               oc, ic, kh, kw)]);
            else if (ndims == 3)
                d += (acc_data_t)src[src_d.off(mb, g * IC + ic, iw)]
                        * (with_groups ? weights[weights_d.off(g, oc, ic, kw)]
                                       : weights[weights_d.off(oc, ic, kw)]);
            else
                assert(false);
#else
            acc_data_t const ss = src[ off_abx(
                    src_d, 0, mb, g * IC + ic, id, ih, iw) ];
            acc_data_t const ww = weights[ off_abxg(
                    weights_d, g, oc, ic, kd, kh, kw) ];
            d += ss * ww;
#endif
        }
        return d;
    };
#endif

    // help compiler optimize the code
    // constants for plain layouts kernel
    const dnnl_dims_t &src_str = src_d.blocking_desc().strides;
    const dim_t src_ic_stride = src_str[1];
    const dim_t src_id_stride = (ndims == 5) ? src_str[2] : 0;
    const dim_t src_ih_stride = (ndims >= 4) ? src_str[ndims - 2] : 0;
    const dim_t src_iw_stride = (ndims >= 3) ? src_str[ndims - 1] : 0;
    const dnnl_dims_t &weights_str = weights_d.blocking_desc().strides;
    const int gr_shift = with_groups ? 1 : 0;
    const dim_t weights_ic_stride = weights_str[1 + gr_shift];
    const dim_t weights_kd_stride
            = (ndims == 5) ? weights_str[2 + gr_shift] : 0;
    const dim_t weights_kh_stride
            = (ndims >= 4) ? weights_str[ndims - 2 + gr_shift] : 0;
    const dim_t weights_kw_stride
            = (ndims >= 3) ? weights_str[ndims - 1 + gr_shift] : 0;

#if FWD_IMPL<=4
    // pooling has a more advanced "iterate over source pre-image tile" method TODO
    auto ker_plain = [&](int const g, int const mb, int const oc, int const od, int const oh, int const ow) {
        assert(3 <= ndims && ndims <= 5);
        acc_data_t d = 0;
        const src_data_t * __restrict src_loc;
        const wei_data_t * __restrict weights_loc;
        {
            const dim_t src_loc_off = off_abx(src_d, 0, mb, g * IC, 0, 0, 0);
            src_loc = src + src_loc_off;
            const dim_t weights_loc_off = off_abxg(weights_d, g, oc, 0, 0, 0, 0);
            weights_loc = weights + weights_loc_off;
        }
        //assert(  g >= 0 &&  g <  G );
        //assert( mb >= 0 && mb < MB );
        //assert( oc >= 0 && oc < OC );
        //assert( od >= 0 && od < OD );
        //assert( oh >= 0 && oh < OH );
        //assert( ow >= 0 && ow < OW );

        if (IC > KW) {
            for_(dim_t kd = 0; kd < KD; ++kd)
            for_(dim_t kh = 0; kh < KH; ++kh)
            for (dim_t kw = 0; kw < KW; ++kw) {
                const dim_t id = od * KSD - padFront + kd * KDD;
                const dim_t ih = oh * KSH - padT + kh * KDH;
                const dim_t iw = ow * KSW - padL + kw * KDW;
                //if (id < 0 || id >= ID || ih < 0 || ih >= IH || iw < 0
                //        || iw >= IW)
                //    continue;
                if (id < 0 || id >= ID) continue;
                if (ih < 0 || ih >= IH) continue;
                if (iw < 0 || iw >= IW) continue;
                for (int ic = 0; ic < IC; ++ic) {
                    const dim_t src_off = ic + id * src_id_stride
                            + ih * src_ih_stride + iw * src_iw_stride;
                    const dim_t weights_off = ic * weights_ic_stride
                            + kd * weights_kd_stride + kh * weights_kh_stride
                            + kw;
                    d += (acc_data_t)src_loc[src_off]
                            * weights_loc[weights_off];
                }
            }
        } else {
            NOVEC for_(dim_t ic = 0; ic < IC; ++ic)
            NOVEC for_(dim_t kd = 0; kd < KD; ++kd)
            NOVEC for_(dim_t kh = 0; kh < KH; ++kh)
            //NOVEC // REQUIRED for VE to avoid [some, NOT ALL] segfaults :(
            NOVEC for (dim_t kw = 0; kw < KW; ++kw) {
                const dim_t id = od * KSD - padFront + kd * KDD;
                const dim_t ih = oh * KSH - padT + kh * KDH;
                const dim_t iw = ow * KSW - padL + kw * KDW;
                //if (id < 0 || id >= ID || ih < 0 || ih >= IH || iw < 0
                //        || iw >= IW)
                //    continue;
                if (id < 0 || id >= ID) continue;
                if (ih < 0 || ih >= IH) continue;
                if (iw < 0 || iw >= IW) continue;
                asm("###"); // this too is required to avoid segfaultj
                const dim_t src_off = ic + id * src_id_stride
                        + ih * src_ih_stride + iw * src_iw_stride;
                const dim_t weights_off = ic * weights_ic_stride
                        + kd * weights_kd_stride + kh * weights_kh_stride + kw;
                d += (acc_data_t)src_loc[src_off]
                        * weights_loc[weights_off];
            }
        }
        return d;
    };
#endif

#if FWD_IMPL > 0 // common "iterator" setup
    // 32-bit coordinate ranges, 64-bit logical/physical offsets
    auto elems = (size_t)MB * OCxG * OD * OH * OW;
    auto dst_dopt = memory_desc_wrapper_opt(dst_d.md_);
    auto bias_dopt = memory_desc_wrapper_opt(
            bias? bias_d.md_: dst_d.md_/*useless*/);
    auto src_dopt = memory_desc_wrapper_opt(src_d.md_);
    auto weights_dopt = memory_desc_wrapper_opt(weights_d.md_);
#if FWD_IMPL==1
#warning "ref_convolution FWD_IMPL==1"
    auto kern1 = [&](int ithr, int nthr) {
#define DBG 0
#define TRY_DST_VEC 1
        // 0 ~ plain    321.204 523.636 447.257 682.423 | 309.768 162.354 162.404 212.127
        // 1 ~ gather   319.689 521.018 444.751 671.041 | 309.212 160.44  160.544 207.727
#define TRY_BIAS_VEC 2
        // oh. add maybe_oscale tests too...
        // 0 ~ scalar 326.504 522.053 444.163 688.857 | 306.638 163.79  171.25 163.849 215.869
        // 1 ~ vec    281.155 391.514 301.232 559.445 | 259.027 142.872 148.047 144.122 171.174
        // 2 ~ maybe_oscale inline
        //            281.457 391.432 300.939 559.421 | 259.063 142.84 148.035 144.144 171.229
        //              (no effect)
        // Remeasure DST,BIAS settings with force_sequential==0:
        // 0,0  40.8026 63.6244 51.8002 84.347  | 34.649  20.951  21.8545 21.3542 28.1471
        // 1,0  40.8036 63.6164 51.768  85.2347 | 34.8757 20.9518 21.8531 21.3536 28.1453
        // 1,1  37.0265 51.8556 42.2909 74.5562 | 40.5033 18.7342 19.2446 18.8925 23.1774
        // 1,2  36.9422 51.8987 42.3375 74.7361 | 40.4855 18.7346 19.243  18.8942 23.1237
        size_t start, end;
        balance211(elems, nthr, ithr, start, end);
        auto const dm = dst_d.ndims(); // MB, OCxG, OD?, OH?, OW
        assert( dm >= 3 && dm <= 5 );

        Coords32 dcrd(dst_dopt.dims(), dm, start, end);
        // vectorized phys-offset calc, & gather
        if(0) std::cout<<" fwd-conv "<<dcrd.lim_str("dcrd")<<"  "<<dcrd.coord_str("dcrd")<<std::endl;
        // VE code is horrible when mixing int/int32 ops:
        int const* const restrict dcrd_outer0 = reinterpret_cast<int const*>
                (&dcrd.vp[0][0]); 
        //int const* const dcrd_space0 = reinterpret_cast<int const*>
        //        (&dcrd.vp[2][0]);

        NOVEC for ( ; dcrd; ++dcrd) { // in vec-length chunks of dst coords
            int const dvl = dcrd.get_vl();

            dim_t dst_off[MVL];
            dst_dopt.vec_off_v(dcrd.base(), &dst_off[0], dvl, false/*pad*/);

#if TRY_BIAS_VEC>0
            // corresponding chunk of bias_off[] TODO
            // bias "coord" is the 1-D set of dcrd.vp[1][i] "OCxG" values.
            // Just copy them into a VecPos32 and use low-level offset calc.
            dim_t bias_off[MVL];
            if (bias) {
                VecPos32 bias_vp; 
                assert( bias_d.ndims() == 1 );
                ShortLoop() for (int i=0; i<dvl; ++i) {
                    // copy dimension 1 of dcrd --> 1-D bias coord in [0,OCxG)
                    bias_vp.vp[0][i] = (dcrd_outer0 + 1 * MVL)[i];
                }
                // vectorized bias_off calc for this chunk of output coords
                bias_dopt.vec_off_vtmp(bias_vp, &bias_off[0],
                        dvl, false/*pad*/);
            }
#endif // TRY_BIAS_VEC

#if DBG
            {
                int errs = 0;
                for (int i=0; i<dvl; ++i) {
                    int const* dcrd_i = dcrd_outer0 + i;
                    int const mb = dcrd_i[0];
                    int const ocxg = dcrd_i[MVL];
                    int const  g = ocxg / OC;
                    int const oc = ocxg % OC;
                    // spatial coords:
                    dcrd_i += MVL; // coord one before spatial
                    int const od = (dm >= 5? *(dcrd_i+=MVL): 0);
                    int const oh = (dm >= 4? *(dcrd_i+=MVL): 0);
                    int const ow = *(dcrd_i+=MVL);
                    auto dst_off_ref = off_abx(dst_d, 0, mb, g * OC + oc, od, oh, ow);
                    if (dst_off[i] != dst_off_ref) {
                        printf("error: i=%d mb=%d g,oc,ocxg=%d,%d,%d od,oh,ow=%d,%d,%d\n",
                                i, mb, g,oc,ocxg, od,oh,ow);
                        if (++errs > 10) break;
                    }
                }
                assert(errs == 0);
            }
#endif

#if TRY_DST_VEC>0
            dst_data_t dst_gather[MVL];
            for (int i=0; i<dvl; ++i) {
                dst_gather[i] = dst[dst_off[i]];
            }
#endif

            for (int i=0; i<dvl; ++i) {
                int const* dcrd_i = dcrd_outer0 + i;
                int const mb = dcrd_i[0];
                int const ocxg = dcrd_i[MVL];
                // nb: dcrd "OCxG" coord --> g and oc decomposition
                int const  g = ocxg / OC;
                int const oc = ocxg % OC;
                // spatial coords:
                dcrd_i += MVL; // coord one before spatial
                int const od = (dm >= 5? *(dcrd_i+=MVL): 0);
                int const oh = (dm >= 4? *(dcrd_i+=MVL): 0);
                int const ow = *(dcrd_i+=MVL);

#if TRY_BIAS_VEC==0
                float a = (bias ? get_bias(bias, bias_d.off(ocxg),
                            mypd->desc()->bias_desc.data_type)
                        : 0);
#else
                float a = (bias ? get_bias(bias, bias_off[i],
                            mypd->desc()->bias_desc.data_type)
                        : 0);
#endif

                if (src_d.is_plain() && weights_d.is_plain()
                        && src_ic_stride == 1 && weights_kw_stride == 1)
                    a += ker_plain(g, mb, oc, od, oh, ow);
                else
                    a += ker(g, mb, oc, od, oh, ow);

                // TODO: break loop HERE, using float a[MVL].
                //      vectorize maybe_oscale & maybe_postops
                //       ? check if maybe_oscale does anything ?
                //       ? inline maybe_postops ?
                // NB: need vector version of eltwises_ !!

#if DBG
                const dim_t dst_off_ref = off_abx(dst_d, 0, mb, g * OC + oc, od, oh, ow);
                assert( dst_off_ref == dst_off[i] );
#endif

#if TRY_DST_VEC<2
                maybe_oscale(a, g, oc);
#else
                //a *= scales[(g * OC + oc) * scale_idx_mult];
                a *= scales[ocxg * scale_idx_mult];
#endif

#if TRY_DST_VEC==0
                maybe_postops(a, dst[dst_off[i]]);
#else
                maybe_postops(a, dst_gather[i]);
#endif

                using cvt_dst = Cvt<dst_data_t, is_int_conv>;
#if TRY_DST_VEC==0
                dst[dst_off[i]] = cvt_dst::qs(a);
#else
                dst_gather[i] = cvt_dst::qs(a);
#endif
            }
#if TRY_DST_VEC>0
            for (int i=0; i<dvl; ++i) { // scatter
                dst[dst_off[i]] = dst_gather[i];
            }
#endif
        }
    };
#elif FWD_IMPL==2
#warning "ref_convolution FWD_IMPL==2"
    auto kern2 = [&](int ithr, int nthr) {
#define DBG 0
#define VEC_CRD 2
        // 0 same as FWD_IMPL==2 otions 1,2
        //0 36.1292 53.2327 46.5454 72.4565 | 44.0839 18.3425 18.8063 18.4794 22.2922
        // 0+dbg  (~same)
        //1 33.9768 47.1865 39.7339 66.9513 | 33.1597 17.9555 17.6643 17.6145 21.5073
        //  33.9777 47.1926 39.7299 66.9484 | 33.1676 17.956  17.6644 17.6152 21.4942
        //2 33.1341 42.588  37.9946 64.5071 | 34.757  16.6386 16.8067 16.6719 18.6816
        size_t start, end;
        balance211(elems, nthr, ithr, start, end);
        auto const dm = dst_d.ndims(); // MB, OCxG, OD?, OH?, OW
        assert( dm >= 3 && dm <= 5 );

        Coords32 dcrd(dst_dopt.dims(), dm, start, end);
        // vectorized phys-offset calc, & gather
        if(0) std::cout<<" fwd-conv "<<dcrd.lim_str("dcrd")<<"  "<<dcrd.coord_str("dcrd")<<std::endl;
        // VE code is horrible when mixing int/int32 ops:
        int const* const restrict dcrd_outer0 = reinterpret_cast<int const*>
                (&dcrd.vp[0][0]); 
        //int const* const dcrd_space0 = reinterpret_cast<int const*>
        //        (&dcrd.vp[2][0]);

        NOVEC for ( ; dcrd; ++dcrd) { // in vec-length chunks of dst coords
            auto const dvl = dcrd.get_vl();

            dim_t dst_off[MVL];
            dst_dopt.vec_off_v(dcrd.base(), &dst_off[0], dvl, false/*pad*/);

            dim_t bias_off[MVL];
            if (bias) {
                // bias "coord" is the 1-D set of dcrd.vp[1][i] "OCxG" values.
                // Just copy them into a VecPos32 and use low-level offset calc.
                VecPos32 bias_vp; 
                assert( bias_d.ndims() == 1 );
                ShortLoop() for (int i=0; i<dvl; ++i) {
                    // copy dimension 1 of dcrd --> 1-D bias coord in [0,OCxG)
                    bias_vp.vp[0][i] = (dcrd_outer0 + 1 * MVL)[i];
                }
                // vectorized bias_off calc for this chunk of output coords
                bias_dopt.vec_off_vtmp(bias_vp, &bias_off[0],
                        dvl, false/*pad*/);
            }

#if VEC_CRD>0 || (VEC_CRD==0 && DBG)
            int dim_zeros[MVL];
            for (int i=0; i<dvl; ++i) dim_zeros[i] = 0;
            int v_g[MVL];
            int v_oc[MVL];
            // transform scalar loads of mb, ocxg, g, oc, od, oh, ow
            // into vectorized pre-loop pseudo-register settings
            /** Suggestive mnemonic for possible vector registers.
             * point to individual vector coordinate pseudo-register content.
             * Inner loops calling functions will force these to be backed by
             * real memory, but jit impl would avoid such spillage. */
//#define COORD_VEC_REGISTER(VAR, ...) \
//            int const (*VAR)[MVL] \
//                = (int const (*)[MVL]) (__VA_ARGS__)
// However, nc++ does not allow array variable initialization as pointer cast
// (Does work as func arg though, as in C)
            typedef int const *coord_register_t;
#define COORD_VEC_REGISTER(VAR, ...) coord_register_t VAR = (__VA_ARGS__)
            auto dcrd_i = dcrd_outer0;
            COORD_VEC_REGISTER(v_mb, dcrd_i);
            COORD_VEC_REGISTER(v_ocxg, dcrd_i + MVL);
            dcrd_i += MVL; // point 1 dim before spatial coords
            COORD_VEC_REGISTER(v_od, (dm >= 5? (dcrd_i+=MVL): dim_zeros));
            COORD_VEC_REGISTER(v_oh, (dm >= 4? (dcrd_i+=MVL): dim_zeros));
            COORD_VEC_REGISTER(v_ow, dcrd_i+=MVL); // always exists, dm >= 3
            for (int i=0; i<dvl; ++i) {
                v_g[i] = v_ocxg[i] / OC;
                v_oc[i] = v_ocxg[i] % OC;
            }
#endif

            dst_data_t dst_gather[MVL];
            for (int i=0; i<dvl; ++i) {
                dst_gather[i] = dst[dst_off[i]];
            }
#if VEC_CRD==0
            for (int i=0; i<dvl; ++i) {
                int const* dcrd_i = dcrd_outer0 + i;
                int const mb = dcrd_i[0];
                int const ocxg = dcrd_i[MVL];
                // nb: dcrd "OCxG" coord --> g and oc decomposition
                int const  g = ocxg / OC;
                int const oc = ocxg % OC;
                // spatial coords:
                dcrd_i += MVL; // coord one before spatial
                int const od = (dm >= 5? *(dcrd_i+=MVL): 0);
                int const oh = (dm >= 4? *(dcrd_i+=MVL): 0);
                int const ow = *(dcrd_i+=MVL);

#if DBG
                assert( v_mb[i] == mb );
                assert( v_ocxg[i] == ocxg );
                assert( v_g[i] == g );
                assert( v_oc[i] == oc );
                assert( v_od[i] == od );
                assert( v_oh[i] == oh );
                assert( v_ow[i] == ow );
#endif
                float a = (bias ? get_bias(bias, bias_off[i],
                            mypd->desc()->bias_desc.data_type)
                        : 0);

                if (src_d.is_plain() && weights_d.is_plain()
                        && src_ic_stride == 1 && weights_kw_stride == 1)
                    a += ker_plain(g, mb, oc, od, oh, ow);
                else
                    a += ker(g, mb, oc, od, oh, ow);

                // TODO: break loop HERE, using float a[MVL].
                //      vectorize maybe_oscale & maybe_postops
                //       ? check if maybe_oscale does anything ?
                //       ? inline maybe_postops ?
                // NB: need vector version of eltwises_ !!

                //maybe_oscale(a, g, oc);
                //a *= scales[(g * OC + oc) * scale_idx_mult];
                a *= scales[ocxg * scale_idx_mult];

                maybe_postops(a, dst_gather[i]);

                using cvt_dst = Cvt<dst_data_t, is_int_conv>;
                dst_gather[i] = cvt_dst::qs(a);
            }
#elif VEC_CRD==1
            for (int i=0; i<dvl; ++i) {
                float a = (bias ? get_bias(bias, bias_off[i],
                            mypd->desc()->bias_desc.data_type)
                        : 0);

                if (src_d.is_plain() && weights_d.is_plain()
                        && src_ic_stride == 1 && weights_kw_stride == 1)
                    a += ker_plain(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);
                else
                    a += ker(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);

                // TODO: break loop HERE, using float a[MVL].
                //      vectorize maybe_oscale & maybe_postops
                //       ? check if maybe_oscale does anything ?
                //       ? inline maybe_postops ?
                // NB: need vector version of eltwises_ !!

                //maybe_oscale(a, g, oc);
                //a *= scales[(g * OC + oc) * scale_idx_mult];
                a *= scales[v_ocxg[i] * scale_idx_mult];

                maybe_postops(a, dst_gather[i]);

                using cvt_dst = Cvt<dst_data_t, is_int_conv>;
                dst_gather[i] = cvt_dst::qs(a);
            }
#elif VEC_CRD==2
            float a[MVL];
            if (bias) {
                auto const bias_data_type = mypd->desc()->bias_desc.data_type;
                ShortLoop() for (int i=0; i<dvl; ++i)
                    a[i] = get_bias(bias, bias_off[i], bias_data_type);
            } else {
                ShortLoop() for (int i=0; i<dvl; ++i)
                    a[i] = 0.f;
            }
            ShortLoop() for (int i=0; i<dvl; ++i) {
                if (src_d.is_plain() && weights_d.is_plain()
                        && src_ic_stride == 1 && weights_kw_stride == 1)
                    a[i] += ker_plain(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);
                else
                    a[i] += ker(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);
            }

            //maybe_oscale(a, g, oc);
            //a *= scales[(g * OC + oc) * scale_idx_mult];
            ShortLoop() for (int i=0; i<dvl; ++i) // vec
                a[i] *= scales[v_ocxg[i] * scale_idx_mult];

#if 0
            // 33.1433 42.5974 38.0082 64.504 34.7634 16.6391 16.8048 16.6713 106.409
            ShortLoop() for (int i=0; i<dvl; ++i) { // novec
                maybe_postops(a[i], dst_gather[i]);
            }
#elif 0 // expand the lambda
            // 33.3003 44.3341 34.5492 64.9665 36.8885 16.7248 16.8914 16.7544 107.789
            NOVEC ShortLoop() for (int i=0; i<dvl; ++i) {
                NOVEC for (int idx = 0; idx < ops.len_; ++idx) {
                    const auto &e = ops.entry_[idx];
                    if (e.kind == dnnl_sum)
                        a[i] += e.sum.scale * dst_gather[i];
                    else
                        a[i] = eltwises_[idx]->compute_scalar(a[i]);
                }
            }
#elif 0 // move vector loop in, conditional out :
            // 34.5431 46.0956 36.6813 68.0721 41.0491 17.1908 17.3588 17.2288 112.347
            // 34.5444 46.104 36.6893 68.0784 41.0519 17.1936 17.3611 17.2285 112.352
            NOVEC PRAG(unroll(dnnl_post_ops::capacity))//;
            for (int idx = 0; idx < ops.len_; ++idx) {
                const auto &e = ops.entry_[idx];
                if (e.kind == dnnl_sum) {
                    VFOR(i,dvl) a[i] += e.sum.scale * dst_gather[i];
                } else { // still unvec...
                    VFOR(i,dvl) a[i] = eltwises_[idx]->compute_scalar(a[i]);
                }
            }
#else
            // 33.572 46.6966 34.5849 65.733 39.2344 16.7971 16.9612 16.8291 107.548
            // 33.5794 46.6989 34.4488 65.676 39.2319 16.7932 16.9586 16.8314 107.6
            //maybe_postops_vec(a, dst_gather, dvl, ops, this->eltwises_);
            // 32.7192 40.2176 38.1839 63.3171 36.6173 16.3757 16.5399 16.4139 102.625
            // 32.7252 40.2222 38.1701 63.3176 36.6168 16.3745 16.5364 16.4146 102.601 
            maybe_postops_vec2(a, dst_gather, dvl);
#endif
            ShortLoop() for (int i=0; i<dvl; ++i) { //vec
                using cvt_dst = Cvt<dst_data_t, is_int_conv>;
                dst_gather[i] = cvt_dst::qs(a[i]);
            }
#else
#error "VEC_CRD to be done"
#endif
            for (int i=0; i<dvl; ++i) { // scatter
                dst[dst_off[i]] = dst_gather[i];
            }
        }
    };
#elif FWD_IMPL==3
#warning "ref_convolution FWD_IMPL==3"
    auto const bias_data_type = mypd->desc()->bias_desc.data_type;
    if (bias) assert( bias_data_type == data_type::s8
            || bias_data_type == data_type::u8
            || bias_data_type == data_type::s32
            || bias_data_type == data_type::f32);
    else assert( bias_data_type == data_type::undef );
    auto kern3 = [&](int ithr, int nthr) {
        // 1 0 2 had a bug in test_convolution_forward_u8s8s32
#define OPT 1
#define OPTx 0
#define OPTb 2
        //0 : 32.7361 43.3405 34.1239 63.0885 32.5495 16.3621 16.527  16.4015 102.718
        //1 : 31.4836 38.6193 31.9966 59.813 32.5218 15.7574 15.9223 15.7973 95.3827
        //  : 31.4821 38.6191 31.9966 59.8161 32.5977 15.7581 15.9239 15.7982 95.3821
        //  : 31.4858 38.6194 31.9952 59.8086 32.5793 15.7572 15.9227 15.798 95.3642
        //1x: 32.9435 43.0106 33.6597 63.2534 32.855 16.346 16.557 16.4326 103.161
        //    32.212 41.9858 34.2578 61.8911 32.6914 16.1272 16.2886 16.1622 99.6881
        //    32.2111 41.9832 34.2584 61.866 32.6232 16.1219 16.2834 16.1595 99.6254
        //2 : 32.3051 41.8717 34.0609 62.1326 32.677 16.1713 16.3332 16.2089 100.276
        //    31.4576 38.5397 31.7452 59.4827 31.9314 15.6437 15.7981 15.6827 94.2623 (float[] tmp)
        //    31.4569 38.5441 31.7429 59.4799 31.9414 15.6887 15.8483 15.7256 94.8569
        //2x: xxxx 32.7346 43.316 34.1261 63.0803 32.5479 16.3638 16.5284 16.4005 102.722
        //    31.7093 39.5492 32.4717 60.1338 32.7817 15.7927 15.9609 15.8336 95.8761
        //---------------------------------------------
        //1  : 31.8091 39.1734 32.5202 60.4456 59.774 32.6867 15.8667 16.0269 15.9022 96.7785
        //compute_scalar --> compute_vec_reg
        //1  : 31.8089 39.1765 32.5195 60.4395 59.7695 32.686 15.6778 15.6871 15.6779 94.4842
        //1b : 31.8773 39.2112 32.5576 60.9185 60.2438 32.7996 15.7669 15.7766 15.7684 95.4458
        //1b2: 31.3331 38.3888 31.7553 59.4035 59.3386 32.1725 15.4924 15.4991 15.491 92.2239
        size_t start, end;
        balance211(elems, nthr, ithr, start, end);
        auto const dm = dst_d.ndims(); // MB, OCxG, OD?, OH?, OW
        assert( dm >= 3 && dm <= 5 );

        Coords32 dcrd(dst_dopt.dims(), dm, start, end);
        // avoid mixing unsigned and int ops VE
        int const* const restrict dcrd_outer0 = reinterpret_cast<int const*>
                (&dcrd.vp[0][0]); 

        NOVEC for ( ; dcrd; ++dcrd) { // in vec-length chunks of dst coords
            int const dvl = dcrd.get_vl();

#if OPT==0 || OPT==1
            dim_t dst_off[MVL]; // vectorized phys-offset calc
            dst_dopt.vec_off_v(dcrd.base(), &dst_off[0], dvl, false/*pad*/);
#endif

#if OPTb==0
            dim_t bias_off[MVL];
            {
                // bias "coord" is the 1-D set of dcrd.vp[1][i] "OCxG" values.
                // Just copy them into a VecPos32 and use low-level offset calc.
                VecPos32 bias_vp; 
                if(bias) assert( bias_d.ndims() == 1 );
                ShortLoop() for (int i=0; i<dvl; ++i) {
                    // copy dimension 1 of dcrd --> 1-D bias coord in [0,OCxG)
                    bias_vp.vp[0][i] = (dcrd_outer0 + 1 * MVL)[i];
                }
                // vectorized bias_off calc for this chunk of output coords
                bias_dopt.vec_off_vtmp(bias_vp, &bias_off[0],
                        dvl, false/*pad*/);
            }
#endif

            int dim_zeros[MVL];
            VFOR(i,dvl) dim_zeros[i] = 0;
            int v_g[MVL];
            int v_oc[MVL];
            typedef int const *coord_register_t;
            // transform scalar loads of mb, ocxg, g, oc, od, oh, ow
            // into vectorized pre-loop pseudo-register settings
            /* Suggestive mnemonic for possible vector registers.
             * point to individual vector coordinate pseudo-register content.
             * Inner loops calling functions will force these to be backed by
             * real memory, but jit impl would avoid such spillage. */
//#define COORD_VEC_REGISTER(VAR, ...) int const (*VAR)[MVL] = (int const (*)[MVL]) (__VA_ARGS__)
// disallowed: array variable initialization as pointer cast (ok for fn args, though)
#define COORD_VEC_REGISTER(VAR, ...) coord_register_t VAR = (__VA_ARGS__)
            auto dcrd_i = dcrd_outer0;
            COORD_VEC_REGISTER(v_mb, dcrd_i);
            dcrd_i += MVL; // also 1 dim before spatial coords
            COORD_VEC_REGISTER(v_ocxg, dcrd_i);
            COORD_VEC_REGISTER(v_od, (dm >= 5? (dcrd_i+=MVL): dim_zeros));
            COORD_VEC_REGISTER(v_oh, (dm >= 4? (dcrd_i+=MVL): dim_zeros));
            COORD_VEC_REGISTER(v_ow, dcrd_i+=MVL); // always exists, dm >= 3
            VFOR(i,dvl) {
                v_g[i] = v_ocxg[i] / OC;
                v_oc[i] = v_ocxg[i] % OC;
            }

#if OPT==0
            dst_data_t dst_gather[MVL]; // gather dst values (MIGHT be used for post_ops)
            VFOR(i,dvl) dst_gather[i] = dst[dst_off[i]];
#endif
#if OPT==1
            // actually compute_scalar operates on float input
            // and ops.entry_[idx].sum.scale is also float
            // so convert to float right away
            float dst_gather[MVL]; // gather dst values (MIGHT be used for post_ops)
            VFOR(i,dvl) dst_gather[i] = dst[dst_off[i]];
#endif
            float a[MVL];
            if (bias) {
#if OPTb>=1
                dim_t bias_off[MVL];
                {
                    // bias "coord" is the 1-D set of dcrd.vp[1][i] "OCxG" values.
                    // Just copy them into a VecPos32 and use low-level offset calc.
                    VecPos32 bias_vp; 
                    assert( bias_d.ndims() == 1 );
                    ShortLoop() for (int i=0; i<dvl; ++i) {
                        // copy dimension 1 of dcrd --> 1-D bias coord in [0,OCxG)
                        bias_vp.vp[0][i] = (dcrd_outer0 + 1 * MVL)[i];
                    }
                    // vectorized bias_off calc for this chunk of output coords
                    bias_dopt.vec_off_vtmp(bias_vp, &bias_off[0],
                            dvl, false/*pad*/);
                }
#endif
#if OPTb<=1
                VFOR(i,dvl) a[i] = get_bias(bias, bias_off[i], bias_data_type);
#else
                // bias is const char* : coerce the gather & cvt to float a[]
#define CASE(dt) case dt: VFOR(i,dvl) a[i] = (float)((const prec_traits<dt>::type *)bias)[bias_off[i]]; break
                switch(bias_data_type) {
                    CASE(data_type::s8);
                    CASE(data_type::u8);
                    CASE(data_type::s32);
                    CASE(data_type::f32);
                }
#endif
            } else {
                VFOR(i,dvl) a[i] = 0.f;
            }
            VFOR(i,dvl) { // fn calls, novec
                if (src_d.is_plain() && weights_d.is_plain()
                        && src_ic_stride == 1 && weights_kw_stride == 1)
                    a[i] += ker_plain(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);
                else
                    a[i] += ker(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);
            }

            //maybe_oscale(a, g, oc);
            VFOR(i,dvl) a[i] *= scales[v_ocxg[i] * scale_idx_mult];

            // NOTES: in eltwise I now usually do something like:
            // using cvt = Cvt<data_t, is_int_dt>;
            // if (is_int_dt) ydata_t[i] = cvt::rs(xfloat);
            // NOTE: looking here I may want to saturate bf16 (XXX check on x86)
#if OPT==0
            maybe_postops_vec2(a, dst_gather, dvl); // faster as lambda, go figure
#endif
#if OPT==1
            maybe_postops_vec3(a, dst_gather, dvl); // faster as lambda, go figure
#endif
#if OPT>=2
            dim_t dst_off[MVL]; // vectorized phys-offset calc
            dst_dopt.vec_off_v(dcrd.base(), &dst_off[0], dvl, false/*pad*/);
            if (ops.len_) {
                float dst_gather[MVL]; // gather dst values (MIGHT be used for post_ops)
                VFOR(i,dvl) dst_gather[i] = dst[dst_off[i]];
                maybe_postops_vec3(a, dst_gather, dvl); // faster as lambda, go figure
            }
#endif

            using cvt_dst = Cvt<dst_data_t, is_int_conv>;
#if OPT==0 || OPT==1
#if OPTx==0 // technically wrong for OPT==1, but faster !!!
            VFOR(i,dvl) {
                dst_gather[i] = cvt_dst::qs(a[i]);
                dst[dst_off[i]] = dst_gather[i];
            }
#else
            VFOR(i,dvl) {
                auto const x = cvt_dst::qs(a[i]);
                dst[dst_off[i]] = x;
            }
#endif
#elif OPT==2
#if OPTx==0
            //dst_data_t dst_gather[MVL]; // technically correct
            float dst_gather[MVL];
            VFOR(i,dvl) {
                dst_gather[i] = cvt_dst::qs(a[i]);
                dst[dst_off[i]] = dst_gather[i];
            }
#else
            VFOR(i,dvl) {
                auto const x = cvt_dst::qs(a[i]);
                dst[dst_off[i]] = x;
            }
#endif
#elif OPT==3 // very slow!
            float x[MVL];
            if (is_int_conv) { // qz_a1b0 is it different from round,saturate? maybe some bias?
                VFOR(i,dvl) {
                    x[i]= qz_a1b0<float,dst_data_t>()(a[i]);
                }
            }else{
                VFOR(i,dvl) {
                    x[i] = saturate<dst_data_t>(a[i]);
                }
            }
            VFOR(i,dvl) {
                dst[dst_off[i]] = x[i];
            }
#endif
#undef COORD_VEC_REGISTER
#undef OPT
        }
    };
#elif FWD_IMPL==4
#warning "ref_convolution FWD_IMPL==4"
    auto const bias_data_type = mypd->desc()->bias_desc.data_type;
    if (bias) {
        assert( bias_data_type == data_type::s8
                || bias_data_type == data_type::u8
                || bias_data_type == data_type::s32
                || bias_data_type == data_type::f32);
        assert( bias_d.ndims() == 1 );
    } else {
        assert( bias_data_type == data_type::undef );
    }

    auto kern4 = [&](int ithr, int nthr) {
        //1b2: 31.3331 38.3888 31.7553 59.4035 59.3386 32.1725 15.4924 15.4991 15.491 92.2239
        size_t start, end;
        balance211(elems, nthr, ithr, start, end);
        auto const dm = dst_d.ndims(); // MB, OCxG, OD?, OH?, OW
        assert( dm >= 3 && dm <= 5 );

        Coords32 dcrd(dst_dopt.dims(), dm, start, end);
        // avoid mixing unsigned and int ops VE
        int const* const restrict dcrd_outer0 = reinterpret_cast<int const*>
                (&dcrd.vp[0][0]); 

        int dim_zeros[MVL]; // Coords32, so int loop index OK
        {
            // dcrd.get_vl() never increases, so can init outside dcrd-loop
            int dvl=dcrd.get_vl();
            VFOR(i,dvl) dim_zeros[i] = 0;
        }


        NOVEC for ( ; dcrd; ++dcrd) { // in vec-length chunks of dst coords
            int const dvl = dcrd.get_vl();
            dim_t dst_off[MVL]; // vectorized phys-offset calc
            float dst_gather[MVL]; // gather dst values (MIGHT be used for post_ops)
            dst_dopt.vec_off_v(dcrd.base(), &dst_off[0], dvl, false/*pad*/);
            VFOR(i,dvl) dst_gather[i] = dst[dst_off[i]];

            typedef int const *coord_register_t; // VE: avoid mix signed/unsigned
#define COORD_VEC_REGISTER(VAR, ...) coord_register_t VAR = (__VA_ARGS__)
            auto dcrd_i = dcrd_outer0 + MVL; // also 1 dim before spatial coords
            COORD_VEC_REGISTER(v_ocxg, dcrd_outer0 + MVL);

            float a[MVL]; // accumulator
            if (bias) {
                // bias "coord" is the 1-D set of dcrd.vp[1][i] "OCxG" values
                // low-level offsets from logical bias coords in a VecPos32
                VecPos32 bias_vp; 
                dim_t bias_off[MVL];
                VFOR(i,dvl) bias_vp.vp[0][i] = v_ocxg[i];
                bias_dopt.vec_off_vtmp(bias_vp, &bias_off[0],
                        dvl, false/*pad*/);
                // bias is const char* : coerce the gather & cvt to float a[]
#define CASE(dt) case dt: VFOR(i,dvl) a[i] = (float)((const prec_traits<dt>::type *)bias)[bias_off[i]]; break
                switch(bias_data_type) {
                    CASE(data_type::s8);
                    CASE(data_type::u8);
                    CASE(data_type::s32);
                    CASE(data_type::f32);
                }
#undef CASE
            } else {
                VFOR(i,dvl) a[i] = 0.f;
            }

            {
                int v_g[MVL];
                int v_oc[MVL];
                VFOR(i,dvl) {
                    v_g[i] = v_ocxg[i] / OC;
                    v_oc[i] = v_ocxg[i] % OC;
                }
                COORD_VEC_REGISTER(v_mb, dcrd_outer0);
                // v_ocxg already set
                COORD_VEC_REGISTER(v_od, (dm >= 5? (dcrd_i+=MVL): dim_zeros));
                COORD_VEC_REGISTER(v_oh, (dm >= 4? (dcrd_i+=MVL): dim_zeros));
                COORD_VEC_REGISTER(v_ow, dcrd_i+=MVL); // always exists, dm >= 3
#undef COORD_VEC_REGISTER
                if (mypd->ker_type() == 0) {
                    VFOR(i,dvl) { // fn calls, novec
                        a[i] += ker(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);
                    }
                } else {
                    VFOR(i,dvl) { // fn calls, novec
                        a[i] += ker_plain(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);
                    }
                }
            }

            //maybe_oscale(a, g, oc);
            //VFOR(i,dvl) a[i] *= scales[v_ocxg[i] * scale_idx_mult];
            if (scale_idx_mult) VFOR(i,dvl) a[i] *= scales[v_ocxg[i]];
            else                VFOR(i,dvl) a[i] *= scales[0];

            maybe_postops_vec3(a, dst_gather, dvl); // faster as lambda? go figure

            VFOR(i,dvl) {
                // technically wrong for OPT==1, but faster !!!
                using cvt_dst = Cvt<dst_data_t, is_int_conv>;
                dst_gather[i] = cvt_dst::qs(a[i]);
                dst[dst_off[i]] = dst_gather[i];
            }
        }
    };
#elif FWD_IMPL==5
#warning "ref_convolution FWD_IMPL==5 (dev)"
    auto const bias_data_type = mypd->desc()->bias_desc.data_type;
    if (bias) {
        assert( bias_data_type == data_type::s8
                || bias_data_type == data_type::u8
                || bias_data_type == data_type::s32
                || bias_data_type == data_type::f32);
        assert( bias_d.ndims() == 1 );
    } else {
        assert( bias_data_type == data_type::undef );
    }

    auto kern5 = [&](int ithr, int nthr) {
        //1b2: 31.3331 38.3888 31.7553 59.4035 59.3386 32.1725 15.4924 15.4991 15.491 92.2239
        size_t start, end;
        balance211(elems, nthr, ithr, start, end);
        auto const dm = dst_d.ndims(); // MB, OCxG, OD?, OH?, OW
        assert( dm >= 3 && dm <= 5 );

        Coords32 dcrd(dst_dopt.dims(), dm, start, end);
        //Coords32 scrd; // actually only need VecPos32
        //Coords32 wcrd; // [g,] oc, ic, [[kd,] kh,] kw
        // avoid mixing unsigned and int ops VE
        int const* const restrict dcrd_outer0 = reinterpret_cast<int const*>
                (&dcrd.vp[0][0]); 

        int dim_zeros[MVL]; // Coords32, so int loop index OK
        {
            // dcrd.get_vl() never increases, so can init outside dcrd-loop
            int dvl=dcrd.get_vl();
            VFOR(i,dvl) dim_zeros[i] = 0;
        }

        auto ker5 = [=](int g, int mb, int oc, int od, int oh, int ow) {
            acc_data_t d = 0;

            // macro based on ve/hoist.hpp derivations
#if 0 // unsigned-safe macro version
#define IKLIMS(ocrd, STRIDE, DILATION, PAD, KSZ, ISZ, k_st, k_en, i_st) do \
            { \
                k_st = (ocrd * STRIDE < PAD \
                        ? (DILATION - 1 + (PAD - ocrd*STRIDE)) / DILATION \
                        : 0); \
                k_en = (ocrd * STRIDE + KSZ * DILATION < ISZ + PAD + DILATION ? KSZ \
                        : (ocrd * STRIDE >= ISZ + PAD ? 0 \
                            : ((ISZ + PAD - ocrd * STRIDE) + DILATION - 1) / DILATION)); \
                if (k_st > k_en) k_st = k_en; \
                i_st = ocrd * STRIDE - PAD + k_st * DILATION; \
            } while(0)
#elif 0 // signed integer simplification
#define IKLIMS(ocrd, STRIDE, DILATION, PAD, KSZ, ISZ, k_st, k_en, i_st) do \
            { \
                k_en = (DILATION - 1 + (PAD - ocrd*STRIDE)); \
                k_st = k_en / DILATION; \
                k_en = (k_en + ISZ) / DILATION; \
                if (k_st < 0) k_st = 0; \
                i_st = ocrd * STRIDE - PAD + k_st * DILATION; \
                if (k_st > KSZ) k_st = KSZ; \
                if (k_en > KSZ) k_en = KSZ; \
                if (k_en < k_st) k_en = k_st; \
            } while(0)
#else // div_floor function
#define IKLIMS(ocrd, STRIDE, PAD, DILATION, KSZ, ISZ, k_st, k_en, i_0) do \
            { \
                int const A = (int)ocrd * (STRIDE) - (PAD); \
                int const B = (int)(DILATION); \
                assert( B > 0 ); \
                hoist_ApiB( k_st, k_en, \
                        /*kh in   */ 0, KSZ, \
                        /*ih=A+kB */ A, B, \
                        /*ih in   */ 0, ISZ); \
                /*if (k_st > k_en) k_st = k_en;*/ \
                i_0 = A; \
            } while(0)
            // ih0 = A + k0*B --> k0 = div_floor(ih0-A +B-1, B)
#endif
            // unlike pooling, we now support dilation
            int kd_st=0, kd_en=1, id_0=0; // id_0 a.k.a A or ocrd*STRIDE-PAD
            int kh_st=0, kh_en=1, ih_0=0;
            int kw_st=0, kw_en=1, iw_0=0;
            if (ndims >= 5)
                IKLIMS(od, KSD, padFront, KDD, KD, ID, kd_st, kd_en, id_0);
            if (ndims >= 4)
                IKLIMS(oh, KSH, padT    , KDH, KH, IH, kh_st, kh_en, ih_0);
            if (1) //ndims >= 3)
                IKLIMS(ow, KSW, padL    , KDW, KW, IW, kw_st, kw_en, iw_0);
#undef IKLIMS


#if 1 // dbg
            //int vvv = (1 && mb == 0 && oc==0);
            int vvv = (0 && mb==0 && oc==0 && file_errors<10);
            if(vvv>1) printf(" KSD=%-3d PAD=%-3d DIL=%-3d KSZ=%-3d ISZ=%-3d\n", KSD,padFront,KDD,KD,ID);
            if(vvv>1) printf(" KSH=%-3d PAD=%-3d DIL=%-3d KSZ=%-3d ISZ=%-3d\n", KSH,padT    ,KDH,KH,IH);
            if(vvv>1) printf(" KSW=%-3d PAD=%-3d DIL=%-3d KSZ=%-3d ISZ=%-3d\n", KSW,padL    ,KDW,KW,IW);
            if(vvv>0) printf(" kd [%d,%d) id_0=%d\n",  kd_st, kd_en, id_0);
            if(vvv>0) printf(" kh [%d,%d) ih_0=%d\n",  kh_st, kh_en, ih_0);
            if(vvv>0) printf(" kw [%d,%d) iw_0=%d\n",  kw_st, kw_en, iw_0);
            int gold[6*100], calc[6*100];
            int gg=0;
            for_(int kd = 0; kd < KD; ++kd)
            for_(int kh = 0; kh < KH; ++kh)
            for (int kw = 0; kw < KW; ++kw) {
                const int id = od * KSD - padFront + kd * KDD;
                const int ih = oh * KSH - padT + kh * KDH;
                const int iw = ow * KSW - padL + kw * KDW;
                if (id < 0 || id >= ID) continue;
                if (ih < 0 || ih >= IH) continue;
                if (iw < 0 || iw >= IW) continue;
                assert( kd >= 0 && kd < KD );
                assert( kh >= 0 && kh < KH );
                assert( kw >= 0 && kw < KW );
                assert( id >= 0 && id < ID );
                assert( ih >= 0 && ih < IH );
                assert( iw >= 0 && iw < IW );
                gold[0*100 + gg] = kd;
                gold[1*100 + gg] = kh;
                gold[2*100 + gg] = kw;
                gold[3*100 + gg] = id;
                gold[4*100 + gg] = ih;
                gold[5*100 + gg] = iw;
                if (++gg >= 100) goto gold_done;
            }
gold_done:
            int cc=0;
            for_(int kd = kd_st; kd < kd_en; ++kd)
            for_(int kh = kh_st; kh < kh_en; ++kh)
            for (int kw = kw_st; kw < kw_en; ++kw) {
                //const int id = od * KSD - padFront + kd * KDD;
                //const int ih = oh * KSH - padT + kh * KDH;
                //const int iw = ow * KSW - padL + kw * KDW;
                int const id = id_0 + kd * KDD;
                int const ih = ih_0 + kh * KDH;
                int const iw = iw_0 + kw * KDW;
                MUST( kd >= 0 && kd < KD );
                MUST( kh >= 0 && kh < KH );
                MUST( kw >= 0 && kw < KW );
                MUST( id >= 0 && id < ID );
                MUST( ih >= 0 && ih < IH );
                MUST( iw >= 0 && iw < IW );
                calc[0*100 + cc] = kd;
                calc[1*100 + cc] = kh;
                calc[2*100 + cc] = kw;
                calc[3*100 + cc] = id;
                calc[4*100 + cc] = ih;
                calc[5*100 + cc] = iw;
                if (++cc >= 100) goto calc_done;
            }
calc_done:
            if(vvv>1) printf(" g,mb,oc=%d,%d,%d od,oh,ow=%d,%d,%d, ngold=%d ncalc=%d\n", g,mb,oc, od,oh,ow, gg, cc);
            int n = (gg < cc? gg: cc);
            int ndiff = 0;
            for(int i=0; i<n; ++i){
                if(vvv>1) printf(" i%3d: gold kd,kh,kw=%3d,%3d,%3d id,ih,iw=%3d,%3d,%3d\n",
                        i, gold[0*100+i],gold[1*100+i],gold[2*100+i], gold[3*100+i],gold[4*100+i],gold[5*100+i]);
                if (gold[0*100+i] != calc[0*100+i]
                        || gold[1*100+i] != calc[1*100+i]
                        || gold[2*100+i] != calc[2*100+i]
                        || gold[3*100+i] != calc[3*100+i]
                        || gold[4*100+i] != calc[4*100+i]
                        || gold[5*100+i] != calc[5*100+i]){
                    ++ndiff;
                    if(vvv>0) printf("  ERR: calc kd,kh,kw=%3d,%3d,%3d id,ih,iw=%3d,%3d,%3d\n",
                            calc[0*100+i],calc[1*100+i],calc[2*100+i], calc[3*100+i],calc[4*100+i],calc[5*100+i]);
                }
            }

            assert( gg == cc );
            assert( ndiff == 0 );
#endif
#if 0 // classic
            for_(int ic = 0; ic < IC; ++ic)
            for_(int kd = 0; kd < KD; ++kd)
            for_(int kh = 0; kh < KH; ++kh)
            for (int kw = 0; kw < KW; ++kw) {
                const int id = od * KSD - padFront + kd * KDD;
                const int ih = oh * KSH - padT + kh * KDH;
                const int iw = ow * KSW - padL + kw * KDW;

                if (id < 0 || id >= ID) continue;
                if (ih < 0 || ih >= IH) continue;
                if (iw < 0 || iw >= IW) continue;

                acc_data_t const ss = src[ off_abx(
                        src_d, 0, mb, g * IC + ic, id, ih, iw) ];
                acc_data_t const ww = weights[ off_abxg(
                        weights_d, g, oc, ic, kd, kh, kw) ];
                d += ss * ww;
            }
#elif 0 // passed
            for_(int ic = 0; ic < IC; ++ic)
            for_(int kd = kd_st; kd < kd_en; ++kd)
            for_(int kh = kh_st; kh < kh_en; ++kh)
            for (int kw = kw_st; kw < kw_en; ++kw) {
                int const id = id_0 + kd * KDD;
                int const ih = ih_0 + kh * KDH;
                int const iw = iw_0 + kw * KDW;
                assert( kd >= 0 && kd < KD );
                assert( kh >= 0 && kh < KH );
                assert( kw >= 0 && kw < KW );
                assert( id >= 0 && id < ID );
                assert( ih >= 0 && ih < IH );
                assert( iw >= 0 && iw < IW );

                acc_data_t const ss = src[ off_abx(
                        src_d, 0, mb, g * IC + ic, id, ih, iw) ];
                acc_data_t const ww = weights[ off_abxg(
                        weights_d, g, oc, ic, kd, kh, kw) ];
                d += ss * ww;
            }
#elif 1
            //       dcrd  // mb, OCxG, [[od,] oh,] ow
            assert( weights_d.ndims() == dm + (with_groups? 1: 0) );
            Coords32 wcrd; // [g,] oc, ic, [[kd,] kh,] kw
            {
                auto * rlo = wcrd.raw_lo();
                auto * rhi = wcrd.raw_hi();
                Coords32::pos_t sz = Coords32::pos_t{1};
                int nd=0;
                if (with_groups) {
                    if(file_errors>0 && file_errors<10) printf(" g=%d",g);
                    *rlo++ = g;
                    *rhi++ = g+1;
                    //sz *= 1;
                    ++nd;
                }
                if(file_errors>0 && file_errors<10) printf(" oc=%d",oc);
                *rlo++ = oc;
                *rhi++ = oc+1;
                //sz *= 1;
                ++nd;
                if(file_errors>0 && file_errors<10) printf(" ic=[%d,%d)",int{0},int{IC});
                *rlo++ = 0;
                *rhi++ = IC;
                sz *= IC;
                ++nd;
                if (dm >= 5) {
                    if(file_errors>0 && file_errors<10) printf(" kd=[%d,%d)",kd_st,kd_en);
                    *rlo++ = kd_st;
                    *rhi++ = kd_en;
                    sz *= kd_en - kd_st;
                    ++nd;
                }
                if (dm >= 4) {
                    if(file_errors>0 && file_errors<10) printf(" kh=[%d,%d)",kh_st,kh_en);
                    *rlo++ = kh_st;
                    *rhi++ = kh_en;
                    sz *= kh_en - kh_st;
                    ++nd;
                }
                if (1) {
                    if(file_errors>0 && file_errors<10) printf(" kw=[%d,%d)",kw_st,kw_en);
                    *rlo++ = kw_st;
                    *rhi++ = kw_en;
                    sz *= kw_en - kw_st;
                    ++nd;
                }
                *wcrd.raw_sz() = sz;
                *wcrd.raw_dim() = weights_d.ndims();
                if(file_errors>0 && file_errors<10) printf(" sz=%ld, weights_d dim=%d cf. %d\n",(long)sz,wcrd.get_dim(),nd);
                wcrd.init_nd(0);
                if(file_errors>0 && file_errors<10) std::cout<<wcrd.lim_str("wcrd")<<std::endl;
                assert( wcrd.get_dim() == nd );
            }

            VecPos32 svp; //mb, g*IC+ic, id,ih,iw linear function of kd,kh,kw
            {
                int const wvl = wcrd.get_vl();
                VFOR(w,wvl) svp.vp[0][w] = mb; // this dim remains const
            }
            // can iterate IN CONJUNCTION WITH wcrd, i think!

            //for_(int ic = 0; ic < IC; ++ic)
            //for_(int kd = kd_st; kd < kd_en; ++kd)
            //for_(int kh = kh_st; kh < kh_en; ++kh)
            //for (int kw = kw_st; kw < kw_en; ++kw)
            for( ; wcrd; ++wcrd)
            {
                //std::cout<<wcrd.coord_str("wcrd")<<std::endl;
                int const wvl = wcrd.get_vl();
                VFOR(w,wvl) { //vec : wcrd --> svp src coords
//#define DBG_SVP(...) do{__VA_ARGS__;}while(0)
#define DBG_SVP(...) do{}while(0)
                    //svp.vp[0][i] = mb; // this dim remains const
                    DBG_SVP(printf(" w=%-3d wcrd --> svp : mb=%-3d",w,svp.vp[0][w]));
                    int wdim=0;
                    // wcrd ~ [g,]oc,ic[,kd,[,kh]],kw
                    if (with_groups) { // g * IC + ic
                        svp.vp[1][w] = wcrd.vp[0][w] * IC + wcrd.vp[2][w];
                        DBG_SVP(printf(" g*IC+ic=%-3d",svp.vp[1][w]));
                        wdim=3;
                    }else{
                        svp.vp[1][w] = wcrd.vp[1][w];
                        DBG_SVP(printf("      ic=%-3d",svp.vp[1][w]));
                        wdim=2;
                    }
                    int sdim=2;
                    if (dm >= 5) {
                        svp.vp[sdim][w] = od * KSD - padFront + wcrd.vp[wdim][w] * KDD;
                        //                -------- .A. ------   ------ kd ------   .B.
                        DBG_SVP(printf(" id=%-3d",svp.vp[sdim][w]));
                        ++sdim; ++wdim;
                    }
                    if (dm >= 4) {
                        int const kh = wcrd.vp[wdim][w];
                        int const a  = oh * KSH - padT;
                        svp.vp[sdim][w] = a + kh * KDH;
                        DBG_SVP(printf(" (oh,KSH,padT,kh=%d,%d,%d,%d) (a,kh,KDH=%d,%d,%d) ih=%-3d",
                                (int)oh,(int)KSH,(int)padT,kh,
                                a,kh,(int)KDH, svp.vp[sdim][w]));
                        ++sdim; ++wdim;
                    }
                    if (1 /*dm >= 3*/) {
                        svp.vp[sdim][w] = ow * KSW - padL + wcrd.vp[wdim][w] * KDW;
                        DBG_SVP(printf(" iw=%-3d",svp.vp[sdim][w]));
                        ++sdim; ++wdim;
                    }
                    DBG_SVP(printf("\n"));
                    // note: svp dimensionality will come from src_dopt (call to vec_off_vtmp)
                }

#define VEC5_OPT 2
                dim_t wei_off[MVL];
                dim_t src_off[MVL];
#if VEC5_OPT<=1 // may need wcrd and svp to persist
                weights_dopt.vec_off_v(wcrd.base(), &wei_off[0], wvl, false/*pad*/);
                src_dopt.vec_off_v(svp, &src_off[0], wvl, false/*pad*/);
#else // wcrd and svp are OK to clobber during wei_off and src_off calc
                weights_dopt.vec_off_vtmp(wcrd.base(), &wei_off[0], wvl, false/*pad*/);
                src_dopt.vec_off_vtmp(svp, &src_off[0], wvl, false/*pad*/);
#endif // VEC5_OPT

                VFOR(w, wvl) { //unvec
#if VEC5_OPT<=1 // 0: compare src_off, wei_off with off_abxg, off_abx
                    //printf("wvl=%d w=%d\n", wvl, w);
                    int dim=0;
                    int g = 0;
                    if (with_groups) {
                        g = wcrd.vp[dim][w];
                        ++dim;
                    }
                    int const oc = wcrd.vp[dim][w]; ++dim;
                    int const ic = wcrd.vp[dim][w]; ++dim;
                    int kd=0, kh=0, kw=0;
                    if (dm >= 5) { kd = wcrd.vp[dim][w]; ++dim; }
                    if (dm >= 4) { kh = wcrd.vp[dim][w]; ++dim; }
                    if (1/*>=3*/){ kw = wcrd.vp[dim][w]; ++dim; }
                    int const id = id_0 + kd * KDD;
                    int const ih = ih_0 + kh * KDH;
                    int const iw = iw_0 + kw * KDW;
                    assert( kd >= 0 && kd < KD );
                    assert( kh >= 0 && kh < KH );
                    assert( kw >= 0 && kw < KW );
                    assert( id >= 0 && id < ID );
                    assert( ih >= 0 && ih < IH );
                    assert( iw >= 0 && iw < IW );

                    if(file_errors>0 && file_errors<10) printf("wvl=%d w=%d\n", wvl, w);
                    dim_t const www = off_abxg(weights_d, g, oc, ic, kd, kh, kw);
                    dim_t const sss = off_abx(src_d, 0, mb, g * IC + ic, id, ih, iw);
                    MUST( www == wei_off[w] );
                    MUST( sss == src_off[w] );
#if VEC5_OPT==0 // debug
                    //printf("w=%-4d www=%-8ld wei_off[w]=%-8ld     sss=%-8ld src_off[w]=%ld\n",
                    //        w, www, wei_off[w], sss, src_off[w]);
                    int const fv = (file_errors>0 && file_errors<10);
                    if(fv) {
                        printf("wvl=%d w=%d\n", wvl, w);
                        if (www != wei_off[w]) {
                            printf("w=%d off_abxg-->www:%ld != wei_off[%d]:%ld ",
                                    (int)w, (long)www, (int)w, (long)(wei_off[w]));
                            printf(" g,oc,ic, kd,kh,kw=%d,%d,%d, %d,%d,%d\n",
                                    g,oc,ic, kd,kh,kw);
                            printf("   wcrd[w=%3d]:", w);
                            for(int dim=0; dim<wcrd.get_dim(); ++dim) printf(" %d",(int)wcrd.vp[dim][w]);
                            printf("\n");
                        }else printf("w=%d www OK\n");

                        if (sss != src_off[w]) {
                            printf("w=%d off_abx--->sss:%ld != src_off[%d]:%ld",
                                    w, (long)sss, w, (long)src_off[w]);
                            printf(" mb,g*IC+ic,id,ih,iw=%d,%d, %d,%d,%d\n",
                                    mb,g*IC+ic, id,ih,iw);
                            printf("    svp[w=%3d]:", w);
                            for(int dim=0; dim<wcrd.get_dim(); ++dim) printf("%d ",(int)svp.vp[dim][w]);
                            printf("\n");
                        }else printf("w=%d sss OK\n");
                    }
                    //assert(file_errors < 1000);
#endif // VEC5_OPT==1
#endif // VEC5_OPT<=1
#if VEC5_OPT<=1 // orig (off_abxg, off_abx)
                    acc_data_t const ss = src[sss];
                    acc_data_t const ww = weights[www];
                    d += ss * ww;
#else // wcrd,svp vec_off_v
                    acc_data_t const ss = src[src_off[w]];
                    acc_data_t const ww = weights[wei_off[w]];
                    d += ss * ww;
#endif
                }
            }
#endif
            if(file_errors > 10) vvv = 0;
            return d;
        };
        auto ker_plain5 = [&](int const g, int const mb, int const oc, int const od, int const oh, int const ow) {
            assert(3 <= ndims && ndims <= 5);
            acc_data_t d = 0;
            const src_data_t * __restrict src_loc;
            const wei_data_t * __restrict weights_loc;
            {
                const dim_t src_loc_off = off_abx(src_d, 0, mb, g * IC, 0, 0, 0);
                src_loc = src + src_loc_off;
                const dim_t weights_loc_off = off_abxg(weights_d, g, oc, 0, 0, 0, 0);
                weights_loc = weights + weights_loc_off;
            }
            assert(  g >= 0 &&  g <  G );
            assert( mb >= 0 && mb < MB );
            assert( oc >= 0 && oc < OC );
            assert( od >= 0 && od < OD );
            assert( oh >= 0 && oh < OH );
            assert( ow >= 0 && ow < OW );

            if (IC > KW) {
                for_(dim_t kd = 0; kd < KD; ++kd)
                for_(dim_t kh = 0; kh < KH; ++kh)
                for (dim_t kw = 0; kw < KW; ++kw) {
                    const dim_t id = od * KSD - padFront + kd * KDD;
                    const dim_t ih = oh * KSH - padT + kh * KDH;
                    const dim_t iw = ow * KSW - padL + kw * KDW;
                    //if (id < 0 || id >= ID || ih < 0 || ih >= IH || iw < 0
                    //        || iw >= IW)
                    //    continue;
                    if (id < 0 || id >= ID) continue;
                    if (ih < 0 || ih >= IH) continue;
                    if (iw < 0 || iw >= IW) continue;
                    for (int ic = 0; ic < IC; ++ic) {
                        const dim_t src_off = ic + id * src_id_stride
                                + ih * src_ih_stride + iw * src_iw_stride;
                        const dim_t weights_off = ic * weights_ic_stride
                                + kd * weights_kd_stride + kh * weights_kh_stride
                                + kw;
                        d += (acc_data_t)src_loc[src_off]
                                * weights_loc[weights_off];
                    }
                }
            } else {
                NOVEC for_(dim_t ic = 0; ic < IC; ++ic)
                NOVEC for_(dim_t kd = 0; kd < KD; ++kd)
                NOVEC for_(dim_t kh = 0; kh < KH; ++kh)
                //NOVEC // REQUIRED for VE to avoid [some, NOT ALL] segfaults :(
                NOVEC for (dim_t kw = 0; kw < KW; ++kw) {
                    const dim_t id = od * KSD - padFront + kd * KDD;
                    const dim_t ih = oh * KSH - padT + kh * KDH;
                    const dim_t iw = ow * KSW - padL + kw * KDW;
                    //if (id < 0 || id >= ID || ih < 0 || ih >= IH || iw < 0
                    //        || iw >= IW)
                    //    continue;
                    if (id < 0 || id >= ID) continue;
                    if (ih < 0 || ih >= IH) continue;
                    if (iw < 0 || iw >= IW) continue;
                    asm("###"); // this too is required to avoid segfaultj
                    const dim_t src_off = ic + id * src_id_stride
                            + ih * src_ih_stride + iw * src_iw_stride;
                    const dim_t weights_off = ic * weights_ic_stride
                            + kd * weights_kd_stride + kh * weights_kh_stride + kw;
                    d += (acc_data_t)src_loc[src_off]
                            * weights_loc[weights_off];
                }
            }
            return d;
        };


        NOVEC for ( ; dcrd; ++dcrd) { // in vec-length chunks of dst coords
            int const dvl = dcrd.get_vl();
            dim_t dst_off[MVL]; // vectorized phys-offset calc
            float dst_gather[MVL]; // gather dst values (MIGHT be used for post_ops)
            dst_dopt.vec_off_v(dcrd.base(), &dst_off[0], dvl, false/*pad*/);
            VFOR(i,dvl) dst_gather[i] = dst[dst_off[i]];

            typedef int const *coord_register_t; // VE: avoid mix signed/unsigned
#define COORD_VEC_REGISTER(VAR, ...) coord_register_t VAR = (__VA_ARGS__)
            auto dcrd_i = dcrd_outer0 + MVL; // also 1 dim before spatial coords
            COORD_VEC_REGISTER(v_ocxg, dcrd_outer0 + MVL);

            float a[MVL]; // accumulator
            if (bias) {
                // bias "coord" is the 1-D set of dcrd.vp[1][i] "OCxG" values
                // low-level offsets from logical bias coords in a VecPos32
                VecPos32 bias_vp; 
                dim_t bias_off[MVL];
                VFOR(i,dvl) bias_vp.vp[0][i] = v_ocxg[i];
                bias_dopt.vec_off_vtmp(bias_vp, &bias_off[0],
                        dvl, false/*pad*/);
                // bias is const char* : coerce the gather & cvt to float a[]
#define CASE(dt) case dt: VFOR(i,dvl) a[i] = (float)((const prec_traits<dt>::type *)bias)[bias_off[i]]; break
                switch(bias_data_type) {
                    CASE(data_type::s8);
                    CASE(data_type::u8);
                    CASE(data_type::s32);
                    CASE(data_type::f32);
                }
#undef CASE
            } else {
                VFOR(i,dvl) a[i] = 0.f;
            }

            // spatial coords of src, dst as raw int pointers
            //      VE mixed ops w/ unsigned and signed are clunky
            static_assert( sizeof(dcrd.vp[dm-1][0]) == sizeof(int),
                    "require VecPos32");
            int const* const mb_ptr = reinterpret_cast<int const*>
                    (&dcrd.vp[0][0]); 
            int const* const o_spatial0 = reinterpret_cast<int const*>
                    (&dcrd.vp[2][0]);
            //int const* const s_spatial0 = reinterpret_cast<int const*>
            //        (&scrd.vp[2][0]); 
            //int const* const w_spatial0 = reinterpret_cast<int const*>
            //        (&wcrd.vp[2+ (with_groups?1:0)][0]); 

            // what type? data_t dst_reg[MVL]; VREG(dst_reg); // oh, we write this as mem :(
            float dst_reg[MVL]; VREG(dst_reg); // oh, we write this as mem :(
            //int kk_dhw[3*MVL]; // if (ws), max-coord memory (vector kpos post-calc)

            // vector precalc constants (vector calc --> mem)
            // ssz ~ source pooling window (tile overlap with unpadded src)
            // dhw ~ shift, start, end iterator (easy vector calc)
            int ssz[MVL]; // ssz < kern ovlp <= KD*KH*KW (small)
            int dhw[12*MVL]; // nc++ slightly prefers single array over 9
            // precalc ssz and dhw vectors for pool window ovlp w/ src
            {
                int v_g[MVL];
                int v_oc[MVL];
                VFOR(i,dvl) {
                    v_g[i] = v_ocxg[i] / OC;
                    v_oc[i] = v_ocxg[i] % OC;
                }
                COORD_VEC_REGISTER(v_mb, dcrd_outer0);
                // v_ocxg already set
                COORD_VEC_REGISTER(v_od, (dm >= 5? (dcrd_i+=MVL): dim_zeros));
                COORD_VEC_REGISTER(v_oh, (dm >= 4? (dcrd_i+=MVL): dim_zeros));
                COORD_VEC_REGISTER(v_ow, dcrd_i+=MVL); // always exists, dm >= 3
#undef COORD_VEC_REGISTER
                if (mypd->ker_type() == 0) { // ref:any
                    NOVEC VFOR(i,dvl) { // fn calls, novec
#if 1
                        a[i] += ker5(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);
#elif 0
                        // vec coord of i'th conv-window ovlp with src
                        //
                        SCRD_LIMITS(i, mb_ptr, dhw, ssz, dm, /*outputs*/ scrd);
                        for (;scrd;++scrd) {
                            for (int icxg = 0; icxg < ICxG; ++icxg) {
                                scrd.vp[1][i] = icxg;   // mb, icxg, id,ih,iw
                                int const ic = icxg / G;
                                int const  g = icxg % G;
                                int const svl = scrd.get_vl();
                                // init wcrd HERE (g-from-ic/G, oc-from-dcrd, ic-from-ic%G,  kd,kh,kw
                                int const* pwcrd= reinterpret_cast<int const*>(&wcrd.vp[0][0]);
                                if (with_groups) {
                                    FOR(i,svl) pwcrd[i] = g;            // g [opt.]
                                    pwcrd += MVL;
                                }
                                FOR(i,svl) pwcrd[i] = v_oc[i];          // oc
                                pwcrd += MVL;
                                FOR(i,svl) pwcrd[i] = ic;               // ic
                                pwcrd += MVL;
                                int const* dspa = o_spatial0;
                                if (dm >= 5) {
                                    FOR(i,svl) pwcrd[i] = dspa[i];
                                    pwcrd += MVL;
                                    dspa += MVL;
                                }
                                if (dm >= 4) {
                                    FOR(i,svl) pwcrd[i] = dspa[i];
                                    pwcrd += MVL;
                                    dspa += MVL;
                                }


                                src_dopt.vec_off_v(scrd.vp, &src_off[0],
                                        svl, false/*pad*/);

                            }
                        }
#endif
                    }
                } else { // ref:plain
                    VFOR(i,dvl) { // fn calls, novec
                        a[i] += ker_plain5(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);
                    }
                }
            }

            //maybe_oscale(a, g, oc);
            //VFOR(i,dvl) a[i] *= scales[v_ocxg[i] * scale_idx_mult];
            if (scale_idx_mult) VFOR(i,dvl) a[i] *= scales[v_ocxg[i]];
            else                VFOR(i,dvl) a[i] *= scales[0];

            maybe_postops_vec3(a, dst_gather, dvl); // faster as lambda? go figure

            VFOR(i,dvl) {
                // technically wrong for OPT==1, but faster !!!
                using cvt_dst = Cvt<dst_data_t, is_int_conv>;
                dst_gather[i] = cvt_dst::qs(a[i]);
                dst[dst_off[i]] = dst_gather[i];
            }
        }
    };
#elif FWD_IMPL==6
#warning "ref_convolution FWD_IMPL==6 (clean)"
    auto const bias_data_type = mypd->desc()->bias_desc.data_type;
    if (bias) {
        assert( bias_data_type == data_type::s8
                || bias_data_type == data_type::u8
                || bias_data_type == data_type::s32
                || bias_data_type == data_type::f32);
        assert( bias_d.ndims() == 1 );
    } else {
        assert( bias_data_type == data_type::undef );
    }

#define OPT6 0
#define LIM6 0 // -1,0 OK, but 1,2,3 (1,2 check out equiv during -1) may segfault
    // my guess is that this is a miscompilation with nc++-3.0.27

#if LIM6==0 // div_floor "old faithful" avoids segfault of inline version of this macro.
#define IKLIMS(ocrd, STRIDE, PAD, DILATION, KSZ, ISZ, k_st, k_en, i_0) do \
            { \
                int const A = (int)ocrd * (STRIDE) - (PAD); \
                int const B = (int)(DILATION); \
                assert( B > 0 ); \
                hoist_ApiB( k_st, k_en, \
                        /*kh in   */ 0, KSZ, \
                        /*ih=A+kB */ A, B, \
                        /*ih in   */ 0, ISZ); \
                /*if (k_st > k_en) k_st = k_en;*/ \
                i_0 = A; \
            } while(0)
#elif LIM6==1 // signed integer simplification (ranges [0,KSZ) [0,ISZ) positive)
#define IKLIMS(ocrd, STRIDE, DILATION, PAD, KSZ, ISZ, k_st, k_en, i_0) do \
            { \
                i_0 = (int)ocrd * (STRIDE) - (PAD); \
                k_en = DILATION - 1 - i_0; \
                k_st = k_en / DILATION; \
                /* */ \
                /* nc++ is NOT using cms opcode !!! */ \
                /*if (k_st < 0) k_st = 0;*/ \
                k_st = ((int)(k_st) < int{0}? int{0}: (int)k_st); \
                /*k_st = (k_st >= 0? k_st: 0);*/ \
                /*even worse: k_st = nstl::max(0,k_st);*/ \
                /* */ \
                /*not needed: if (k_st > KSZ) k_st = KSZ;*/ \
                k_en = (k_en + ISZ) / DILATION; \
                /*if (k_en > KSZ) k_en = KSZ;*/ \
                k_en = (k_en > KSZ? KSZ: k_en); \
                /*not needed: if (k_en < k_st) k_en = k_st;*/ \
            } while(0)
#elif LIM6==2 // signed integer simplification
#define IKLIMS(ocrd, STRIDE, DILATION, PAD, KSZ, ISZ, k_st, k_en, i_0) do \
            { \
                asm("#LIM6==2 beg"); \
                int const A = (int)ocrd * (STRIDE) - (PAD); \
                int const B = (int)(DILATION); \
                int kk_en = B - 1 - A; \
                int kk_st = kk_en / B; \
                kk_st = (kk_st < 0? 0: kk_st); \
                kk_en = (kk_en + (int)(ISZ)) / B; \
                kk_en = (kk_en > (int)(KSZ)? (int)(KSZ): kk_en); \
                /*not needed: if (kk_en < kk_st) kk_en = kk_st;*/ \
                if (k_en < k_st) k_en = k_st; \
                k_st = kk_st; \
                k_en = kk_en; \
                i_0 = A; \
                asm("#LIM6==2 end"); \
            } while(0)
#elif LIM6==3 // unsigned-safe macro version (BUGGY) (XXX should fix, someday)
#define IKLIMS(ocrd, STRIDE, DILATION, PAD, KSZ, ISZ, k_st, k_en, i_st) do \
            { \
                k_st = (ocrd * STRIDE < PAD \
                        ? (DILATION - 1 + (PAD - ocrd*STRIDE)) / DILATION \
                        : 0); \
                i_st = ocrd * STRIDE - PAD + /* k_st=0 * DILATION */ ; \
                int iLast = ISZ; \
                k_en = (ocrd * STRIDE + KSZ * DILATION < iLast + PAD + DILATION ? KSZ \
                        : (ocrd * STRIDE >= iLast + PAD ? 0 \
                            : ((iLast + PAD - ocrd * STRIDE) + DILATION - 1) / DILATION)); \
                if (k_st > k_en) k_st = k_en; \
            } while(0)
#else // div_floor function, with debug alt. version
    // although kk_st,kk_en check out as OK, inlining that method leads to segfaults !!!
#define IKLIMS(ocrd, STRIDE, PAD, DILATION, KSZ, ISZ, k_st, k_en, i_0) do \
            { \
                int const A = (int)ocrd * (STRIDE) - (PAD); \
                int const B = (int)(DILATION); \
                assert( B > 0 ); \
                hoist_ApiB( k_st, k_en, \
                        /*kh in   */ 0, KSZ, \
                        /*ih=A+kB */ A, B, \
                        /*ih in   */ 0, ISZ); \
                /*if (k_st > k_en) k_st = k_en;*/ \
                i_0 = A; \
                asm("#kk"); \
                int kk_en = DILATION - 1 - i_0; \
                int kk_st = kk_en / DILATION; \
                kk_st = (kk_st < 0? 0: kk_st); \
                int iLast = ISZ; \
                kk_en = (kk_en + iLast) / DILATION; \
                kk_en = (kk_en > KSZ? KSZ: kk_en); \
                asm("#kk2"); \
                /*if (kk_en < kk_st) kk_en = kk_st;*/ \
                /* kk_en will not agree sometimes when k_en < 0 */ \
                if( (kk_st != k_st) \
                        || (k_en >= 0 && kk_en != k_en) \
                        || (k_en < 0 && kk_en > 0) \
                  ){ \
                    printf(" ocrd,S,P,D, K,I=%d,%d,%d,%d %d,%d A=%d B=%d k[%d,%d) kk[%d,%d)\n", \
                            (int)(ocrd),(int)(STRIDE),(int)(PAD),(int)(DILATION), (int)(KSZ),(int)(ISZ), \
                            A,B, (int)(k_st),(int)(k_en), kk_st,kk_en); \
                    exit(1); \
                } \
            } while(0)
            // ih0 = A + k0*B --> k0 = div_floor(ih0-A +B-1, B)
#endif

    auto ker6 = [=](int g, int mb, int oc, int od, int oh, int ow) {
        acc_data_t d = 0;

        // macro based on ve/hoist.hpp derivations
        // unlike pooling, we now support dilation
        int kd_st=0, kd_en=1, id_0=0; // id_0 a.k.a A or ocrd*STRIDE-PAD
        int kh_st=0, kh_en=1, ih_0=0;
        int kw_st=0, kw_en=1, iw_0=0;
        if (ndims >= 5)
            IKLIMS(od, KSD, padFront, KDD, KD, ID, kd_st, kd_en, id_0);
        if (ndims >= 4)
            IKLIMS(oh, KSH, padT    , KDH, KH, IH, kh_st, kh_en, ih_0);
        if (1) //ndims >= 3
            IKLIMS(ow, KSW, padL    , KDW, KW, IW, kw_st, kw_en, iw_0);

        auto const dm = dst_d.ndims(); // MB, OCxG, OD?, OH?, OW
        //assert( weights_d.ndims() == dm + (with_groups? 1: 0) );
        //       dcrd  // mb, OCxG, [[od,] oh,] ow
        Coords32 wcrd; // [g,] oc, ic, [[kd,] kh,] kw
        {
            auto * rlo = wcrd.raw_lo();
            auto * rhi = wcrd.raw_hi();
            // XXX move out, set g,oc,IC ranges just once (const)
            Coords32::pos_t sz = Coords32::pos_t{1};
            if (with_groups) {
                *rlo++ = g;
                *rhi++ = g+1;
            }
            *rlo++ = oc;
            *rhi++ = oc+1;
            *rlo++ = 0;
            *rhi++ = IC;
            sz *= IC;
            if (dm >= 5) {
                *rlo++ = kd_st;
                *rhi++ = kd_en;
                sz *= kd_en - kd_st;
            }
            if (dm >= 4) {
                *rlo++ = kh_st;
                *rhi++ = kh_en;
                sz *= kh_en - kh_st;
            }
            if (1) {
                *rlo++ = kw_st;
                *rhi++ = kw_en;
                sz *= kw_en - kw_st;
            }
            *wcrd.raw_sz() = sz;
            *wcrd.raw_dim() = weights_d.ndims();
            wcrd.init_nd(0);
        }

        // move out XXX and set mb just once
        VecPos32 svp; //mb, g*IC+ic, id,ih,iw linear function of kd,kh,kw
        {
            int const wvl = wcrd.get_vl(); // may only decrease later
            VFOR(w,wvl) svp.vp[0][w] = mb; // this dim remains const
        }

        //int const* restrict const vw = &wcrd.vp[0][0]; // XXX
        for( ; wcrd; ++wcrd) // wcrd ~ [g,]oc,ic[,kd,[,kh]],kw
        {
            //std::cout<<wcrd.coord_str("wcrd")<<std::endl;
            int const wvl = wcrd.get_vl();
            VFOR(w,wvl) { //vec : wcrd --> svp src coords
                //svp.vp[0][i] = mb; // this dim remains const
                int wdim=0;
                if (with_groups) { // g * IC + ic
                    svp.vp[1][w] = wcrd.vp[0][w] * IC + wcrd.vp[2][w];
                    wdim=3;
                }else{
                    svp.vp[1][w] = wcrd.vp[1][w];
                    wdim=2;
                }
                int sdim=2;
                if (dm >= 5) {
                    svp.vp[sdim][w] = id_0 + wcrd.vp[wdim][w] * KDD;
                    ++sdim; ++wdim;
                }
                if (dm >= 4) {
                    svp.vp[sdim][w] = ih_0 + wcrd.vp[wdim][w] * KDH;
                    ++sdim; ++wdim;
                }
                if (1 /*dm >= 3*/) {
                    svp.vp[sdim][w] = iw_0 + wcrd.vp[wdim][w] * KDW;
                    ++sdim; ++wdim;
                }
            }

            dim_t wei_off[MVL];
            dim_t src_off[MVL];
            // vtmp: wcrd and svp are OK to clobber during wei/src_off calc
            weights_dopt.vec_off_vtmp(wcrd.base(), &wei_off[0], wvl, false/*pad*/);
            src_dopt.vec_off_vtmp(svp, &src_off[0], wvl, false/*pad*/);
            VFOR(w, wvl) { //vec
                acc_data_t const ss = src[src_off[w]];
                acc_data_t const ww = weights[wei_off[w]];
                d += ss * ww;
            }
        }
        return d;
    };

    auto ker_plain6 = [&](int const g, int const mb, int const oc, int const od, int const oh, int const ow) {
        //assert(3 <= ndims && ndims <= 5);
        acc_data_t d = 0;
        const src_data_t * __restrict src_loc;
        const wei_data_t * __restrict weights_loc;
        {
            const dim_t src_loc_off = off_abx(src_d, 0, mb, g * IC, 0, 0, 0);
            src_loc = src + src_loc_off;
            const dim_t weights_loc_off = off_abxg(weights_d, g, oc, 0, 0, 0, 0);
            weights_loc = weights + weights_loc_off;
        }
        //assert(  g >= 0 &&  g <  G );
        //assert( mb >= 0 && mb < MB );
        //assert( oc >= 0 && oc < OC );
        //assert( od >= 0 && od < OD );
        //assert( oh >= 0 && oh < OH );
        //assert( ow >= 0 && ow < OW );

#if OPT6<0 // ~ original impl
        if (IC > KW) {
            for_(dim_t kd = 0; kd < KD; ++kd)
            for_(dim_t kh = 0; kh < KH; ++kh)
            for (dim_t kw = 0; kw < KW; ++kw) {
                const dim_t id = od * KSD - padFront + kd * KDD;
                const dim_t ih = oh * KSH - padT + kh * KDH;
                const dim_t iw = ow * KSW - padL + kw * KDW;
                if (id < 0 || id >= ID || ih < 0 || ih >= IH || iw < 0
                        || iw >= IW)
                    continue;
                //if (id < 0 || id >= ID) continue;
                //if (ih < 0 || ih >= IH) continue;
                //if (iw < 0 || iw >= IW) continue;
                for (int ic = 0; ic < IC; ++ic) {
                    const dim_t src_off = ic + id * src_id_stride
                            + ih * src_ih_stride + iw * src_iw_stride;
                    const dim_t weights_off = ic * weights_ic_stride
                            + kd * weights_kd_stride + kh * weights_kh_stride
                            + kw;
                    d += (acc_data_t)src_loc[src_off]
                            * weights_loc[weights_off];
                }
            }
        } else {
            NOVEC for_(dim_t ic = 0; ic < IC; ++ic)
            NOVEC for_(dim_t kd = 0; kd < KD; ++kd)
            NOVEC for_(dim_t kh = 0; kh < KH; ++kh)
            //NOVEC // REQUIRED for VE to avoid [some, NOT ALL] segfaults :(
            NOVEC for (dim_t kw = 0; kw < KW; ++kw) {
                const dim_t id = od * KSD - padFront + kd * KDD;
                const dim_t ih = oh * KSH - padT + kh * KDH;
                const dim_t iw = ow * KSW - padL + kw * KDW;
                //if (id < 0 || id >= ID || ih < 0 || ih >= IH || iw < 0
                //        || iw >= IW)
                //    continue;
                if (id < 0 || id >= ID) continue;
                if (ih < 0 || ih >= IH) continue;
                if (iw < 0 || iw >= IW) continue;
                asm("###"); // this too is required to avoid VE segfault
                const dim_t src_off = ic + id * src_id_stride
                        + ih * src_ih_stride + iw * src_iw_stride;
                const dim_t weights_off = ic * weights_ic_stride
                        + kd * weights_kd_stride + kh * weights_kh_stride + kw;
                d += (acc_data_t)src_loc[src_off]
                        * weights_loc[weights_off];
            }
        }
#endif //OPT6<0
#if OPT6>=0
        // new vectorization, based on ker6
        // currently, using Coords32 to lump the for loops
        // is SLOWER than nc++'s vectorization of the nested loops.
        // (even with 'asm' and NOVEC, it seems).
        Coords32 crd; // ic, [[kd,] kh,] kw
#if OPT6==0
        int kd_st=0, kd_en=1; // id_0 a.k.a A or ocrd*STRIDE-PAD
        int kh_st=0, kh_en=1;
        int kw_st=0, kw_en=1;
#endif
        int id_0=0, ih_0=0, iw_0=0;
        {
#if OPT6>=1
            int kd_st=0, kd_en=1; // id_0 a.k.a A or ocrd*STRIDE-PAD
            int kh_st=0, kh_en=1;
            int kw_st=0, kw_en=1;
#endif
            if (ndims >= 5)
                IKLIMS(od, KSD, padFront, KDD, KD, ID, kd_st, kd_en, id_0);
            if (ndims >= 4)
                IKLIMS(oh, KSH, padT    , KDH, KH, IH, kh_st, kh_en, ih_0);
            if (1) //ndims >= 3
                IKLIMS(ow, KSW, padL    , KDW, KW, IW, kw_st, kw_en, iw_0);
            //assert( weights_d.ndims() == dm + (with_groups? 1: 0) );
            //       dcrd  // mb, OCxG, [[od,] oh,] ow
            {
                auto const dm = dst_d.ndims(); // MB, OCxG, OD?, OH?, OW
                auto * rlo = crd.raw_lo();
                auto * rhi = crd.raw_hi();
                Coords32::pos_t sz = Coords32::pos_t{1};
                *rlo++ = 0;
                *rhi++ = IC;
                sz *= IC;
                if (dm >= 5) {
                    *rlo++ = kd_st;
                    *rhi++ = kd_en;
                    sz *= kd_en - kd_st;
                }
                if (dm >= 4) {
                    *rlo++ = kh_st;
                    *rhi++ = kh_en;
                    sz *= kh_en - kh_st;
                }
                if (1) {
                    *rlo++ = kw_st;
                    *rhi++ = kw_en;
                    sz *= kw_en - kw_st;
                }
                *crd.raw_sz() = sz;
                *crd.raw_dim() = weights_d.ndims();
                crd.init_nd(0);
            }
        }

#if OPT6==0 // JUST the loop-limit-precalc...
        if (IC > KW) {
            for_(dim_t kd = kd_st; kd < kd_en; ++kd)
            for_(dim_t kh = kh_st; kh < kh_en; ++kh)
            for (dim_t kw = kw_st; kw < kw_en; ++kw) {
                const dim_t id = od * KSD - padFront + kd * KDD;
                const dim_t ih = oh * KSH - padT + kh * KDH;
                const dim_t iw = ow * KSW - padL + kw * KDW;
                for (int ic = 0; ic < IC; ++ic) {
                    const dim_t src_off = ic + id * src_id_stride
                            + ih * src_ih_stride + iw * src_iw_stride;
                    const dim_t weights_off = ic * weights_ic_stride
                            + kd * weights_kd_stride + kh * weights_kh_stride
                            + kw;
                    d += (acc_data_t)src_loc[src_off]
                            * weights_loc[weights_off];
                }
            }
        } else {
            for_(dim_t ic = 0; ic < IC; ++ic)
            for_(dim_t kd = kd_st; kd < kd_en; ++kd)
            for_(dim_t kh = kh_st; kh < kh_en; ++kh)
            for (dim_t kw = kw_st; kw < kw_en; ++kw) {
                const dim_t id = od * KSD - padFront + kd * KDD;
                const dim_t ih = oh * KSH - padT + kh * KDH;
                const dim_t iw = ow * KSW - padL + kw * KDW;
                const dim_t src_off = ic + id * src_id_stride
                        + ih * src_ih_stride + iw * src_iw_stride;
                const dim_t weights_off = ic * weights_ic_stride
                        + kd * weights_kd_stride + kh * weights_kh_stride + kw;
                d += (acc_data_t)src_loc[src_off]
                        * weights_loc[weights_off];
            }
        }
#else // OPT6 > 0
        int const* pw = reinterpret_cast<int const*>(&crd.vp[0][0]);
        int const ws_ic = weights_ic_stride;
        int const ws_kd = weights_kd_stride;
        int const ws_kh = weights_kh_stride;
        int const ws_kw = weights_kw_stride;
        for( ; crd; ++crd) // crd ~ ic[,kd,[,kh]],kw
        {
            int const wvl = crd.get_vl();
            dim_t wei_off[MVL]; VREG(wei_off);
            dim_t src_off[MVL]; VREG(src_off);
            int const* restrict const vic = &pw[0*MVL];
            if (dm >= 5) {
#if OPT6==1
                VFOR(w,wvl) { //vec : crd --> wei/src_off[]
                    {
                        int const ic = pw[0*MVL+w];
                        src_off[w] = ic;
                        wei_off[w] = ic * ws_ic;
                    }
                    {
                        int kd = pw[1*MVL+w];
                        wei_off[w] += kd * ws_kd;
                        //int const id = id_0 + kd * KDD;
                        kd = kd * KDD + id_0;
                        src_off[w] += kd * src_id_stride;
                    }
                    {
                        int const kh = pw[2*MVL+w];
                        wei_off[w] += kh * ws_kh;
                        int const ih = ih_0 + kh * KDH;
                        src_off[w] += ih * src_ih_stride;
                    }
                    {
                        int const kw = pw[3*MVL+w];
                        wei_off[w] += kw;
                        int const iw = iw_0 + kw * KDW;
                        src_off[w] += iw * src_iw_stride;
                    }
                }
#else
                int const* restrict const vkd = &pw[1*MVL];
                int const* restrict const vkh = &pw[2*MVL];
                int const* restrict const vkw = &pw[3*MVL];
                VFOR(w,wvl) { //vec : crd --> wei/src_off[]
                    wei_off[w] = vic[w] * weights_ic_stride
                            + vkd[w] * weights_kd_stride
                            + vkh[w] * weights_kh_stride
                            + vkw[w];
                    src_off[w] = vic[w]
                            + (id_0 + vkd[w] * KDD)/*id*/ * src_id_stride
                            + (ih_0 + vkh[w] * KDH)/*ih*/ * src_ih_stride
                            + (iw_0 + vkw[w] * KDW)/*iw*/ * src_iw_stride;
                }
#endif
            } else if (dm >= 4) {
#if OPT6==1
                VFOR(w,wvl) { //vec : crd --> wei/src_off[]
                    int const ic = pw[0*MVL+w];
                    int const kh = pw[1*MVL+w];
                    int const kw = pw[2*MVL+w];
                    wei_off[w] = ic * ws_ic
                            + kh * ws_kh + kw;
                    int const ih = ih_0 + kh * KDH;
                    int const iw = iw_0 + kw * KDW;
                    src_off[w] = ic + ih * src_ih_stride
                            + iw * src_iw_stride;
                }
#else
                int const* restrict const vkh = &pw[1*MVL];
                int const* restrict const vkw = &pw[2*MVL];
                VFOR(w,wvl) { //vec : crd --> wei/src_off[]
                    wei_off[w] = vic[w] * ws_ic
                            //+ vkd[w] * ws_kd
                            + vkh[w] * ws_kh
                            + vkw[w];
                    src_off[w] = vic[w]
                            //+ (id_0 + vkd[w] * KDD)/*id*/ * src_id_stride
                            + (ih_0 + vkh[w] * KDH)/*ih*/ * src_ih_stride
                            + (iw_0 + vkw[w] * KDW)/*iw*/ * src_iw_stride;
                }
#endif
            } else { // dm >= 3
#if OPT6==1
                VFOR(w,wvl) { //vec : crd --> wei/src_off[]
                    int const ic = pw[0*MVL+w];
                    int const kw = pw[1*MVL+w];
                    wei_off[w] = ic * ws_ic + kw;
                    int const iw = iw_0 + kw * KDW;
                    src_off[w] = ic + iw * src_iw_stride;
                }
#else
                //int const* restrict const vkd = &pw[1*MVL];
                //int const* restrict const vkh = &pw[1*MVL];
                int const* restrict const vkw = &pw[1*MVL];
                VFOR(w,wvl) { //vec : crd --> wei/src_off[]
                    wei_off[w] = vic[w] * weights_ic_stride
                            //+ vkd[w] * weights_kd_stride
                            //+ vkh[w] * weights_kh_stride
                            + vkw[w];
                    src_off[w] = vic[w]
                            //+ (id_0 + vkd[w] * KDD)/*id*/ * src_id_stride
                            //+ (ih_0 + vkh[w] * KDH)/*ih*/ * src_ih_stride
                            + (iw_0 + vkw[w] * KDW)/*iw*/ * src_iw_stride;
                }
#endif
            }
            VFOR(w,wvl) {
                acc_data_t const ss = src_loc[src_off[w]];
                acc_data_t const ww = weights_loc[wei_off[w]];
                d += ss * ww;
            }
        }//for-crd
#endif
#endif // OPT6>=0
        return d;
    };
#undef IKLIMS

    auto kern6 = [&](int ithr, int nthr) {
        size_t start, end;
        balance211(elems, nthr, ithr, start, end);
        auto const dm = dst_d.ndims(); // MB, OCxG, OD?, OH?, OW
        assert( dm >= 3 && dm <= 5 );

        Coords32 dcrd(dst_dopt.dims(), dm, start, end);
        // avoid mixing unsigned and int ops VE
        int const* const restrict dcrd_outer0 = reinterpret_cast<int const*>
                (&dcrd.vp[0][0]); 

        int dim_zeros[MVL]; // Coords32, so int loop index OK
        {
            // dcrd.get_vl() never increases, so can init outside dcrd-loop
            int dvl=dcrd.get_vl();
            VFOR(i,dvl) dim_zeros[i] = 0;
        }


        NOVEC for ( ; dcrd; ++dcrd) { // in vec-length chunks of dst coords
            int const dvl = dcrd.get_vl();
            dim_t dst_off[MVL]; // vectorized phys-offset calc
            float dst_gather[MVL]; // gather dst values (MIGHT be used for post_ops)
            dst_dopt.vec_off_v(dcrd.base(), &dst_off[0], dvl, false/*pad*/);
            VFOR(i,dvl) dst_gather[i] = dst[dst_off[i]];

            typedef int const *coord_register_t; // VE: avoid mix signed/unsigned
#define COORD_VEC_REGISTER(VAR, ...) coord_register_t VAR = (__VA_ARGS__)
            auto dcrd_i = dcrd_outer0 + MVL; // also 1 dim before spatial coords
            COORD_VEC_REGISTER(v_ocxg, dcrd_outer0 + MVL);

            float a[MVL]; // accumulator
            if (bias) {
                // bias "coord" is the 1-D set of dcrd.vp[1][i] "OCxG" values
                // low-level offsets from logical bias coords in a VecPos32
                VecPos32 bias_vp; 
                dim_t bias_off[MVL];
                VFOR(i,dvl) bias_vp.vp[0][i] = v_ocxg[i];
                bias_dopt.vec_off_vtmp(bias_vp, &bias_off[0],
                        dvl, false/*pad*/);
                // bias is const char* : coerce the gather & cvt to float a[]
#define CASE(dt) case dt: VFOR(i,dvl) a[i] = (float)((const prec_traits<dt>::type *)bias)[bias_off[i]]; break
                switch(bias_data_type) {
                    CASE(data_type::s8);
                    CASE(data_type::u8);
                    CASE(data_type::s32);
                    CASE(data_type::f32);
                }
#undef CASE
            } else {
                VFOR(i,dvl) a[i] = 0.f;
            }

#if 0
            // spatial coords of src, dst as raw int pointers
            //      VE mixed ops w/ unsigned and signed are clunky
            static_assert( sizeof(dcrd.vp[dm-1][0]) == sizeof(int),
                    "require VecPos32");
            int const* const mb_ptr = reinterpret_cast<int const*>
                    (&dcrd.vp[0][0]); 
            int const* const o_spatial0 = reinterpret_cast<int const*>
                    (&dcrd.vp[2][0]);
            //int const* const s_spatial0 = reinterpret_cast<int const*>
            //        (&scrd.vp[2][0]); 
            //int const* const w_spatial0 = reinterpret_cast<int const*>
            //        (&wcrd.vp[2+ (with_groups?1:0)][0]); 

            // what type? data_t dst_reg[MVL]; VREG(dst_reg); // oh, we write this as mem :(
            float dst_reg[MVL]; VREG(dst_reg); // oh, we write this as mem :(
            //int kk_dhw[3*MVL]; // if (ws), max-coord memory (vector kpos post-calc)

            // vector precalc constants (vector calc --> mem)
            // ssz ~ source pooling window (tile overlap with unpadded src)
            // dhw ~ shift, start, end iterator (easy vector calc)
            int ssz[MVL]; // ssz < kern ovlp <= KD*KH*KW (small)
            int dhw[12*MVL]; // nc++ slightly prefers single array over 9
            // precalc ssz and dhw vectors for pool window ovlp w/ src
            //POOL_WINDOW_PRECALC(dm, dvl, o_spatial0, /*outputs*/ ssz, dhw);
            // macro version of window_precalc
            // Each destination spatial pixel has a source (and weight) pre-image tile.
            // Calculate 3 vectors of destination window info per spatial dimension.
            //
            // - ls_sz ~ src (/weights) tile coord span.
            // - ls_st ~ logical source start coord; in interval [0,ID|IH|IW)
            // - lk_st ~ logical kernel start coord; in interval [0,KD|KH|KW)
#define CONV_WINDOW_LOOP(d_spatial, dvl, STRIDE, DILATION, PAD, KSZ, ISZ, lscrd, p_st, p_en, ssz) do \
            { \
                ShortLoop() for(int i=0; i<dvl; ++i) { \
                    int lscrd = d_spatial[i]/*od or oh or ow*/ * STRIDE - PAD; \
                    ls_st[i] = (lscrd > 0? lscrd: 0); \
                    int ls_en = (lscrd + KSZ < ISZ? lscrd + KSZ: ISZ); \
                    if (ls_en < ls_st[i]) ls_en = ls_st[i]; \
                    ls_sz[i] = ls_en - ls_st[i]; \
                    ssz[i] *= ls_sz[i]; \
                } \
            } while(0)

            // expects SD,KD,ID,padF, etc int consts defined as usual
            // dcrd info passed in as dm~dcrd.get_dim() dvl~dcrd.get_vl(),
            // and d_spatial0 ~ (int32_t*)&dcrd.vp[2][0].
#define CONV_WINDOW_PRECALC(dm, dvl, d_spatial0, ssz, dhw) do \
            { \
                int const* d_spatial = d_spatial0; \
                int * p_off; /* input coord offset (from destination coord) */ \
                int * p_st; /* input coord start */ \
                int * p_en; /* input coord end, >= start*/ \
                ShortLoop() for(int i=0; i<dvl; ++i) \
                ssz[i] = 1; \
                if (dm >= 5) { \
                    p_off = dhw + 8*MVL; \
                    p_st  = dhw + 9*MVL; \
                    p_en  = dhw + 10*MVL; \
                    k_st  = dhw + 11*MVL; \
                    POOL_WINDOW_LOOP(d_spatial, dvl, SD, padF, KD, ID, \
                            p_off, p_st, p_en, ssz, k_st); \
                    d_spatial += MVL; /*dcrd.MaxVl*/ \
                } \
                if (dm >= 4) { \
                    p_off = dhw + 4*MVL; \
                    p_st  = dhw + 5*MVL; \
                    p_en  = dhw + 6*MVL; \
                    k_st  = dhw + 7*MVL; \
                    POOL_WINDOW_LOOP(d_spatial, dvl, SH, padT, KH, IH, \
                            p_off, p_st, p_en, ssz, k_st); \
                    d_spatial += MVL; /*dcrd.MaxVl*/ \
                } \
                p_off = dhw + 0*MVL; \
                p_st  = dhw + 1*MVL; \
                p_en  = dhw + 2*MVL; \
                p_en  = dhw + 3*MVL; \
                POOL_WINDOW_LOOP(d_spatial, dvl, SW, padL, KW, IW, \
                        p_off, p_st, p_en, ssz, k_st); \
            } while(0)
#endif

            {
                int v_g[MVL];
                int v_oc[MVL];
                VFOR(i,dvl) {
                    v_g[i] = v_ocxg[i] / OC;
                    v_oc[i] = v_ocxg[i] % OC;
                }
                COORD_VEC_REGISTER(v_mb, dcrd_outer0);
                // v_ocxg already set
                COORD_VEC_REGISTER(v_od, (dm >= 5? (dcrd_i+=MVL): dim_zeros));
                COORD_VEC_REGISTER(v_oh, (dm >= 4? (dcrd_i+=MVL): dim_zeros));
                COORD_VEC_REGISTER(v_ow, dcrd_i+=MVL); // always exists, dm >= 3
#undef COORD_VEC_REGISTER
                if (mypd->ker_type() == 0) { // ref:any
                    NOVEC VFOR(i,dvl) { // fn calls, novec
#if 1
                        a[i] += ker6(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);
#elif 0
#endif
                    }
                } else { // ref:plain
                    VFOR(i,dvl) { // fn calls, novec
                        a[i] += ker_plain6(v_g[i], v_mb[i], v_oc[i], v_od[i], v_oh[i], v_ow[i]);
                    }
                }
            }

            //maybe_oscale(a, g, oc);
            //VFOR(i,dvl) a[i] *= scales[v_ocxg[i] * scale_idx_mult];
            if (scale_idx_mult) VFOR(i,dvl) a[i] *= scales[v_ocxg[i]];
            else                VFOR(i,dvl) a[i] *= scales[0];

            maybe_postops_vec3(a, dst_gather, dvl); // faster as lambda? go figure

            VFOR(i,dvl) {
                // technically wrong for OPT==1, but faster !!!
                using cvt_dst = Cvt<dst_data_t, is_int_conv>;
                dst_gather[i] = cvt_dst::qs(a[i]);
                dst[dst_off[i]] = dst_gather[i];
            }
        }
    };
#else
#error "FWD_IMPL not implemented"
#endif // FWD_IMPL choice
#endif // FWD_IMPL>0

#if FWD_IMPL==0
#warning "ref_convolution.cpp FWD_IMPL==0"
    // TODO ||ize offset calcs for VE
    parallel_nd(G, MB, OC, OD, OH, OW,
            [&](int g, int mb, int oc, int od, int oh, int ow) {
                float a = bias ? get_bias(bias, bias_d.off(g * OC + oc),
                                  mypd->desc()->bias_desc.data_type)
                               : 0;

                if (src_d.is_plain() && weights_d.is_plain()
                        && src_ic_stride == 1 && weights_kw_stride == 1)
                    a += ker_plain(g, mb, oc, od, oh, ow);
                else
                    a += ker(g, mb, oc, od, oh, ow);

                const dim_t dst_off = off_abx(dst_d, 0, mb, g * OC + oc, od, oh, ow);

                maybe_oscale(a, g, oc);
                maybe_postops(a, dst[dst_off]);

                using cvt_dst = Cvt<dst_data_t, is_int_conv>;
                dst[dst_off] = cvt_dst::qs(a);
            });
#elif FWD_IMPL==1
    bool constexpr force_sequential = 0; // 1 for debug
    parallel((force_sequential? 1: 0), kern1);
#elif FWD_IMPL==2
    bool constexpr force_sequential = 0; // 1 for debug
    parallel((force_sequential? 1: 0), kern2);
#elif FWD_IMPL==3
    bool constexpr force_sequential = 0; // 1 for debug
    parallel((force_sequential? 1: 0), kern3);
#elif FWD_IMPL==4
    bool constexpr force_sequential = 0; // 1 for debug
    parallel((force_sequential? 1: 0), kern4);
#elif FWD_IMPL==5
    bool constexpr force_sequential = 0; // 1 for debug
    parallel((force_sequential? 1: 0), kern5);
#elif FWD_IMPL==6
    bool constexpr force_sequential = 0; // 1 for debug
    parallel((force_sequential? 1: 0), kern6);
#else
#error "unknown FWD_IMPL"
#endif
}

using namespace data_type;

template struct ref_convolution_fwd_t<f32>;
template struct ref_convolution_fwd_t<bf16,bf16,bf16,f32>;
template struct ref_convolution_fwd_t<bf16,bf16,f32,f32>;

template struct ref_convolution_fwd_t<u8, s8, f32, s32>;
template struct ref_convolution_fwd_t<u8, s8, s32, s32>;
template struct ref_convolution_fwd_t<u8, s8, s8, s32>;
template struct ref_convolution_fwd_t<u8, s8, u8, s32>;
template struct ref_convolution_fwd_t<s8, s8, f32, s32>;
template struct ref_convolution_fwd_t<s8, s8, s32, s32>;
template struct ref_convolution_fwd_t<s8, s8, s8, s32>;
template struct ref_convolution_fwd_t<s8, s8, u8, s32>;

} // namespace cpu
} // namespace impl
} // namespace dnnl

// vim: et ts=4 sw=4 cindent cino=+2s,l0,\:4,N-s
#endif // FWD_IMP==-1
