/*******************************************************************************
* Copyright 2020 NEC Labs America LLC
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

#ifndef CPU_VE_SIMPLE_Q10N_VEC_F32_U8_HPP
#define CPU_VE_SIMPLE_Q10N_VEC_F32_U8_HPP
#include "cpu/ve/simple_q10n_vec.hpp"

/** \file TEMPORARY q10n specialization for f32 to u8 round(saturate) */

namespace dnnl {
namespace impl {
namespace cpu {

// f32->u8 direct 6x2x1024x4x102 --mode=C
// 0 --> 6.34595 (cf. generic w/ oscale=1 or 2 of 23.27 ms)
// 1 --> 6.35303 --> 3.00 | 18 | ...
//   -->  w/ st1b asm loop
template <>
struct qzv_a1b0<float, uint8_t, void>
{
    void operator()(float const* in, uint8_t *out, unsigned const len) {
        //printf("  YAHOO  "); fflush(stdout);
        size_t const blk = MVL;
        // change to saturate, then round, like trunk
        // first convert to int32 all with vector ops
        int32_t sat[MVL] __attribute__((aligned(8))); // later might used packed ops?
        NOVECTOR for (size_t i=0; i<len; i+=blk){
            size_t const vl = (len-i > blk? blk: len-i);
            // TODO mxcsr_roundv(float*, float lbound, float ubound, vl, int*) ??
            // (all in one asm construct)
            {
                int a; // tmp register
                asm("\t# VE_ROUND_FLOAT_TO_INT_SAT_U8\n\t"
                        "lvl %[vl]\n\t"
                        "vldu.nc %v63, 4, %[in]\n\t"
                        "\n"
                        "#      f32 saturate between 0.0f and 255.0f\n\t"
                        "and %[a], 0, (0)1\t\t# (i32)0.0f\n\t"
                        "vfmax.s %v63, %[a], %v63\n\t"
                        "lea.sl  %[a],1132396544\t\t# (i32)255.0f\n\t"
                        "vfmin.s %v63, %[a], %v63\n\t"
                        "\n"
                        "#      cvt float to int32, like nearbyintf\n\t"
                        "vcvt.w.s.sx.rz %v62, %v63\n\t"
                        "vstl %v62, 4, %[out] # int32 output in lower half\n\t"
                        : [a]"+r"(a)
                        , "=m"( /*dummy output*/ *(int (*)[vl]) &sat[0] )
                        : [in]"r"(&in[i]), [vl]"r"(vl), [out]"r"(&sat[0])
                        , "m"( /*dummy input*/ *(const float (*)[vl]) &in[i])
                        : "%v62", "%v63", "%s63"
                   );
            }

#define Q_FLT_U8_VERIFY 1
#if Q_FLT_U8_VERIFY
            // fill output with zeroes, initially.
            NOVECTOR for(int j=0; j<vl; ++j)
                    out[i+j] = '\0'; // for test
#endif



            // method:
            // default direct_copy                     ~47 ms
            // 0 : simplest, scalar copy loop           2.96 ms
            //     
            // 1 : more complex, vectorize attempt      18.9 ms
            // 2 : WIP (segfault) loop of 1 might be tiny bit better
            // 3 : 3.09
            // 4 : 3.09 --> 2.60 (8-loop all asm)
            // 5 : 2.60
#define Q_FLT_U8 5
           
#if Q_FLT_U8 == 0
            // Perf 3.65 ms
            // simple, all-sclar, "just works"
            // apart from srl;sll (not needed) loop looks tight.
            uint32_t const* s = (uint32_t const*)(void const*)&sat[0];
            uint8_t *o = &out[i];
            // --j loop is much better
            NOVECTOR for (int j=vl; --j>=0; ++o, ++s){
                //*o = (uint8_t) *s; // *s << 56 >> 56
                *o = *s; // still "*s << 56 >> 56"
            }
            // bad -- asm halts optimization
            //unsigned j4 = vl/4*4; // asm kills optimization...
#elif Q_FLT_U8 == 1
            // pack first into aligned buf w/ vector ops, them memcpy ??
            //
            // EXCEPT builtin_memcpy aborts compiler
            //         while trying schedule the memcpy
            //
            int32_t const *s = &sat[0];

            // TMPBUF 0 should work nicely, but has compiler error
            // compiler error seems related to asm comments ?
#define TMPBUF 0

#if TMPBUF
            uint64_t tmp[ MVL / 8 ];
            uint64_t *t = &tmp[0];
            //uint8_t *b = (uint8_t*)(void*)&tmp[0];
#endif
            uint8_t * __restrict o = &out[i];
            // NOTE: vsrl sat[], [{0,1,2,...}*8] would do all the shifts,
            //   leaving ready to '&' in sets of 8.
            // - then write to mem[0..vl-1],
            // - reread mem with vl=8 and vrand (vector-reduce-and)
            // - write vrand results into tmp[0..vl/8-1]
            // - (remainder as usual)
            //asm("# j8 loop");

            int j8full = vl >> 3;
            if (j8full) {
                uint32_t shif[8]; VREG(shif); //{0,8,16,24,32,40,48,56};
                for(int n=0; n<8; ++n) shif[n] = n*8;
                NOVECTOR for ( ; j8full; s+=8, --j8full)
                {
                    uint64_t merged;
#if 0 // orig, OK
                    // order might be reversed (machine endianness)
                    merged =   (int64_t)s[0]
                            + ((int64_t)s[1] << 8)
                            + ((int64_t)s[2] << 16)
                            + ((int64_t)s[3] << 24)
                            + ((int64_t)s[4] << 32)
                            + ((int64_t)s[5] << 40)
                            + ((int64_t)s[6] << 48)
                            + ((int64_t)s[7] << 56);
#else
                    // rewrite as compact vectorization:
                    // vec load, cvt to u64, vec shift
                    // final xor-merge of shited byte values
                    // - nned ShortLoop to generate compact asm
                    merged = 0;
                    ShortLoop() for(int n=0; n<8; ++n) {
                        merged |= ((uint64_t)s[n] << shif[n]);
                    }
#endif

#if TMPBUF
                    *t = merged;
                    ++t;
#elif 1 // write directly to output
#if 0 // fairly ugly
                    {
                        uint8_t const* __restrict const b = (uint8_t *)(void*)&merged;
                        uint8_t      * __restrict const c = o;
                        // try output write in same loop...
                        // XXX check alignment and faster write possibilities
                        c[0] = b[0];
                        c[1] = b[1];
                        c[2] = b[2];
                        c[3] = b[3];
                        c[4] = b[4];
                        c[5] = b[5];
                        c[6] = b[6];
                        c[7] = b[7];
                    }
#else
                    uint64_t a,b,c,d; // tmp registers
                    asm("# 8-byte arb align bytewise copy\n\t"
                            "ld1b.zx    %s[tmp0], 0(,%[src])\n\t"
                            "ld1b.zx    %s[tmp1], 1(,%[src])\n\t"
                            "ld1b.zx    %s[tmp2], 2(,%[src])\n\t"
                            "ld1b.zx    %s[tmp3], 3(,%[src])\n\t"
                            "st1b       %s[tmp0], 0(,%[dst])\n\t"
                            "st1b       %s[tmp1], 1(,%[dst])\n\t"
                            "st1b       %s[tmp2], 2(,%[dst])\n\t"
                            "st1b       %s[tmp3], 3(,%[dst])\n\t"
                            "ld1b.zx    %s[tmp0], 4(,%[src])\n\t"
                            "ld1b.zx    %s[tmp1], 5(,%[src])\n\t"
                            "ld1b.zx    %s[tmp2], 6(,%[src])\n\t"
                            "ld1b.zx    %s[tmp3], 7(,%[src])\n\t"
                            "st1b       %s[tmp0], 4(,%[dst])\n\t"
                            "st1b       %s[tmp1], 5(,%[dst])\n\t"
                            "st1b       %s[tmp2], 6(,%[dst])\n\t"
                            "st1b       %s[tmp3], 7(,%[dst])\n\t"
                            : [tmp0]"=r"(a), [tmp1]"=r"(b), [tmp2]"=r"(c), [tmp3]"=r"(d) // m
                            : [src]"r"(&merged), [dst]"r"(o)
                            //, m
                            :
                            );
#endif
                    o += 8;
                    //b += 8;
#endif
                }
            }
            int j8rem = (vl & 0x7);
            if (j8rem) {
                //asm("# j8 remainder loop\n\t");
                // bytewise remainder loop
#if TMPBUF
                uint8_t *b = (uint8_t *)(void*)t;
                NOVECTOR for( ; j8rem; ++b, ++s, --j8rem){
                    *b = *s;
                }
#else // write remainder directly to output
                NOVECTOR for( ; j8rem; ++o, ++s, --j8rem){
                    *o = *s;
                }
#endif
            }
            //asm("# j8 done. sat[vl] packed as u8 into aligned tmp[] buffer\n\t");
#if 0 && TMPBUF
            if (0) { // check above loops
                s = &sat[0];
                uint8_t *b = (uint8_t*)(void*)&tmp[0];
                int nerr = 0;
                for (int j=0; j<vl; ++j) {
                    //assert( b[j] == s[j] ); // equality, for uint8_t
                    if( (uint32_t)b[j] != (uint32_t)s[j] ){
                        printf(" oops b[%d] != s[%d] (0x%04x vs. 0x%04x)\n",
                                j, j, (uint32_t)b[j], (uint32_t)s[j] );
                        ++nerr;
                    }
                }
                if (nerr) {
                    fflush(stdout);
                    exit(13);
                }
            }
#endif

#if TMPBUF
            //
            // punt to memcpy to deal with ptr alignment and vectorization
            // (potentially fast if no fn call overhead)
            //
            //memcpy( (void*)&out[i], (void const*)&tmp[0], vl );
            //
            // CHECK if out is 4 or 8-byte aligned and vld/vst that section XXX
            //
            if (1) { // bytewise cpy tmp[] to out ** should combine w/ j8full/j8rem loops
                uint8_t *b = (uint8_t*)(void*)&tmp[0];
                uint8_t *o = &out[i];
#if 1 // this works
                NOVECTOR for (int j=vl ; j; --j) {
                    *o++ = *b++;
                }
                asm("# end slow bytewise copy... but compiles\n\t");
#elif 0 // nc++ compiler error!
                int j8full = vl >> 3;
                if (j8full) {
                    // compiler error :(
                    NOVECTOR for (int ful = (vl>>3) ; ful; b+=8, o+=8, --ful) {
                        o[0] = b[0];
                        o[1] = b[1];
                        o[2] = b[2];
                        o[3] = b[3];
                        o[4] = b[4];
                        o[5] = b[5];
                        o[6] = b[6];
                        o[7] = b[7];
                    }
                }
                int j8rem = (vl & 0x7);
                if (j8rem) {
                    NOVECTOR for ( ; j8rem; ++o, ++b, --j8rem) {
                        *o = *b;
                    }
                }
#endif
            }
#endif // TMPBUF final "memcpy"

#elif Q_FLT_U8 == 2 // see segfaults here.
            //
            // before writing the 'C' loop carefully, tried asm tight loop
            // (but still can generate much reg spillage, reg copies, ...)
            //
            if(1) {
                //printf(" vl=%d asm ", (int)vl); fflush(stdout);
                // if align of out[i] is 4, try vector-pack, vector write XXX
                uint64_t a; // tmp register
                asm("# scalar s32-->u8 bytwise copy\n\t"
                        //"adds.l     %[s], -4, %[s]          # --s\n\t"
                        //"adds.l     %[o], -1, %[o]          # --o\n"
                        "br.w   2f\n\t"
                        "\n    1:\n\t"
                        "adds.l     %[s], 4, %[s]           # ++s\n\t"
                        "adds.l     %[o], 1, %[o]           # ++o\n"
                        "    2:\n\t"
                        "ldl.sx     %[a], 0(,%[s])          # a = load *s\n\t"
                        "adds.w.sx  %[j], -1, %[j]          # --j\n\t"
                        //"sll        %[a], %[a], 56          # a = (uint8_t)a step 1\n\t"
                        //"srl        %[a], %[a], 56          # a = (uint8_t)a step 2\n\t"
                        //"and        %[a], %[a], (56)0       # 1-op cvt, not req for u8\n\t"
                        "st1b       %[a], 0(,%[o])          # *o = (uint8_t)a\n\t"
                        //"brle.w    0, %[j], .L_scalar_copy\n\t"
                        "brle.w     0, %[j], 1b\n\t"
                        : [a]"=r"(a)
                        //, [s]"+r"(s), [o]"+r"(o)
                        //, "=m"( /*dummy output*/ *(uint8_t (*)[vl]) o)
                        //: //[s]"r"(s), [o]"r"(o), [j]"r"(vl) 
                        : [j]"r"(vl)
                        , [s]"r"(&sat[0]), [o]"r"(&out[i])
                        //, "m"( /*dummy input*/ *(const uint32_t (*)[vl]) s)
                        :
                   );
            }
#elif Q_FLT_U8 == 3
            {
                // Perf  3.09 ms (code looks horrid)
                uint32_t const* s = (uint32_t const*)(void const*)&sat[0];
                uint8_t *o = &out[i];
                int j = vl;
                if (j&1) {
                    *o++ = *s++;
                    --j;
                }
                if (j&3) {
                    o[0] = s[0];
                    o[1] = s[1];
                    o+=2; s+=2; j-=2;
                }
                if (j&7) {
                    o[0] = s[0];
                    o[1] = s[1];
                    o[2] = s[2];
                    o[3] = s[3];
                    o+=4; s+=4; j -= 4;
                }
                for ( ; j ; s+=8, o+=8, j-=8) {
#if 0
                    o[0] = s[0];
                    o[1] = s[1];
                    o[2] = s[2];
                    o[3] = s[3];
                    o[4] = s[4];
                    o[5] = s[5];
                    o[6] = s[6];
                    o[7] = s[7];
#else
                    uint64_t a,b,c,d; // tmp registers
                    asm("# 8-byte arb align bytewise copy\n\t"
                            "ldl.zx     %[tmp0], 0(,%[src])\n\t"
                            "ldl.zx     %[tmp1], 4(,%[src])\n\t"
                            "ldl.zx     %[tmp2], 8(,%[src])\n\t"
                            "ldl.zx     %[tmp3], 12(,%[src])\n\t"
                            "st1b       %[tmp0], 0(,%[dst])\n\t"
                            "st1b       %[tmp1], 1(,%[dst])\n\t"
                            "st1b       %[tmp2], 2(,%[dst])\n\t"
                            "st1b       %[tmp3], 3(,%[dst])\n\t"
                            "ldl.zx     %[tmp0], 16(,%[src])\n\t"
                            "ldl.zx     %[tmp1], 20(,%[src])\n\t"
                            "ldl.zx     %[tmp2], 24(,%[src])\n\t"
                            "ldl.zx     %[tmp3], 28(,%[src])\n\t"
                            "st1b       %[tmp0], 4(,%[dst])\n\t"
                            "st1b       %[tmp1], 5(,%[dst])\n\t"
                            "st1b       %[tmp2], 6(,%[dst])\n\t"
                            "st1b       %[tmp3], 7(,%[dst])\n\t"
                            : [tmp0]"=r"(a), [tmp1]"=r"(b), [tmp2]"=r"(c), [tmp3]"=r"(d) // m
                            : [src]"r"(s), [dst]"r"(o)
                            //, m
                            :
                            );
#endif
                }
            }
#elif Q_FLT_U8 == 4
            {
                // Perf  3.09 (same)
                // still huge spill/restore between asm blocks
                uint64_t a,b,c,d; // tmp registers
                uint32_t const* s = (uint32_t const*)(void const*)&sat[0];
                uint8_t *o = &out[i];
                int j = vl;
                if (j&1) {
                    *o++ = *s++;
                    --j;
                }
#if 1
                if (j&3) {
                    asm("# 2-byte arb align bytewise copy\n\t"
                            "ldl.zx     %[tmp0], 0(,%[src])\n\t"
                            "ldl.zx     %[tmp1], 4(,%[src])\n\t"
                            "st1b       %[tmp0], 0(,%[dst])\n\t"
                            "st1b       %[tmp1], 1(,%[dst])\n\t"
                            : [tmp0]"=r"(a), [tmp1]"=r"(b) //, m
                            : [src]"r"(s), [dst]"r"(o) //, m
                            :
                            );
                    o+=2; s+=2; j-=2;
                }
#else
                    asm("# 2-byte arb align bytewise copy\n\t"
                        "and    %[tmp1], 1, %[j]\n\t"
                        "breq.w 0, %[tmp1], 1f\n\t"
                        "ldl.zx %[tmp0], 0(,%[src])\n\t"
                        "ldl.zx %[tmp1], 4(,%[src])\n\t"
                        "st1b   %[tmp0], 0(,%[dst])\n\t"
                        "st1b   %[tmp1], 1(,%[dst])\n\t"
                        "    1:\n\t"
                        : [tmp0]"=r"(a), [tmp1]"=r"(b), [j]"=r"(j) //, m
                        : [src]"r"(s), [dst]"r"(o) //, m
                        :
                       );
#endif
                if (j&7) {
                    asm("# 4-byte arb align bytewise copy\n\t"
                            "ldl.zx     %[tmp0], 0(,%[src])\n\t"
                            "ldl.zx     %[tmp1], 4(,%[src])\n\t"
                            "ldl.zx     %[tmp2], 8(,%[src])\n\t"
                            "ldl.zx     %[tmp3], 12(,%[src])\n\t"
                            "st1b       %[tmp0], 0(,%[dst])\n\t"
                            "st1b       %[tmp1], 1(,%[dst])\n\t"
                            "st1b       %[tmp2], 2(,%[dst])\n\t"
                            "st1b       %[tmp3], 3(,%[dst])\n\t"
                            : [tmp0]"=r"(a), [tmp1]"=r"(b), [tmp2]"=r"(c)
                            , [tmp3]"=r"(d) //, m
                            : [src]"r"(s), [dst]"r"(o) //, m
                            :
                            );
                    o+=4; s+=4; j -= 4;
                }
#if 0
                for ( ; j>0 ; s+=8, o+=8, j-=8) {
                    asm("# 8-byte arb align bytewise copy\n\t"
                            "ldl.zx     %[tmp0], 0(,%[src])\n\t"
                            "ldl.zx     %[tmp1], 4(,%[src])\n\t"
                            "ldl.zx     %[tmp2], 8(,%[src])\n\t"
                            "ldl.zx     %[tmp3], 12(,%[src])\n\t"
                            "st1b       %[tmp0], 0(,%[dst])\n\t"
                            "st1b       %[tmp1], 1(,%[dst])\n\t"
                            "st1b       %[tmp2], 2(,%[dst])\n\t"
                            "st1b       %[tmp3], 3(,%[dst])\n\t"
                            "ldl.zx     %[tmp0], 16(,%[src])\n\t"
                            "ldl.zx     %[tmp1], 20(,%[src])\n\t"
                            "ldl.zx     %[tmp2], 24(,%[src])\n\t"
                            "ldl.zx     %[tmp3], 28(,%[src])\n\t"
                            "st1b       %[tmp0], 4(,%[dst])\n\t"
                            "st1b       %[tmp1], 5(,%[dst])\n\t"
                            "st1b       %[tmp2], 6(,%[dst])\n\t"
                            "st1b       %[tmp3], 7(,%[dst])\n\t"
                            : [tmp0]"=r"(a), [tmp1]"=r"(b), [tmp2]"=r"(c)
                            , [tmp3]"=r"(d) //, m
                            //, [src]"+r"(s), [dst]"+r"(o) //, m
                            : [src]"r"(s), [dst]"r"(o) //, m
                            :
                            );
                }
#else
                //for ( ; j ; s+=8, o+=8, j-=8) {
                //printf(" j%d",(int)j);
                    asm("# 8-byte arb align bytewise copy\n\t"
                            "brge.w     0, %[j], 2f\n\t"
                            "\n1: # loop\n\t"
                            "ldl.zx     %[tmp0], 0(,%[src])\n\t"
                            "ldl.zx     %[tmp1], 4(,%[src])\n\t"
                            "ldl.zx     %[tmp2], 8(,%[src])\n\t"
                            "ldl.zx     %[tmp3], 12(,%[src])\n\t"
                            "st1b       %[tmp0], 0(,%[dst])\n\t"
                            "st1b       %[tmp1], 1(,%[dst])\n\t"
                            "st1b       %[tmp2], 2(,%[dst])\n\t"
                            "st1b       %[tmp3], 3(,%[dst])\n\t"
                            "ldl.zx     %[tmp0], 16(,%[src])\n\t"
                            "ldl.zx     %[tmp1], 20(,%[src])\n\t"
                            "ldl.zx     %[tmp2], 24(,%[src])\n\t"
                            "ldl.zx     %[tmp3], 28(,%[src])\n\t"
                            "adds.w.sx  %[j], -8, %[j]\n\t"
                            "st1b       %[tmp0], 4(,%[dst])\n\t"
                            "st1b       %[tmp1], 5(,%[dst])\n\t"
                            "adds.l     %[src], 32, %[src]\n\t"
                            "st1b       %[tmp2], 6(,%[dst])\n\t"
                            "st1b       %[tmp3], 7(,%[dst])\n\t"
                            "adds.l     %[dst],  8, %[dst]\n\t"
                            "brlt.w     0, %[j], 1b\n\t"
                            "\n2: # done\n\t"
                            : [tmp0]"=r"(a), [tmp1]"=r"(b)
                            , [tmp2]"=r"(c), [tmp3]"=r"(d)
                            , [j]"+r"(j), [src]"+r"(s), [dst]"+r"(o) //, m
                            :
                            :
                            );
                //printf(":%d",(int)j);
                //}
#endif
            }
#elif Q_FLT_U8 == 5
            // Perf 1.23 ms (another 2x)
            uint64_t a,b,c,d; // tmp registers
            uint32_t const* s = (uint32_t const*)(void const*)&sat[0]; // sat is aligned(8)
            uint8_t *o = &out[i];
            int j = vl;
                        //"and    %[tmp1], 1, %[j]\n\t"
                        //"breq.w 0, %[tmp1], 1f\n\t"
            asm("# memcpy_32bit_8lsbs(8bit* o, 32bit* s, vl)\n\t"
                    "# Alternate to remove 'and's : brgt.w.nt 7, %[cnt], 1f # else 8-loop\n\t"
                    "# prefer alt because align(8) of s destroyed only at end!\n\t"
                    "# also should use ld + shift for align(8) reads\n\t"
                    "\n    #1: # if (cnt&1) process 1 item\n\t"
                    //"and        %[d], 1, %[cnt]\n\t"
                    "and        %[d], %[cnt], (63)0\n\t"
                    "breq.w     0, %[d], 2f\n\t"
                    "ldl.zx     %[a], 0(,%[src])\n\t"
                    "  adds.w.sx  %[cnt], -1, %[cnt]\n\t"
                    "st1b       %[a], 0(,%[dst])\n\t"
                    "  addu.l     %[src],  1, %[src]\n\t"
                    "  addu.l     %[dst],  4, %[dst]\n\t"
                    "\n    2: # if (cnt&3) 2-items\n\t"
                    //"and        %[d], 3, %[cnt]\n\t"
                    "and        %[d], %[cnt], (62)0\n\t"
                    "breq.w     0, %[d], 3f\n\t"
                    "ldl.zx     %[a], 0(,%[src])\n\t"
                    "ldl.zx     %[b], 4(,%[src])\n\t"
                    "  adds.w.sx  %[cnt], -2, %[cnt]\n\t"
                    "st1b       %[a], 0(,%[dst])\n\t"
                    "st1b       %[b], 1(,%[dst])\n\t"
                    "  addu.l     %[src],  2, %[src]\n\t"
                    "  addu.l     %[dst],  8, %[dst]\n\t"
                    "\n    2: # if (cnt&7) 4-items\n\t"
                    //"and        %[d], 7, %[cnt]\n\t"
                    "and        %[d], %[cnt], (61)0\n\t"
                    "breq.w     0, %[d], 3f\n\t"
                    "ldl.zx     %[a], 0(,%[src])\n\t"
                    "ldl.zx     %[b], 4(,%[src])\n\t"
                    "ldl.zx     %[c], 8(,%[src])\n\t"
                    "ldl.zx     %[d], 12(,%[src])\n\t"
                    "  adds.w.sx  %[cnt], -4, %[cnt]\n\t"
                    "st1b       %[a], 0(,%[dst])\n\t"
                    "st1b       %[b], 1(,%[dst])\n\t"
                    "st1b       %[c], 2(,%[dst])\n\t"
                    "st1b       %[d], 3(,%[dst])\n\t"
                    "  addu.l     %[src],  4, %[src]\n\t"
                    "  addu.l     %[dst],  16, %[dst]\n\t"
                    "\n    3: # if (cnt)\n\t"
                    "brge.w     0, %[cnt], 5f\n\t"
                    "\n    4: # bytewise 8-item loop\n\t"
                    "ldl.zx     %[a], 0(,%[src])\n\t"
                    "ldl.zx     %[b], 4(,%[src])\n\t"
                    "ldl.zx     %[c], 8(,%[src])\n\t"
                    "ldl.zx     %[d], 12(,%[src])\n\t"
                    "st1b       %[a], 0(,%[dst])\n\t"
                    "st1b       %[b], 1(,%[dst])\n\t"
                    "st1b       %[c], 2(,%[dst])\n\t"
                    "st1b       %[d], 3(,%[dst])\n\t"
                    "ldl.zx     %[a], 16(,%[src])\n\t"
                    "ldl.zx     %[b], 20(,%[src])\n\t"
                    "ldl.zx     %[c], 24(,%[src])\n\t"
                    "ldl.zx     %[d], 28(,%[src])\n\t"
                    "  adds.w.sx  %[cnt], -8, %[cnt]\n\t"
                    "st1b       %[a], 4(,%[dst])\n\t"
                    "st1b       %[b], 5(,%[dst])\n\t"
                    "st1b       %[c], 6(,%[dst])\n\t"
                    "st1b       %[d], 7(,%[dst])\n\t"
                    "  addu.l     %[src],  8, %[src]\n\t"
                    "  addu.l     %[dst],  32, %[dst]\n\t"
                    "brlt.w     0, %[cnt], 4b\n\t"
                    "\n    5: # done bytewise i32-->u8 copy\n\t"
                    : [a]"=r"(a), [b]"=r"(b) , [c]"=r"(c), [d]"=r"(d)
                    , [cnt]"+r"(j), [src]"+r"(s), [dst]"+r"(o) //, m
                    : //[src]"r"(s), [dst]"r"(o), [cnt]"r"(vl) //, m
                    :
               );
#elif Q_FLT_U8 == 6
            // Perf 3.55 ms (another 2x)
            uint64_t a,b,c,d; // tmp registers
            uint32_t const* s = (uint32_t const*)(void const*)&sat[0]; // sat is aligned(8)
            uint8_t *o = &out[i];
            int j = vl;
                        //"and    %[tmp1], 1, %[j]\n\t"
                        //"breq.w 0, %[tmp1], 1f\n\t"
            // 32-bit src is 8-byte-aligned.  We use this fact to
            // load 64-bits, and shift to produce 2 st1b outputs.
            asm("# memcpy_32bit_8lsbs(8bit* o, 32bit* s, vl)\n\t"
                    "# Reorder to maintian align(8) of src as long as possible\n\t"
                    "\n    1: # while (8<=cnt) process 8 items, src-align(8)\n\t"
                    "brgt.w     8, %[cnt], 3f\n\t"
                    "\n    2: # bytewise 8-item loop\n\t"
                    "ldl.zx     %[a], 0(,%[src])\n\t"
                    "ldl.zx     %[b], 4(,%[src])\n\t"
                    "ldl.zx     %[c], 8(,%[src])\n\t"
                    "ldl.zx     %[d], 12(,%[src])\n\t"
                    "st1b       %[a], 0(,%[dst])\n\t"
                    "st1b       %[b], 1(,%[dst])\n\t"
                    "st1b       %[c], 2(,%[dst])\n\t"
                    "st1b       %[d], 3(,%[dst])\n\t"
                    "ldl.zx     %[a], 16(,%[src])\n\t"
                    "ldl.zx     %[b], 20(,%[src])\n\t"
                    "ldl.zx     %[c], 24(,%[src])\n\t"
                    "ldl.zx     %[d], 28(,%[src])\n\t"
                    "  adds.w.sx  %[cnt], -8, %[cnt]\n\t"
                    "st1b       %[a], 4(,%[dst])\n\t"
                    "st1b       %[b], 5(,%[dst])\n\t"
                    "st1b       %[c], 6(,%[dst])\n\t"
                    "st1b       %[d], 7(,%[dst])\n\t"
                    "  addu.l     %[src],  8, %[src]\n\t"
                    "  addu.l     %[dst],  32, %[dst]\n\t"
                    "brle.w     8, %[cnt], 2b\n\t"
                    "\n    3: # if (4<=cnt) process 4 items, src-align(8)\n\t"
                    "brgt.w     4, %[cnt], 4f\n\t"
                    "ldl.zx     %[a], 0(,%[src])\n\t"
                    "ldl.zx     %[b], 4(,%[src])\n\t"
                    "ldl.zx     %[c], 8(,%[src])\n\t"
                    "ldl.zx     %[d], 12(,%[src])\n\t"
                    "  adds.w.sx  %[cnt], -4, %[cnt]\n\t"
                    "st1b       %[a], 0(,%[dst])\n\t"
                    "st1b       %[b], 1(,%[dst])\n\t"
                    "st1b       %[c], 2(,%[dst])\n\t"
                    "st1b       %[d], 3(,%[dst])\n\t"
                    "  addu.l     %[src],  4, %[src]\n\t"
                    "  addu.l     %[dst],  16, %[dst]\n\t"
                    "\n    4: # if (2<=cnt) process 2 items, src-align(4)\n\t"
                    "brgt.w     2, %[cnt], 5f\n\t"
                    "ldl.zx     %[a], 0(,%[src])\n\t"
                    "ldl.zx     %[b], 4(,%[src])\n\t"
                    "  adds.w.sx  %[cnt], -2, %[cnt]\n\t"
                    "st1b       %[a], 0(,%[dst])\n\t"
                    "st1b       %[b], 1(,%[dst])\n\t"
                    "  addu.l     %[src],  2, %[src]\n\t"
                    "  addu.l     %[dst],  8, %[dst]\n\t"
                    "\n    5: # if (1<=cnt) process 1 items, src-align(4)\n\t"
                    "brgt.w     1, %[cnt], 6f\n\t"
                    "ldl.zx     %[a], 0(,%[src])\n\t"
                    "  adds.w.sx  %[cnt], -1, %[cnt]\n\t"
                    "st1b       %[a], 0(,%[dst])\n\t"
                    "  addu.l     %[src],  1, %[src] # final: don't care\n\t"
                    "  addu.l     %[dst],  4, %[dst] # final: don't care\n\t"
                    "\n    6: # aligned 32bit --> 8bit done\n\t"
                    : [a]"=r"(a), [b]"=r"(b), [c]"=r"(c), [d]"=r"(d) //, m
                    : [src]"r"(s), [dst]"r"(o), [cnt]"r"(vl) //, m
                    :
               );
#endif




#if Q_FLT_U8_VERIFY
            if (1) { //check above asm
                int nerr = 0;
                fflush(stdout);
                for(int j=0; j<vl; ++j){
                    if ((uint32_t)out[i+j] != (uint32_t)sat[j]) {
                        printf(" oops o[%d] != s[%d] (0x%04x vs. 0x%04x)\n",
                                j, j, (uint32_t)out[i+j], (uint32_t)sat[j]);
                        ++nerr;
                    }
                }
                if (nerr) {
                    fflush(stdout);
                    exit(13);
                }
            }
#endif
        }
    }
};

} // namespace cpu
} // namespace impl
} // namespace dnnl

// vim: et ts=4 sw=4 cindent cino=+2s,l0,\:4,N-s filetype=cpp
#endif // CPU_VE_SIMPLE_Q10N_VEC_F32_U8_HPP
