/*******************************************************************************
* Copyright 2016-2020 Intel Corporation
*
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*******************************************************************************/

#include <assert.h>
#include <type_traits>

#include "common/c_types_map.hpp"
#include "common/dnnl_thread.hpp"
#include "common/math_utils.hpp"
#include "common/type_helpers.hpp"

#include "cpu/ref_eltwise.hpp"
#include "cpu/ve/ref_convolution_util.hpp" //cvt interface to simple_q10n
#if defined(__ve)
#include "common/ve/memory_desc_wrapper_opt.hpp"
#include "common/dnnl_optimize.h"
#endif

namespace dnnl {
namespace impl {
namespace cpu {

#define DATA_OFF(f, n, c, d, h, w) \
    (ndims == 1) \
            ? (f).off(n) \
            : ((ndims == 2) ? (f).off(n, c) \
                            : ((ndims == 3) ? (f).off(n, c, w) \
                                            : ((ndims == 4) ? (f).off( \
                                                       n, c, h, w) \
                                                            : (f).off(n, c, d, \
                                                                    h, w))))

using namespace alg_kind;
using namespace math;

#ifndef MVL
#define MVL 256 /*FIXME*/
#endif

/** \file
 * should investigate:
 *
 * - vectorized log1pf
 * - "direct" impl of soft_relu (run Remez Exchange)
 * - generic impl offset calc NOT using memory_desc_wrapper_opt vectorization
 * - possible to reduce lambda usage more w/ helper structs? reduce code duplication?
 */
namespace {

// emulate 'if constexpr', pre c++17 (and eliminate < 0 warning for unsigned)
template<typename T> inline static
        typename std::enable_if<std::is_signed<T>::value, void>::type
        mkNonNegative(T &x) {
            x = (x < T{0}? T{0}: x);
        }
template<typename T> inline static
        typename std::enable_if<!std::is_signed<T>::value, void>::type
        mkNonNegative(T &x) {
            ;
        }

static inline double fast_exp_64(const double x) noexcept {
        // Based on Schraudolph 1999, A Fast, Compact Approximation of the Exponential Function.
        // - Adapted to use 64-bit integer; reduces staircase effect.
        // - Valid for x in approx range (-700, 700).
        union{double d_; int64_t i_;} uid; //This could be moved to the thread scope.
        //BBBD(sizeof(uid)!=8)
        uid.i_ = int64_t(double((int64_t(1) << 52) / log(2.0)) * x + double((int64_t(1) << 52) * 1023 - 0)); //c=0 for 1.0 at zero.
        return uid.d_;
    }

// VE optimization level may omit input arg range checks for speed.
// this can cause tests to fail where inputs are illegal, or when
// inputs approach +/-inf, NaN, +/-0 limit cases.
#define NO_RANGE_CHECKS 0
#define TU_MATH_FN template <typename T, \
        typename U = typename utils::remove_reference<T>::type> \
        static inline U
#define TAU_MATH_FN template <typename T, typename A, \
        typename U = typename utils::remove_reference<T>::type> \
        static inline U

// begin with all ve math funcs same as unchecked src/common/math_utils.hpp
// override "as needed" for VE
// note: also override as you use ve_ALG_fwd<T,U> syntax (to specify U)
//       also override to let nc++ inline (grrrr!)
//#define ve_relu_fwd(...) relu_fwd(__VA_ARGS__)
TAU_MATH_FN ve_relu_fwd(T s, A alpha) {
    return s > T{0}? (U)s: (U)(s * alpha);
}
#define ve_tanh_fwd(...) tanh_fwd(__VA_ARGS__)
#define ve_elu_fwd(...) elu_fwd(__VA_ARGS__)
#define ve_square_fwd(...) square_fwd(__VA_ARGS__)
#define ve_abs_fwd(...) abs_fwd(__VA_ARGS__)
#define ve_sqrt_fwd(...) sqrt_fwd(__VA_ARGS__)
#define ve_linear_fwd(...) linear_fwd(__VA_ARGS__)
#if 0
// nc++ won't vectorize mixed-type "max"
#define ve_bounded_relu_fwd(...) bounded_relu_fwd(__VA_ARGS__)
#else
TAU_MATH_FN ve_bounded_relu_fwd(T s, A alpha) {
    //T const t_alpha{alpha};
    T const t_alpha = (T)alpha; // narrowing float alpha-->int OK
    mkNonNegative(s);
    s = (s > t_alpha? t_alpha: s); // same-type important for vectorization
    return (U)s;
}
#endif
#if 0
// log1pf is NOT vectorized by nc++-3.0.27
#define ve_soft_relu_fwd(...) soft_relu_fwd(__VA_ARGS__)
#else
TU_MATH_FN ve_soft_relu_fwd(T s) {
#if NO_RANGE_CHECKS
    return (U)::logf(1.0f + ::expf((float)s));
#else
    //float const max_logf = 8.872284e+01; // ::log(FLT_MAX)
    //float const max_logf = 20.0f; // ::log(FLT_MAX)
    //float fs = (float)s; // narrowing int-->float is OK
    //return (U)(fs < max_logf? ::logf(1.0f + ::expf(fs)): fs);
#if 1 // normal
    // after 20, output value is equal to input with err < 2e-9 already
    float const fs = (float)s; // narrowing int-->float is OK
    return (U)( s < T{20}? ::logf(1.0f + ::expf(fs)): fs );
#else //debug
    float const max_logf = 20.0f; // ::log(FLT_MAX)
    float fs = (float)s; // narrowing int-->float is OK
    //printf("v"); fflush(stdout);
    float x = (fs < max_logf? ::logf(1.0f + ::expf(fs)): fs);
    float gold = (s < (T)logf(FLT_MAX)? (T)log1pf(::expf(fs)) : s);
    float diff = x - gold;
    if (fabs(diff) > 1e-7){
        printf(" soft_relu(x=%g) = %15.9f differs from gold %15.9f by %g\n",x,gold,diff);
    }
    //assert( fabs(x - log1pf(::expf(fs))) < 1.e-6f );
    return (U)x;
#endif
#endif
}
#endif
#if 0
//#define ve_logistic_fwd(...) logistic_fwd(__VA_ARGS__)
#else
template <typename T, typename U = typename utils::remove_reference<T>::type>
static inline U ve_logistic_fwd(T const s) {
    //float fs{s};
    float fs = (float)s; // narrowing int-->float is OK
#if !NO_RANGE_CHECKS
    float constexpr neg_max_logf = -87.0f; // exp(small x) ~ 0
    if (fs < neg_max_logf) fs = neg_max_logf;
#endif
    return logistic_fwd<float,U>(fs);
}
#endif
#if 0
#define ve_exp_fwd(...) exp_fwd(__VA_ARGS__)
#else
TU_MATH_FN ve_exp_fwd(T s) {
    float fs = (float)s; // narrowing int-->float is OK
#if NO_RANGE_CHECKS
    return ::expf(fs);
#elif 0 // simpler version
    float constexpr neg_max_logf = -87.0f; // exp(small x) ~ 0
    if (fs < neg_max_logf) fs = neg_max_logf;
    return exp_fwd<float,U>(fs);
#else // check both +/- inf limits
    //printf("v"); fflush(stdout);
    float constexpr float_inf = HUGE_VALF;
    float constexpr max_logf = 87.0f;
    return fs > max_logf? float_inf: fs < -max_logf? 0.0f: std::expf(fs);
#endif
}
#endif
#define ve_gelu_tanh_fwd(...) gelu_tanh_fwd(__VA_ARGS__)
#define ve_swish_fwd(...) swish_fwd(__VA_ARGS__)
#define ve_log_fwd(...) log_fwd(__VA_ARGS__)
#define ve_clip_fwd(...) clip_fwd(__VA_ARGS__)
#if NO_RANGE_CHECKS
#define ve_pow_fwd(...) pow_fwd(__VA_ARGS__)
#else
// this is SLOW -- eltwise_fwd primitive avoids this by vectorizing
// each special case individually.
TAU_MATH_FN ve_pow_fwd(T s, A alpha, A beta) {
    U ret;
    if (beta == A{0}) {
        ret = alpha;
    } else if (beta == A{1}) {
        ret = alpha * s;
    } else if (beta == 0.5f) {
#if NO_RANGE_CHECKS
        float const fs = (float)s;
#else
        float fs = (float)s; // narrowing int-->float is OK
        mkNonNegative(fs);
#endif
        ret = alpha * sqrtf(fs);
    } else if (beta == 1.5f) {
#if NO_RANGE_CHECKS
        float const fs = (float)s;
#else
        float fs = (float)s; // narrowing int-->float is OK
        mkNonNegative(fs);
#endif
        ret = (float)s * sqrtf(float(s));
    } else if (beta == A{2}) {
        ret = alpha * s * s;
#if !NO_RANGE_CHECKS
    } else if (beta < 0.f && beta == (float)(int)beta && (-(int)beta & 0x1) ) {
        //printf(" beta<0 is a -ve odd integer\n");
        // CORRECT powf(-0,beta) value is -inf
        // -O4 REF value is incorrect (-O0 is correct)
        //
        // (either/both fast-math and vector math funcs allow cheating on limit-cases)
        // (perhaps -O2 will give standards-compliant ::powf result? or an nc++ flag?)
        // alpha -ve/+ve is yet another corner case to fix
        float const fs = s;
        if (alpha >= 0.0f) {
            float const negzero = -0.0f;
            float const fixup = HUGE_VALF;
            ret = alpha * (fs == negzero
                    ? fixup : ::powf(fs, beta));
            //if (ret !=  alpha*::powf(fs,beta)) printf(" powf fixup+ fs=%g powf=%g ret=%g\n", fs, alpha*::powf(fs,beta), ret);
        }else{
            float const fixup = HUGE_VALF;
            ret = alpha * (::fabs(fs) == 0.0f
                    ? fixup : ::powf(fs, beta));
            //if (ret !=  alpha*::powf(fs,beta)) printf(" powf fixup fs=%g powf=%g ret=%g\n", fs, alpha*::powf(fs,beta), ret);
        }
#endif
    }else{
        //dst[e] = pow_fwd(src[e], alpha, beta);
        // beta=0.0f special case done above, avoid test...
        ret = alpha * ::powf((float)s, beta);
    }
    return ret;
}
#endif
#define ve_gelu_erf_fwd(...) gelu_erf_fwd(__VA_ARGS__)

// use_dst_for_bwd versions punt to previous ve_ versions
#define ve_relu_use_dst_for_bwd(...) ve_relu_fwd(__VA_ARGS__)
#define ve_tanh_use_dst_for_bwd(...) ve_tanh_fwd(__VA_ARGS__)
#define ve_elu_use_dst_for_bwd(...) ve_elu_fwd(__VA_ARGS__)
#define ve_sqrt_use_dst_for_bwd(...) ve_sqrt_fwd(__VA_ARGS__)
#define ve_logistic_use_dst_for_bwd(...) ve_logistic_fwd(__VA_ARGS__)
#define ve_exp_use_dst_for_bwd(...) ve_exp_fwd(__VA_ARGS__)

/** stack usage threshold (max array size) for channel offsets.
 *
 * \todo VE blocksz chooser for tmp arrays should have use a
 * template compiler-time const max size. Best value depends
 * on across/within/fwd/bwd. (Significant speed differences).
 * Compile-time bound on tmp stack arrays is frequently req'd
 * to allow nc++ to vectorize.
 *
 */
// oh, pragmas need this version (actual number, not constexpr)
#define STACK_ELEMS 4096
static dim_t constexpr stack_elems = STACK_ELEMS; // a generally decent value

static inline dim_t stack_friendly_blksz(dim_t const hi){
    // borrow from ve/ref_lrn.cpp
    dim_t ret = (hi>0? hi: 1);
    if (hi > stack_elems) {
        ret = stack_elems;
        dim_t const nFull = hi/stack_elems;
        dim_t const rem   = hi%stack_elems;
        if (rem < stack_elems/4) {
            dim_t const nLoops = nFull + (rem!=0);
            ret = (hi+nLoops-1) / nLoops;
            //printf("+%d",(int)ret); // rough equipartition
            if (ret < stack_elems - MVL) {
                ret = (ret+MVL-1)/MVL*MVL;
                //printf("^%d",(int)ret); // round up
            }
        }
    }
    assert( ret <= stack_elems );
    return ret;
}
/** but also want blksz low enough that max_threads can actually be used...
 * XXX return to this with measurements XXX. */
static inline dim_t stack_friendly_blksz(dim_t const hi, dim_t const other_work){
    dim_t stack_lim = (other_work*hi/dnnl_get_max_threads());
    // adjust following, which assumes one MVL is "enough work"
    // here MVL ~ min desirable blksz (but we can go lower, a bit)
    if (stack_lim < MVL) stack_lim = MVL;
    stack_lim = (stack_lim+(MVL-1)) / MVL * MVL;
    if (stack_lim > stack_elems) stack_lim = stack_elems;

    // now heuristic with possibly smaller version of stack_elems
    dim_t ret = (hi>0? hi: 1);
    if (hi > stack_lim) {
        ret = (stack_lim+31)/32*32;
        dim_t const nFull = hi/stack_lim;
        dim_t const rem   = hi%stack_lim;
        if (rem < MVL/4) {
            dim_t const nLoops = nFull + (rem!=0);
            ret = (hi+nLoops-1) / nLoops;
            //printf("+%d",(int)ret); // rough equipartition
            if (ret < stack_lim - MVL) {
                ret = (ret+MVL-1)/MVL*MVL;
                //printf("^%d",(int)ret); // round up
            }
        }
        ret = (ret+31)/32*32;
    }
    //assert( ret < stack_elems );
    return ret;
}

static float compute_eltwise_scalar_fwd(
        const alg_kind_t alg, float s, float alpha, float beta) {
    float d = 0.f;
    switch (alg) {
        case eltwise_relu: d = ve_relu_fwd(s, alpha); break;
        case eltwise_tanh: d = ve_tanh_fwd(s); break;
        case eltwise_elu: d = ve_elu_fwd(s, alpha); break;
        case eltwise_square: d = ve_square_fwd(s); break;
        case eltwise_abs: d = ve_abs_fwd(s); break;
        case eltwise_sqrt: d = ve_sqrt_fwd(s); break;
        case eltwise_linear: d = ve_linear_fwd(s, alpha, beta); break;
        case eltwise_bounded_relu: d = ve_bounded_relu_fwd(s, alpha); break;
        case eltwise_soft_relu: d = ve_soft_relu_fwd(s); break;
        case eltwise_logistic: d = ve_logistic_fwd(s); break;
        case eltwise_exp: d = ve_exp_fwd(s); break;
        case eltwise_gelu_tanh: d = ve_gelu_tanh_fwd(s); break;
        case eltwise_swish: d = ve_swish_fwd(s, alpha); break;
        case eltwise_log: d = ve_log_fwd(s); break;
        case eltwise_clip: d = ve_clip_fwd(s, alpha, beta); break;
        case eltwise_pow: d = ve_pow_fwd(s, alpha, beta); break;
        case eltwise_gelu_erf: d = ve_gelu_erf_fwd(s); break;

        case eltwise_relu_use_dst_for_bwd: d = ve_relu_fwd(s, alpha); break;
        case eltwise_tanh_use_dst_for_bwd: d = ve_tanh_fwd(s); break;
        case eltwise_elu_use_dst_for_bwd: d = ve_elu_fwd(s, alpha); break;
        case eltwise_sqrt_use_dst_for_bwd: d = ve_sqrt_fwd(s); break;
        case eltwise_logistic_use_dst_for_bwd: d = ve_logistic_fwd(s); break;
        case eltwise_exp_use_dst_for_bwd: d = ve_exp_fwd(s); break;

        default: assert(!"unknown eltwise alg_kind");
    }
    return d;
}

/** for alg_kind pow, you may precalculate and re-use the "beta" subcases.
 */
static inline int eltwise_fwd_subcase(const alg_kind_t alg, float const alpha, float const beta){
    int ret = 0;
    if (alg == eltwise_pow) {
        ret = (beta==0.f? 1
                : beta==1.f? 2
                : beta==0.5f? 3
                : beta==1.5f? 4
                : beta==2.f? 5
                : (beta < 0.f && beta == (float)(int)beta && (-(int)beta & 0x1) )? 6
                : 7);
    }
    return ret;
}

/** vector version of compute_eltwise_scalar_fwd.
 * \pre do NOT require vl <= maximum vector register length [at least for now]
 * \todo check VE other opportunities for compute_eltwise_vector_fwd.
 * \todo increase inline size limits to accomodate this fn (or make it a fn call)
 */
static void compute_eltwise_vector_fwd( const alg_kind_t alg,
        float alpha, float beta, float const scale,
        float * const d, float const* const s, int const vl,
        int const subalg=0/*can be in [1,15] for eltwise_pow dispatch*/) {
    switch (alg+subalg) {
#define vfor(...) for(int i=0; i<vl; ++i) {d[i] = (__VA_ARGS__)*scale;} break
//#define vfor(...) ShortLoop() for(int i=0; i<vl; ++i) {d[i] = (__VA_ARGS__)*scale;} break
        //case eltwise_relu: vfor(ve_relu_fwd(s[i], alpha));
        case eltwise_relu_use_dst_for_bwd:
        case eltwise_relu: vfor(s[i] > 0.f? s[i]: s[i] * alpha);
        case eltwise_tanh_use_dst_for_bwd:
        case eltwise_tanh: vfor(::tanhf(s[i]));
        case eltwise_elu_use_dst_for_bwd:
#if 1
        // ouch HORRIBLE %vm save,restore
        case eltwise_elu: vfor(s[i] > 0.f? s[i]: alpha * ::expm1f(s[i]));
#else // ouch vector save/restore around __vec_expm1f call
        case eltwise_elu: { for(int i=0; i<vl; ++i) {
            d[i] = alpha * ::expm1f(s[i]);
            d[i] = (s[i] < 0.f? d[i]: s[i]);
        }} break;
#endif
        case eltwise_square: vfor(s[i]*s[i]);
        case eltwise_abs: vfor(s[i] > 0.f? s[i]: -s[i]);
        case eltwise_sqrt_use_dst_for_bwd:
        case eltwise_sqrt: vfor(::sqrtf(s[i])); // VE does it in double, then cvts back to float
        case eltwise_linear: vfor(alpha * s[i] + beta);
        case eltwise_bounded_relu: { float fs;
            vfor(fs = (s[i] > 0.f? s[i]: 0.f), (fs > alpha? alpha : fs));
        }
        case eltwise_clip: { float fs;
            vfor(fs = (s[i] > alpha? s[i]: alpha), (fs > beta? beta : fs));
        }
        case eltwise_soft_relu: vfor(s[i] < 20.f? ::logf(1.0f + ::expf(s[i])): s[i]);
        case eltwise_logistic_use_dst_for_bwd:
        case eltwise_logistic: {
            float constexpr float_inf = HUGE_VALF;
            float constexpr max_logf = 87.0f;
            vfor(s[i] > max_logf? float_inf: s[i] < -max_logf? 0.0f: std::expf(s[i]));
        }
        case eltwise_exp_use_dst_for_bwd:
        case eltwise_exp: {
            float constexpr float_inf = HUGE_VALF;
            float constexpr max_logf = 87.0f;
            vfor(s[i] > max_logf? float_inf: s[i] < -max_logf? 0.0f: std::expf(s[i]));
        }
        case eltwise_gelu_tanh: { //vfor(ve_gelu_tanh_fwd(s[i]));
            constexpr float sqrt_2_over_pi = 0.797884;
            constexpr float fitting_const = 0.044715;
            for (int i=0; i<vl; ++i) {
                float v = ::tanhf(sqrt_2_over_pi * s[i] * (1 + fitting_const * s[i] * s[i]));
                d[i] = 0.5 * s[i] * (1. + v);
            }
            break;
        }
        case eltwise_swish: vfor(s[i] / (1.0f + ::expf(-alpha * s[i])));
        case eltwise_log: vfor(::logf(s[i]));
        case eltwise_pow: { float fs;
            if (beta == 0.f) {vfor(alpha);}
            if (beta == 1.f) {vfor(alpha * s[i]);}
            if (beta == 0.5f) {vfor(alpha * ::sqrtf(
                        s[i] < 0.f? 0.f: s[i]));}
            if (beta == 1.5f) {vfor(fs = (s[i]<0.f? 0.f: s[i]),
                    alpha * fs * ::sqrtf(fs));}
            if (beta == 2.0f) {vfor(alpha * s[i] * s[i]);}
            if (beta < 0.f && beta == (float)(int)beta && (-(int)beta & 0x1) ) {
                // nc++ diffficulties with ::powf, except at -O0
                //printf(" beta<0 is a -ve odd integer\n");
                if (alpha >= 0.0f){
                    float const negzero = -0.0f;
                    float const fixup = HUGE_VALF;
                    vfor(alpha * (s[i] == negzero
                                ? fixup : ::powf(s[i], beta)));
                }else{
                    float const fixup = HUGE_VALF;
                    vfor(alpha * (::fabs(s[i]) == 0.0f
                            ? fixup : ::powf(s[i], beta)));
                }
            }
            vfor(alpha * ::powf(s[i], beta));
        }
        case eltwise_pow+1: vfor(alpha); // beta==0.f
        case eltwise_pow+2: vfor(alpha * s[i]); // beta==1.f
        case eltwise_pow+3: vfor(alpha * ::sqrtf(s[i] < 0.f? 0.f: s[i])); // beta==0.5f
        case eltwise_pow+4: { float fs;
            vfor(fs = (s[i]<0.f? 0.f: s[i]), alpha * ::sqrtf(fs)); // beta==1.5f
        }
        case eltwise_pow+5: vfor(alpha * s[i] * s[i]); // beta==2.f
        case eltwise_pow+6: { // beta < 0 is a -ve odd integer
            float const fixup = HUGE_VALF;
            if (alpha >= 0.0f){
                float const negzero = -0.0f;
                vfor(alpha * (s[i] == negzero
                            ? fixup : ::powf(s[i], beta)));
            }else{
                vfor(alpha * (::fabs(s[i]) == 0.0f
                            ? fixup : ::powf(s[i], beta)));
            }
        }
        case eltwise_pow+7: vfor(alpha * ::powf(s[i], beta)); //any other beta

        case eltwise_gelu_erf: vfor(ve_gelu_erf_fwd(s[i]));

#undef vfor
        default: assert(!"unknown eltwise alg_kind");
    }
}

static float compute_eltwise_scalar_bwd(
        const alg_kind_t alg, float dd, float s, float alpha, float beta) {
    float ds = 0.f;
    switch (alg) {
        case eltwise_relu: ds = relu_bwd(dd, s, alpha); break;
        case eltwise_tanh: ds = tanh_bwd(dd, s); break;
        case eltwise_elu: ds = elu_bwd(dd, s, alpha); break;
        case eltwise_square: ds = square_bwd(dd, s); break;
        case eltwise_abs: ds = abs_bwd(dd, s); break;
        case eltwise_sqrt: ds = sqrt_bwd(dd, s); break;
        case eltwise_linear: ds = linear_bwd(dd, s, alpha, beta); break;
        case eltwise_bounded_relu: ds = bounded_relu_bwd(dd, s, alpha); break;
        case eltwise_soft_relu: ds = soft_relu_bwd(dd, s); break;
        case eltwise_logistic: ds = logistic_bwd(dd, s); break;
        case eltwise_exp: ds = exp_bwd(dd, s); break;
        case eltwise_gelu_tanh: ds = gelu_tanh_bwd(dd, s); break;
        case eltwise_swish: ds = swish_bwd(dd, s, alpha); break;
        case eltwise_log: ds = log_bwd(dd, s); break;
        case eltwise_clip: ds = clip_bwd(dd, s, alpha, beta); break;
        case eltwise_pow: ds = pow_bwd(dd, s, alpha, beta); break;
        case eltwise_gelu_erf: ds = gelu_erf_bwd(dd, s); break;

        case eltwise_relu_use_dst_for_bwd:
            ds = relu_bwd_use_dst(dd, s, alpha);
            break;
        case eltwise_tanh_use_dst_for_bwd: ds = tanh_bwd_use_dst(dd, s); break;
        case eltwise_elu_use_dst_for_bwd:
            ds = elu_bwd_use_dst(dd, s, alpha);
            break;
        case eltwise_sqrt_use_dst_for_bwd: ds = sqrt_bwd_use_dst(dd, s); break;
        case eltwise_logistic_use_dst_for_bwd:
            ds = logistic_bwd_use_dst(dd, s);
            break;
        case eltwise_exp_use_dst_for_bwd: ds = exp_bwd_use_dst(dd, s); break;

        default: assert(!"unknown eltwise alg_kind");
    }
    return ds;
}
} //namespace <anon>

ref_eltwise_scalar_fwd_t::ref_eltwise_scalar_fwd_t(
        alg_kind_t alg, float alpha, float beta, float scale)
    : alg_(alg), alpha_(alpha), beta_(beta), scale_(scale) {
    assert(utils::one_of(alg_, eltwise_relu, eltwise_tanh, eltwise_elu,
            eltwise_square, eltwise_abs, eltwise_sqrt, eltwise_linear,
            eltwise_bounded_relu, eltwise_soft_relu, eltwise_logistic,
            eltwise_exp, eltwise_gelu_tanh, eltwise_swish, eltwise_log,
            eltwise_clip, eltwise_pow, eltwise_gelu_erf,
            eltwise_relu_use_dst_for_bwd, eltwise_tanh_use_dst_for_bwd,
            eltwise_elu_use_dst_for_bwd, eltwise_sqrt_use_dst_for_bwd,
            eltwise_logistic_use_dst_for_bwd, eltwise_exp_use_dst_for_bwd));
}

ref_eltwise_scalar_fwd_t::ref_eltwise_scalar_fwd_t(
        const post_ops_t::entry_t::eltwise_t &eltwise)
    : ref_eltwise_scalar_fwd_t(
            eltwise.alg, eltwise.alpha, eltwise.beta, eltwise.scale) {}

float ref_eltwise_scalar_fwd_t::compute_scalar(float s) {
    return compute_eltwise_scalar_fwd(alg_, s, alpha_, beta_) * scale_;
}

// pow subcase not exposed XXX
void ref_eltwise_scalar_fwd_t::compute_vec_reg(
    float * const dst, float const* const src, int const vl) {
    compute_eltwise_vector_fwd(alg_, alpha_, beta_, scale_, dst, src, vl);
}

//
// TODO vectorize this one too  (same approach)
// 1. amalgamate c<C loops
// 2. inline the lamda (remove fn call)
// 3. move switch(alg_kind) outside the parallel_nd so most
//    math fns can be inlined (and vectorized)
//
// pretty much the same as for forward_dense
// XXX for VE, this needs multi-block to use vector length >8.
//     jit might try 2d-vector load, o/w could merge 32x8 blocks
//     of source and then calculate and split into dst[]
//
template <impl::data_type_t data_type>
void ref_eltwise_fwd_t<data_type>::execute_forward_nCspBc_padded(
        const exec_ctx_t &ctx) const {
    auto src = CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
    auto dst = CTX_OUT_MEM(data_t *, DNNL_ARG_DST);

    bool constexpr is_int_dt = utils::one_of(data_type, data_type::s32, data_type::s8, data_type::u8);
    using cvt = Cvt<data_t, is_int_dt>;

    const memory_desc_wrapper data_d(pd()->src_md());
    const blocking_desc_t &blk = data_d.blocking_desc();
    const dim_t block = blk.inner_blks[0];

    const dim_t MB = pd()->MB();
    const dim_t C = pd()->C() / block;
    const dim_t C_PADDED = data_d.padded_dims()[1] / block;
    const dim_t tail = pd()->C() % block;
    const dim_t SP = pd()->D() * pd()->H() * pd()->W();
    const auto alg_kind = pd()->desc()->alg_kind;
    const float alpha = pd()->desc()->alpha;
    const float beta = pd()->desc()->beta;
#define ELT_FWD_BLK_VEC 0
    if(1){
        if (src!=dst) printf("fwd eltwise nCsp8c_padded %d\n", ELT_FWD_BLK_VEC);
        else printf("fwd eltwise generic nCsp8c_padded %d, src=dst\n", ELT_FWD_BLK_VEC);
    }

#if ELT_FWD_BLK_VEC==0
    // unvectorizable on VE (compute_eltwise_scalar_fwd function reference)
    auto ker = [=](data_t &d, data_t s) {
        //d = compute_eltwise_scalar_fwd(alg_kind, s, alpha, beta);
        float f = compute_eltwise_scalar_fwd(alg_kind, s, alpha, beta);
        if (is_int_dt) d = cvt::rs(f);
        else d = f;
    };

    parallel_nd(MB, C_PADDED, SP, [&](dim_t n, dim_t c, dim_t sp) {
        auto d_off = (n * C_PADDED * SP + c * SP + sp) * block;
        if (c < C) {
            for (dim_t v = 0; v < block; v++)
                ker(dst[d_off + v], src[d_off + v]);
        } else {
            for (dim_t v = 0; v < tail; v++)
                ker(dst[d_off + v], src[d_off + v]);
        }
    });
#else
#error "ELT_FWD_BLK_VEC value TBD"
#endif // ELT_FWD_BLK_VEC
#undef ELT_FWD_BLK_VEC
}

template <impl::data_type_t data_type>
void ref_eltwise_fwd_t<data_type>::execute_forward_generic(
        const exec_ctx_t &ctx) const {
    /* fast return */
    if (pd()->has_zero_dim_memory()) return;

    bool constexpr is_int_dt = utils::one_of(data_type, data_type::s32, data_type::s8, data_type::u8);
    using cvt = Cvt<data_t, is_int_dt>;

    auto src = CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
    auto dst = CTX_OUT_MEM(data_t *, DNNL_ARG_DST);
#define ELT_FWD_GEN_VEC 2
    // for f32 exp log
    // 0 : 20.1 22.16
    // 1 : 2.8  3.8
    // 2 :
    if(0){
        if (src!=dst) printf("fwd eltwise generic %d\n", ELT_FWD_GEN_VEC);
        else printf("fwd eltwise generic %d, src=dst\n", ELT_FWD_GEN_VEC);
    }

#if !ELT_FWD_GEN_VEC
    const memory_desc_wrapper data_d(pd()->src_md());
#else
    const memory_desc_wrapper_opt data_d(pd()->src_md());
#endif

    const dim_t MB = pd()->MB();
    const dim_t C = pd()->C();
    const dim_t D = pd()->D();
    const dim_t H = pd()->H();
    const dim_t W = pd()->W();
    const auto alg_kind = pd()->desc()->alg_kind;
    const float alpha = pd()->desc()->alpha;
    const float beta = pd()->desc()->beta;
    const int ndims = pd()->desc()->data_desc.ndims;

    // How to cover this code (not possible in default benchdnn eltwise tests?)
    //   examples:
    // --dir=FWD_D --tag=ABx16a16b --alg=log --alpha=0 --beta=0 45x87x33x3
    // --dir=FWD_D --tag=ABx16a16b --alg=linear --alpha=0 --beta=0.25 45x87x33x3
    // --dir=FWD_D --tag=ABx16a16b --alg=linear --alpha=0.22 --beta=0.33 45x87x33x3
    // --dir=FWD_D --tag=ABx16a16b --alg=clip --alpha=0.11 --beta=0.88 45x87x33x3
    // --dir=FWD_D --tag=ABx16a16b --alg=log,sqrt --alpha=0 --beta=0 45x87x33x3
    // NO --dir=FWD_D --tag=ABx16a16b --alg=pow --alpha=0.25 --beta=3.14 45x87x33x3

#if !ELT_FWD_GEN_VEC
    // --dir=FWD_D --tag=ABx16a16b --alg=log --alpha=0 --beta=0 45x87x33x3
    // 19.15 ms
    parallel_nd(
            MB, C, D, H, W, [&](dim_t n, dim_t c, dim_t d, dim_t h, dim_t w) {
                auto data_off = DATA_OFF(data_d, n, c, d, h, w);
                //dst[data_off] = compute_eltwise_scalar_fwd(
                //        alg_kind, src[data_off], alpha, beta);
                float f = compute_eltwise_scalar_fwd( alg_kind, src[data_off], alpha, beta);
                if (is_int_dt) f = cvt::rs(f);
                dst[data_off] = f;
            });
#elif ELT_FWD_GEN_VEC == 1 // based on backward "first attempt"
    // 5.5 ms
    const ptrdiff_t nelems = static_cast<ptrdiff_t>(data_d.nelems(false));
    parallel(0, [&](const int ithr, const int nthr) {
            dim_t start = 0, end = 0;
            balance211(nelems, nthr, ithr, start, end);
            if (start == end) return;
            dim_t loff[MVL]; // array of logical offsets
            size_t data_off[MVL];      // array of physical offsets
            // also possible: coords iter and vec_off_p or so
            for (dim_t i = start; i < end; i+=MVL) {
                dim_t const vl = (end - i > MVL? MVL: end - i);
                ShortLoop() for(dim_t j=0; j<vl; ++j)
                    loff[j] = i+j;
                // vectorized offset calcs
                data_d.vec_off_l( &loff[0], vl, (dim_t*)&data_off[0]);
                ShortLoop() for(dim_t j=0; j<vl; ++j) {
                    data_t s  = src[data_off[j]];
                    data_t &d = dst[data_off[j]];
                    float f = compute_eltwise_scalar_fwd(alg_kind, s, alpha, beta);
                    if (is_int_dt) f = cvt::rs(f);
                    d = f;
                }
            }
    });
#elif ELT_FWD_GEN_VEC == 2 // use larger inner loop size --> 1.01 ms
    //
    // This approach blocks not by MVL but some larger stack-centric size
    //
    const ptrdiff_t nelems = static_cast<ptrdiff_t>(data_d.nelems(false));
    dim_t const blksz = stack_friendly_blksz(nelems);
    assert( blksz <= /*constexpr*/ stack_elems );
    //printf(" nelems=%ld blksz=%ld\n",nelems, blksz);
#define Medium_ PragmaQuote(_NEC loop_count(STACK_ELEMS))
#if 1
    //
    //  0.92 ms ~ 10% faster  moving 'case' outside parallel_nd
    //
    if (alg_kind == eltwise_log) {
        parallel_nd(utils::div_up(nelems, blksz), [&](dim_t const blk)
                {
                dim_t const start = blk * blksz;
                dim_t const end   = (start + blksz > nelems? nelems: start + blksz);
                dim_t const sz = end - start;
                //assert( sz <= /*constexpr*/ stack_elems );
                // Only need sz, but fixed-size array allows nc++ to make ivdep assumptions
                dim_t loff[stack_elems];
                size_t data_off[stack_elems];

                Medium_ for (int i=0; i<sz; ++i)
                loff[i] = start + i; // a win if sz > ~1
                data_d.vec_off_l( &loff[0], sz, (dim_t*)&data_off[0] );

                Medium_ for(dim_t i=0; i<sz; ++i) {
                    data_t s  = src[data_off[i]];
                    data_t &d = dst[data_off[i]];
                    d = ve_log_fwd(s);
                }
                });
    }else
#endif
    parallel_nd(utils::div_up(nelems, blksz), [&](dim_t const blk)
    {
        dim_t const start = blk * blksz;
        dim_t const end   = (start + blksz > nelems? nelems: start + blksz);
        dim_t const sz = end - start;
        //assert( sz <= /*constexpr*/ stack_elems );
        // Only need sz, butfixed-size array allows nc++ to make ivdep assumptions
        dim_t loff[stack_elems];
        size_t data_off[stack_elems];

#define Medium_ PragmaQuote(_NEC loop_count(STACK_ELEMS))
        Medium_ for (int i=0; i<sz; ++i)
            loff[i] = start + i; // a win if sz > ~1
        data_d.vec_off_l( &loff[0], sz, (dim_t*)&data_off[0] );

        switch(alg_kind){
#define CASE2(ALG,EXPR) case eltwise_##ALG: { \
            Medium_ for(dim_t i=0; i<sz; ++i) { \
                data_t const s  = src[data_off[i]]; \
                /*data_t &d = dst[data_off[i]];*/ \
                dst[data_off[i]] = EXPR; \
            }} break
#define CASE(ALG,...) CASE2(ALG, ve_##ALG##_fwd(__VA_ARGS__))
        CASE(pow, s, alpha, beta);
        CASE(log, s);
        CASE(linear, s, alpha, beta);
        CASE(sqrt, s);
        CASE(clip, s, alpha, beta);
#if 1
        CASE(soft_relu, s);
#else
        case(eltwise_soft_relu): { // log1pf is NOT vectorized by nc++-3.0.27
            //float constexpr max_logf = 8.872284e+01f; // ::log(FLT_MAX) (move const outside loop)
            float constexpr max_logf = 88.0f; // need room for 1+exp
            Medium_ for(dim_t i=0; i<sz; ++i) {
#if 1
                dst[data_off[i]] = ve_soft_relu_fwd(src[data_off[i]]);
#elif 1 // same speed
                float const s  = src[data_off[i]];
                dst[data_off[i]] = (data_t)(s < max_logf? ::logf(1.0f + ::expf(s)) : s);
#elif 1 // 1.01 ms
                float const s  = src[data_off[i]];
                float d = s;
                if (s >= max_logf) d = s;
                else d = ::logf(1.0f + ::expf(s));
                dst[data_off[i]] = d;
#elif 0 // cannot convince __vec_expf to use non-all-ones-mask in %vm1 :(
                float const s  = src[data_off[i]];
                float d = s;
                if (s < max_logf) d = ::expf(s);
                if (s < max_logf) d += 1.0f;
                if (s < max_logf) d = ::logf(d);
                dst[data_off[i]] = data_t{d};
#endif
            }} break;
#endif
        case eltwise_logistic_use_dst_for_bwd: // identical
#if 1
        CASE(logistic, s);
#else
        case eltwise_logistic:
#if 0
        { // logistic has VE limit-case issues
            float constexpr min_logistic = -87; // close to -log(FLT_MAX)
            Medium_ for(dim_t i=0; i<sz; ++i) {
                float s = src[data_off[i]];
                if (s < min_logistic) s = min_logistic;
                dst[data_off[i]] = logistic_fwd<data_t,float>(s);
            }} break;
#else
        {
            Medium_ for(dim_t i=0; i<sz; ++i) {
                float const s  = src[data_off[i]];
                dst[data_off[i]] = ve_logistic_fwd<float,data_t>(s);
                //dst[data_off[i]] = ve_logistic_fwd(s);
            }} break;
#endif
#endif

        case eltwise_exp_use_dst_for_bwd: // lazy way
        case eltwise_exp: { // ovflw handling for VE
            //float constexpr max_logf = 8.872284e+01f; // log(FLT_MAX)
            float constexpr max_logf = 87.0f;
            float constexpr float_inf = HUGE_VALF;
            Medium_ for(dim_t i=0; i<sz; ++i) {
                float s = src[data_off[i]];
                s = (s > 87.0f? float_inf : ::expf(s));
                data_t d;
                if (is_int_dt) d = cvt::rs(s);
                else           d = s;
                dst[data_off[i]] = d;
            }} break;

        default: {
            for(dim_t i=0; i<sz; ++i) {
                dst[data_off[i]] = data_t{0};
            }}
            printf(" Error: TBD eltwise fwd generic for %s\n", dnnl_alg_kind2str(alg_kind));
            assert(!" TBD missing alg_kind case for ref eltwise fwd generic");
#undef CASE
#undef CASE2
        }
    });
#else //ELT_FWD_GEN_VEC == 3 // move switch out by 2 loops
#error "TBD"
#endif // ELT_FWD_GEN_VEC
}

template <impl::data_type_t data_type>
void ref_eltwise_fwd_t<data_type>::execute_forward_dense(
        const exec_ctx_t &ctx) const {
    bool constexpr is_int_dt = utils::one_of(data_type, data_type::s32, data_type::s8, data_type::u8);
    using cvt = Cvt<data_t, is_int_dt>;

    auto src = CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
    auto dst = CTX_OUT_MEM(data_t *, DNNL_ARG_DST);

    const memory_desc_wrapper data_d(pd()->src_md());

    const ptrdiff_t nelems = static_cast<ptrdiff_t>(data_d.nelems(true));
    const auto alg_kind = pd()->desc()->alg_kind;
    const float alpha = pd()->desc()->alpha;
    const float beta = pd()->desc()->beta;

    src += data_d.offset0();
    dst += data_d.offset0();

    if(1){
        if (src!=dst) printf("fwd eltwise dense\n");
        else printf("fwd eltwise dense, src=dst\n");
    }

    if (alg_kind == eltwise_relu) {
        // a fast path for relu as the most popular activation
        parallel_nd(
                nelems, [&](ptrdiff_t e) { dst[e] = relu_fwd(src[e], alpha); });
        return;
    }

    /** 0~ original 1~ current best 2~ experimental */
#if 0 // original: nc++ does not vectorize this (some unvectorizable func calls)
    parallel_nd(nelems, [&](ptrdiff_t e) {
        const data_t s = src[e];
        data_t &d = dst[e];
        d = compute_eltwise_scalar_fwd(alg_kind, s, alpha, beta);
    });
#else
    switch (alg_kind) {
        //case eltwise_relu: d = relu_fwd(s, alpha); break;
#define CASE2(ALG,EXPR) case eltwise_##ALG: { \
                /*asm("### eltwise fwd dense " #ALG);*/ \
                parallel_nd(nelems, [&](ptrdiff_t e) { \
                    const data_t s = src[e]; \
                    data_t &d = dst[e]; \
                    EXPR; \
                }); \
    } break
#define CASE(ALG,...) CASE2(ALG, d = ve_##ALG##_fwd(__VA_ARGS__))

        // eval EXPR as float, then round and saturate (if nec) to data_type
#define CASE2_CVT_RS(ALG,EXPR) case eltwise_##ALG: { \
                /*asm("### eltwise fwd dense " #ALG);*/ \
                parallel_nd(nelems, [&](ptrdiff_t e) { \
                    const float s = (float)src[e]; \
                    data_t &d = dst[e]; \
                    EXPR; \
                }); \
    } break
#define CASE_CVT_RS(ALG,...) CASE2_CVT_RS(ALG, d = cvt::rs(__VA_ARGS__))
//#define CASE(ALG,...) case eltwise_##ALG: { \
//                parallel_nd(nelems, [&](ptrdiff_t e) { \
//                    const data_t s = src[e]; \
//                    data_t &d = dst[e]; \
//                    d = ALG##_fwd(__VA_ARGS__); \
//                }); \
//    } break
        CASE(tanh, s);
        CASE(elu, s, alpha);
        CASE(square, s);
        CASE(abs, s);
        CASE(sqrt, s);
        CASE(linear, s, alpha, beta);

#define CASE_SPECIAL_BEG(ALG) \
        case eltwise_##ALG: \
        { \
            parallel(0, [&src,&dst,&nelems](const int ithr, const int nthr) { \
                dim_t start = 0, end = 0; \
                balance211(nelems, nthr, ithr, start, end); \
                if (start == end) return
#define CASE_SPECIAL_END }); } break

#if 1
        // slow ?
        CASE(bounded_relu, s, alpha);
#else
        case eltwise_bounded_relu:
            {
                parallel(0, [&](const int ithr, const int nthr) {
                    dim_t start = 0, end = 0;
                    balance211(nelems, nthr, ithr, start, end);
                    if (start == end) return;
                    data_t const d_alpha = alpha;
                    for (dim_t i = start; i < end; i++) {
#if 1 // 0.02 ms (cf ~ 10 ms in WAY==1) XXX PERF timing?
                        data_t s = src[i];
                        //if /*constexpr*/ (std::is_signed<data_t>::value)
                        //    s = (s < 0? data_t{0}: s);
                        mkNonNegative(s);
                        s = (s > d_alpha? d_alpha: s);
                        dst[i] = s;
#else // perhaps a bit slower??
                        dst[i] = bounded_relu_fwd(src[i], alpha);
#endif
                    }
                });
            } break;
#endif

#if 1
        // slow?
        //CASE(soft_relu, s);
        case eltwise_soft_relu:
            {
                parallel_nd(nelems, [&](ptrdiff_t e) {
                        const data_t s = src[e];
                        data_t &d = dst[e];
                        d = ve_soft_relu_fwd(s);
                        });
            } break;
#else
        case eltwise_soft_relu:
        { // moving const outside
            // soft relu is 
            parallel(0, [&](const int ithr, const int nthr) {
                    dim_t start = 0, end = 0;
                    balance211(nelems, nthr, ithr, start, end);
                    if (start == end) return;
                    float const max_logf = 8.872284e+01; // ::log(FLT_MAX)
                    for (dim_t i = start; i < end; i++) {
                        // log1pf is NOT vectorized by nc++-3.0.27
                        float const s = src[i];
                        dst[i] = (data_t)( s < max_logf ? ::logf(1.0f + ::expf(s)) : s);
                    }
            });
        } break;
#endif

#if 0
        // no:
        CASE2(logistic_use_dst_for_bwd, logistic_fwd(s));
        //CASE_CVT_RS(logistic_use_dst_for_bwd, ve_logistic_fwd(s));
#elif 0
        // VE vector outputs 1, not 0 for too-small input:
        case eltwise_logistic_use_dst_for_bwd:
        {
            parallel(0, [&src,&dst,&nelems](const int ithr, const int nthr) {
                dim_t start = 0, end = 0;
                balance211(nelems, nthr, ithr, start, end);
                if (start == end) return;
                //float const neg_max_logf = -8.872284e+01f; // ::log(FLT_MAX)
                float constexpr neg_max_logf = -87.0f; // exp(small x) ~ 0
                for (dim_t i = start; i < end; ++i) {
#if 0 // nice behaviour (forgiving) but fails benchdnn
                    float s = src[i];
                    if (s < neg_max_logf) s = neg_max_logf;
                    dst[i] = logistic_fwd<data_t,float>(s);
#else
                    float v = ::expf((float)-src[i]);
                    float w = 1.0f / (1.0f + v);
                    dst[i] = w;
#endif
                }
            });
        } break;
#endif

        case eltwise_logistic_use_dst_for_bwd:
#if 0
        CASE(logistic, s);
#else
        // both exp and logistic have 'inf' issues for VE vectorization
        //CASE2(logistic, d = compute_eltwise_scalar_fwd(alg_kind, s, alpha, beta));
        // VE default might not be checking input domain for vector math functions
        // XXX identical case! (merge) XXX
        case eltwise_logistic: {
            parallel_nd(nelems, [&](ptrdiff_t e) {
                float s = src[e];
                if (s < -87.0f) s = -87.0f;
                data_t &d = dst[e];
                d = logistic_fwd<data_t,float>(s);
            });
        } break;
#endif

        // ovflw handling: CASE(exp, s);
        case eltwise_exp_use_dst_for_bwd:
        case eltwise_exp: {
            float const float_inf = HUGE_VALF;
            parallel_nd(nelems, [&](ptrdiff_t e) {
                float s = src[e];
                data_t &d = dst[e];
                s = (s > 87.0f? float_inf : ::expf(s));
                //ok 
                if (is_int_dt) d = cvt::rs(s); else d = s;
                // no d = cvt::s(s);
                // no d = (data_t)s; (int8 does not go to -127)
                // no d = cvt::rs(s); do not want inf--> 3.4e38
                // no d = cvt::int_sat(s); // leave float inf as inf?
            });
        } break;
#if 0
        // round,saturate expr of float s
        // does not agree with benchdnn, but I think behaviour is better here
        CASE_CVT_RS(exp_use_dst_for_bwd, ve_exp_fwd(s));
#elif 0
        // VE vector ovflw may not yield inf for large AND small inputs
        CASE_SPECIAL_BEG(exp_use_dst_for_bwd);
        float const float_inf = HUGE_VALF;
        float constexpr max_logf = 87.0f;
        for (dim_t i = start; i < end; ++i) {
            float s = src[i];
            dst[i] = (s > max_logf? float_inf: s < -max_logf? 0.0f: std::expf(s));
        }
        CASE_SPECIAL_END;
#endif
        CASE(gelu_tanh, s);
        CASE(swish, s, alpha);
        CASE(log, s);
        CASE(clip, s, alpha, beta);
#if 1
        CASE(pow, s, alpha, beta);
#else
        // ovflw cases: CASE(pow, s, alpha, beta);
        // when input is -0, VE output is -inf, but ref output wants +inf
        // assert( signbit(powf(negzero,-1.0f)) ) FAILS at -O4, OK at -O0
        // XXX This is a WRONG WAY to appease benchdnn (should fix ref calc first) XXX
        //     and then readjust here. Perhaps compile ref calc at -O2 or so?
        case eltwise_pow: {
            if (beta == 0.0f){
                parallel_nd(nelems, [&](ptrdiff_t e) {dst[e] = alpha;});
            }else if (beta == 1.0f){
                parallel_nd(nelems, [&](ptrdiff_t e) {dst[e] = alpha * src[e];});
            }else if (beta == 0.5f){
                parallel_nd(nelems, [&](ptrdiff_t e) {dst[e] = alpha * sqrtf(src[e]);});
            }else if (beta == 1.5f){
                parallel_nd(nelems, [&](ptrdiff_t e) {float const s = src[e]; dst[e] = alpha * s * sqrtf(s);});
            }else if (beta == 2.0f){
                parallel_nd(nelems, [&](ptrdiff_t e) {float const s = src[e]; dst[e] = alpha * s * s;});
            }else if (beta < 0.f && beta == (float)(int)beta
                    && (-(int)beta & 0x1) ) {
                //printf(" beta<0 is a -ve odd integer\n");
                // when beta is a -ve odd integer, CORRECT powf(-0,beta) value is -inf
                // -O4 REF value is incorrect (-O0 is correct)
                //
                // (either/both fast-math and vector math funcs allow cheating on limit-cases)
                // (perhaps -O2 will give standards-compliant ::powf result? or an nc++ flag?)
                // alpha -ve/+ve is yet another corner case to fix
                if (alpha >= 0.0f) {
                    float const negzero = -0.0f;
                    float const fixup = HUGE_VALF;
                    parallel_nd(nelems, [&](ptrdiff_t e) {
                            float const s = src[e];
                            dst[e] = alpha * (s == negzero
                                    ? fixup : ::powf(s, beta));
                            });
                }else{
                    float const negzero = -0.0f;
                    float const fixup = HUGE_VALF;
                    parallel_nd(nelems, [&](ptrdiff_t e) {
                            float const s = src[e];
                            dst[e] = alpha * (::fabs(s) == 0.0f
                                    ? fixup : ::powf(s, beta));
                            });
                }
            }else{
                parallel_nd(nelems, [&](ptrdiff_t e) {
                        //dst[e] = pow_fwd(src[e], alpha, beta);
                        // beta=0.0f special case done above, avoit test...
                        dst[e] = alpha * ::powf(src[e], beta);
                });
            }
        } break;
#endif

        CASE(gelu_erf, s);
        CASE2(relu_use_dst_for_bwd, d = ve_relu_fwd(s, alpha));
        CASE2(tanh_use_dst_for_bwd, d = ve_tanh_fwd(s));
        CASE2(elu_use_dst_for_bwd, d = ve_elu_fwd(s, alpha));

#if 0 // error for --dt=s8
        //CASE2(sqrt_use_dst_for_bwd, d = ve_sqrt_fwd(s));
#elif 0 // debug
        CASE_SPECIAL_BEG(sqrt_use_dst_for_bwd);
        for (dim_t i = start; i < end; ++i) {
#if 0 // naive, with rs
            float s = sqrt_fwd((float)src[i]);
            if (src[i] < 0) printf("s=%g -->qs=%f and rs=%f\n",s,(double)cvt::qs(s),(double)cvt::rs(s));
            // both qs and rs seem ok.  is rs is faster?
            dst[i] = cvt::rs(s);
#else // ve does vrsqrt in double, so...
            dst[i] = cvt::rs(::sqrtf(src[i]));
#endif
        }
        CASE_SPECIAL_END;
#else // final
        CASE_CVT_RS(sqrt_use_dst_for_bwd,::sqrtf(s));
#endif


        default: assert(!"unknown eltwise alg_kind");
        parallel_nd(nelems, [&](ptrdiff_t e) {
                dst[e] = 0.f;
                });
    }
#undef CASE
#undef CASE2
#undef CASE_SPECIAL_BEG
#undef CASE_SPECIAL_END
#endif
}

template <impl::data_type_t data_type>
void ref_eltwise_bwd_t<data_type>::execute_backward_generic(
        const exec_ctx_t &ctx) const {
    /* fast return */
    if (pd()->has_zero_dim_memory()) return;

    auto src = pd()->use_dst() ? CTX_IN_MEM(const data_t *, DNNL_ARG_DST)
                               : CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
    auto diff_dst = CTX_IN_MEM(const data_t *, DNNL_ARG_DIFF_DST);
    auto diff_src = CTX_OUT_MEM(data_t *, DNNL_ARG_DIFF_SRC);

#define ELT_BKW_GEN_VEC 1
    // --eltwise --dir=BWD_D --tag=aBx16b,aBx8b --alg=log --alpha=0 --beta=0 45x87x33x3
    // 0 : 266 266 ms (--mode=C timings)
    // 1 : (nonvec) 24.9 24.9 ms 
    // 1 : (   vec) 5.15 5.19 ms ~ 50x speedup from orig (much more could be done for VE)
#if !ELT_BKW_GEN_VEC
    const memory_desc_wrapper data_d(pd()->src_md());
    const memory_desc_wrapper diff_data_d(pd()->diff_src_md());
#else
    const memory_desc_wrapper_opt data_d(pd()->src_md());
    const memory_desc_wrapper_opt diff_data_d(pd()->diff_src_md());
#endif

    const dim_t MB = pd()->MB();
    const dim_t C = pd()->C();
    const dim_t D = pd()->D();
    const dim_t H = pd()->H();
    const dim_t W = pd()->W();
    const auto alg_kind = pd()->desc()->alg_kind;
    const float alpha = pd()->desc()->alpha;
    const float beta = pd()->desc()->beta;
    const int ndims = pd()->desc()->data_desc.ndims;
    //printf(" bwd eltwise generic nelems=%ld nelems(true)=%ld NCDHW=%ld\n",
    //        (long)data_d.nelems(false), (long)data_d.nelems(true),
    //        (long)MB*C*D*H*W );
    // some benchdnn cases that excercise bwd generic eltwise:
    //           --tag=aBx16b,aBx8b --alg=log,sqrt

#if !ELT_BKW_GEN_VEC
    parallel_nd(
            MB, C, D, H, W, [&](dim_t n, dim_t c, dim_t d, dim_t h, dim_t w) {
                auto data_off = DATA_OFF(data_d, n, c, d, h, w);
                auto diff_data_off = DATA_OFF(diff_data_d, n, c, d, h, w);
                //printf( " %-8ld data_off=%-8ld diff_data_off=%-8ld ncdhw{%ld,%ld,%ld,%ld,%ld}\n",
                //        (((n * C + c) * D + d) * H + h) * W +w,
                //        data_off, diff_data_off, n,c,d,h,w );
                data_t s = src[data_off];
                data_t dd = diff_dst[diff_data_off];
                data_t &ds = diff_src[diff_data_off];
                ds = compute_eltwise_scalar_bwd(alg_kind, dd, s, alpha, beta);
            });
#else
    // 1. block over MB and C (and D?), see src/cpu/ref_softmax (or ve/ version?) to
    // nelems ~ logical number of elements
    const ptrdiff_t nelems = static_cast<ptrdiff_t>(data_d.nelems(false));
    parallel(0, [&](const int ithr, const int nthr) {
            dim_t start = 0, end = 0;
            balance211(nelems, nthr, ithr, start, end);
            if (start == end) return;
#define MVL 256
            dim_t loff[MVL]; // array of logical offsets
            size_t data_off[MVL];      // array of physical offsets
            size_t diff_data_off[MVL]; // array of physical offsets
            // also possible: coords iter and vec_off_p or so
            for (dim_t i = start; i < end; i+=MVL) {
                dim_t const vl = (end - i > MVL? MVL: end - i);
                ShortLoop() for(dim_t j=0; j<vl; ++j)
                    loff[j] = i+j;
                // vectorized offset calcs
                data_d     .vec_off_l( &loff[0], vl, (dim_t*)&data_off[0]);
                // "Inline halted: code size" ...
                diff_data_d.vec_off_l( &loff[0], vl, (dim_t*)&diff_data_off[0]);
#if 0 // unvectorized initial version :
                ShortLoop() for(dim_t j=0; j<vl; ++j) {
                    //printf( "%-8ld data_off=%-8ld diff_data_off=%-8ld start,end,i,j=%ld,%ld,%ld,%ld\n",
                    //        loff[j], data_off[j], diff_data_off[j], start, end, i, j);
                    data_t s   = src     [data_off     [j]];
                    data_t dd  = diff_dst[diff_data_off[j]];
                    data_t &ds = diff_src[diff_data_off[j]];
                    ds = compute_eltwise_scalar_bwd(alg_kind, dd, s, alpha, beta);
                }
#else // What alg_kind does bwd generic hit, for vectorization?
                // pow, sqrt, log, sqrt_dst for sure
                // nc++ prefers cases outside of loop so function calls
                //      can individually be inlined & vectorized.
                //
                // TODO compare with moving switch outside one more for loop
                //
                switch(alg_kind) {
#define CASE_BEG(ALG_KIND) \
                    case ALG_KIND: \
                    { \
                            ShortLoop() for(dim_t j=0; j<vl; ++j) { \
                                data_t s   = src     [data_off     [j]]; \
                                data_t dd  = diff_dst[diff_data_off[j]]; \
                                data_t &ds = diff_src[diff_data_off[j]]
#define CASE_END }} break
#define SRC src[data_off[j]]
#define DIFF_DST diff_dst[diff_data_off[j]]
#define DIFF_SRC diff_src[diff_data_off[j]]

#define CASE2(ALG,EXPR) case eltwise_##ALG: { \
    ShortLoop() for(dim_t j=0; j<vl; ++j) { \
        data_t s   = src     [data_off     [j]]; \
        data_t dd  = diff_dst[diff_data_off[j]]; \
        data_t &ds = diff_src[diff_data_off[j]]; \
        EXPR; /* ex. ds = relu_bwd(dd,s,alpha); */ \
    }} break
#define CASE(ALG,...) CASE2(ALG, ds = ALG##_bwd(__VA_ARGS__))

#if 1 // full set of cases: (as in dense case, below)
        CASE(relu, dd, s, alpha);
        CASE(tanh, dd, s);
        CASE(elu, dd, s, alpha);
        CASE(square, dd, s);
        CASE(abs, dd, s);
        CASE(sqrt, dd, s);
        CASE(linear, dd, s, alpha, beta);
        //
        CASE(bounded_relu, dd, s, alpha);
        CASE(soft_relu, dd, s);
        CASE(logistic, dd, s);
        // ovflw cases: CASE(exp, dd, s);
        case eltwise_exp: {
            float const float_inf = HUGE_VALF;
            ShortLoop() for(dim_t j=0; j<vl; ++j) {
                data_t const s   = src     [data_off     [j]];
                DIFF_SRC = (data_t)( DIFF_DST *
                    (s > 87.0f? float_inf : std::expf(s))
                    );
            }} break;
        //
        CASE(gelu_tanh, dd, s);
        CASE(swish, dd, s, alpha);
        CASE(log, dd, s);
        CASE(clip, dd, s, alpha, beta);
        CASE(pow, dd, s, alpha, beta);
        CASE(gelu_erf, dd, s);
        //
        CASE2(relu_use_dst_for_bwd, ds = relu_bwd_use_dst(dd, s, alpha));
        CASE2(tanh_use_dst_for_bwd, ds = tanh_bwd_use_dst(dd, s));
        CASE2(elu_use_dst_for_bwd, ds = elu_bwd_use_dst(dd, s, alpha));
        CASE2(sqrt_use_dst_for_bwd, ds = sqrt_bwd_use_dst(dd, s));
        CASE2(logistic_use_dst_for_bwd, ds = logistic_bwd_use_dst(dd, s));
        CASE2(exp_use_dst_for_bwd, ds = exp_bwd_use_dst(dd, s));

#else // an initial set of encountered cases ..

#if 1
                    CASE(pow, dd, s, alpha, beta);
#elif 1 // pow_bwd ... "Structure pointer inhibits" message (for bf16?)
                    // --tag=aBx16b --alg=pow --alpha=0.25 --beta=3.14 45x87x33x3
                    // 36.9 ms
                    CASE_BEG(eltwise_pow);
                    ds = pow_bwd(dd, s, alpha, beta);
                    CASE_END;
#elif 1 // hmm, is it template depth?
                    // this one is still 36.9 ms
                    case eltwise_pow:
                    {
                        if (beta==0) {
                            ShortLoop() for(dim_t j=0; j<vl; ++j) {
                                DIFF_SRC = 0;
                            }
                        }else{
                            // XXX alpha*beta NOT moved out and up :(
                            ShortLoop() for(dim_t j=0; j<vl; ++j) {
                                DIFF_SRC = DIFF_DST * pow_fwd(SRC, alpha*beta, beta-1);
                            }
                        }
                    } break;
#endif

                    CASE(sqrt, dd, s);
                    //CASE_BEG(eltwise_sqrt);
                    //ds = sqrt_bwd(dd, s);
                    //CASE_END;

                    CASE(gelu_tanh, dd, s);
                    //CASE_BEG(eltwise_gelu_tanh);
                    //ds = gelu_tanh_bwd(dd, s);
                    //CASE_END;

#if 1
                    CASE(log, dd, s);
#elif 1
                    CASE_BEG(eltwise_log);
                    ds = log_bwd(dd, s);
                    CASE_END;
#elif 1 // longhand, for compiler diagnostics...
                    case eltwise_log:
                    {
                        ShortLoop()//;
                        for(dim_t j=0; j<vl; ++j) {
                            DIFF_SRC = log_bwd( DIFF_DST, SRC );
                        }
                    } break;
#endif

                    CASE_BEG(eltwise_sqrt_use_dst_for_bwd);
                    ds = sqrt_bwd_use_dst(dd, s);
                    CASE_END;

#endif // an initial set of encountered cases ..

                    default:
                    ShortLoop() for(dim_t j=0; j<vl; ++j) {
                        //data_t s   = src     [data_off     [j]];
                        //data_t dd  = diff_dst[diff_data_off[j]];
                        data_t &ds = diff_src[diff_data_off[j]];
                        ds = data_t{0};
                    }
                    printf("\nError: unhandled elwise bwd generic case %s\n",
                            dnnl_alg_kind2str(alg_kind));
                    //assert(!"unhandled eltwise bwd generic case");
                }
#endif
            }
    });
#undef CASE_BEG
#undef CASE_END
#undef SRC
#undef DIFF_DST
#undef DIFF_SRC
#undef CASE
#undef CASE2
#endif // ELT_BKW_GEN_VEC
#undef ELT_BKW_GEN_VEC
}

//template <>
//void ref_eltwise_bwd_t<data_type::f32>::execute_backward_dense(
// Original had an f32 partial specialization, which was slow.
// nc++ left the math inlines as fn calls, so had only scalar loops.
//
// However [reason unknown] when non-specialized, the inlines are expanded
// and VE begins to vectorize nicely.
//
// The remaining bf16 specialization is of little interest for VE
//
template <impl::data_type_t data_type>
void ref_eltwise_bwd_t<data_type>::execute_backward_dense(
        const exec_ctx_t &ctx) const {
    auto src = pd()->use_dst() ? CTX_IN_MEM(const data_t *, DNNL_ARG_DST)
                               : CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
    auto diff_dst = CTX_IN_MEM(const data_t *, DNNL_ARG_DIFF_DST);
    auto diff_src = CTX_OUT_MEM(data_t *, DNNL_ARG_DIFF_SRC);

    const memory_desc_wrapper data_d(pd()->src_md());
    const memory_desc_wrapper diff_data_d(pd()->diff_src_md());

    const auto nelems = data_d.nelems(true);
    const auto alg_kind = pd()->desc()->alg_kind;
    const float alpha = pd()->desc()->alpha;
    const float beta = pd()->desc()->beta;

    src += data_d.offset0();
    diff_dst += diff_data_d.offset0();
    diff_src += diff_data_d.offset0();

#if 0 // orig: nc++ did not vectorize if some 'switch' cases have func calls
    parallel(0, [&](const int ithr, const int nthr) {
        dim_t start = 0, end = 0;
        balance211(nelems, nthr, ithr, start, end);
        if (start == end) return;

        for (dim_t i = start; i < end; i++) {
            diff_src[i] = compute_eltwise_scalar_bwd(
                    alg_kind, diff_dst[i], src[i], alpha, beta);
        }
    });
#else
    switch(alg_kind) {
        // TODO comparing ovhd (small size src), I guess it might be good
        //      to compare with the non-balancing version of CASE in FWD code
#define CASE2(ALG,EXPR) case eltwise_##ALG: { \
    parallel(0, [&](const int ithr, const int nthr) { \
            dim_t start = 0, end = 0; \
            balance211(nelems, nthr, ithr, start, end); \
            if (start == end) return; \
            \
            for (dim_t i = start; i < end; i++) { \
                auto const s = src[i]; \
                auto const dd = diff_dst[i]; \
                data_t &ds = diff_src[i]; \
                EXPR; /*ex. ds = relu_bwd(dd,s,alpha); */ \
            } \
        }); \
    } break
#define CASE(ALG,...) CASE2(ALG, ds = ALG##_bwd(__VA_ARGS__))
#if 0
        CASE(relu, dd, s, alpha);
#else // original longhand equivalent (for looking at asm code)
        case eltwise_relu: {
            parallel(0, [&](const int ithr, const int nthr) {
                dim_t start = 0, end = 0;
                balance211(nelems, nthr, ithr, start, end);
                if (start != end) {
                    for (dim_t i = start; i < end; i++) {
                        diff_src[i] = relu_bwd(diff_dst[i], src[i], alpha);
                    }
                }
            });
        } break;
#endif
        CASE(tanh, dd, s);
        CASE(elu, dd, s, alpha);
        CASE(square, dd, s);
        CASE(abs, dd, s);
        CASE(sqrt, dd, s);
        CASE(linear, dd, s, alpha, beta);
        //
        CASE(bounded_relu, dd, s, alpha);
        CASE(soft_relu, dd, s);
        CASE(logistic, dd, s);
        // ovflw cases: CASE(exp, dd, s);
        case eltwise_exp: {
            float const float_inf = HUGE_VALF;
            parallel_nd(nelems, [&](ptrdiff_t e) {
                auto const s = src[e];
                auto const dd = diff_dst[e];
                data_t &ds = diff_src[e];
                ds = (data_t)( dd *
                        (s > 87.0f? float_inf : std::expf(s))
                        );
            });
            } break;
        //
        CASE(gelu_tanh, dd, s);
        CASE(swish, dd, s, alpha);
        CASE(log, dd, s);
        CASE(clip, dd, s, alpha, beta);
        CASE(pow, dd, s, alpha, beta);
        CASE(gelu_erf, dd, s);
        //
        CASE2(relu_use_dst_for_bwd, ds = relu_bwd_use_dst(dd, s, alpha));
        CASE2(tanh_use_dst_for_bwd, ds = tanh_bwd_use_dst(dd, s));
        CASE2(elu_use_dst_for_bwd, ds = elu_bwd_use_dst(dd, s, alpha));
        CASE2(sqrt_use_dst_for_bwd, ds = sqrt_bwd_use_dst(dd, s));
        CASE2(logistic_use_dst_for_bwd, ds = logistic_bwd_use_dst(dd, s));
        CASE2(exp_use_dst_for_bwd, ds = exp_bwd_use_dst(dd, s));
    }
#undef CASE
#undef CASE2
#endif
}

// VE doesn't care about bf16 impl
template <>
void ref_eltwise_bwd_t<data_type::bf16>::execute_backward_dense(
        const exec_ctx_t &ctx) const {
    using namespace memory_tracking::names;

    auto src = pd()->use_dst() ? CTX_IN_MEM(const data_t *, DNNL_ARG_DST)
                               : CTX_IN_MEM(const data_t *, DNNL_ARG_SRC);
    auto diff_dst = CTX_IN_MEM(const data_t *, DNNL_ARG_DIFF_DST);
    auto diff_src = CTX_OUT_MEM(data_t *, DNNL_ARG_DIFF_SRC);

    auto scratchpad = ctx.get_scratchpad_grantor();
    auto s_f = scratchpad.template get<float>(key_eltwise_src);
    auto dd_f = scratchpad.template get<float>(key_eltwise_diff_dst);

    const memory_desc_wrapper data_d(pd()->src_md());
    const memory_desc_wrapper diff_data_d(pd()->diff_src_md());

    const auto nelems = data_d.nelems(true);
    const auto alg_kind = pd()->desc()->alg_kind;
    const float alpha = pd()->desc()->alpha;
    const float beta = pd()->desc()->beta;

    src += data_d.offset0();
    diff_dst += diff_data_d.offset0();
    diff_src += diff_data_d.offset0();

    parallel(0, [&](const int ithr, const int nthr) {
        dim_t start = 0, end = 0;
        balance211(nelems, nthr, ithr, start, end);
        if (start == end) return;

        cvt_bfloat16_to_float(s_f + start, src + start, end - start);
        cvt_bfloat16_to_float(dd_f + start, diff_dst + start, end - start);

        for (dim_t i = start; i < end; i++) {
            dd_f[i] = compute_eltwise_scalar_bwd(
                    alg_kind, dd_f[i], s_f[i], alpha, beta);
        }

        cvt_float_to_bfloat16(diff_src + start, dd_f + start, end - start);
    });
}

template struct ref_eltwise_fwd_t<data_type::f32>;
template struct ref_eltwise_fwd_t<data_type::bf16>;
template struct ref_eltwise_fwd_t<data_type::s32>;
template struct ref_eltwise_fwd_t<data_type::s8>;
template struct ref_eltwise_fwd_t<data_type::u8>;

template struct ref_eltwise_bwd_t<data_type::f32>;
template struct ref_eltwise_bwd_t<data_type::bf16>;

} // namespace cpu
} // namespace impl
} // namespace dnnl

// vim: et ts=4 sw=4 cindent cino=+2s,l1,\:4,N-s
